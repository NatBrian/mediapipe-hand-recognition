{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Point History Model Comparison\n",
        "\n",
        "This notebook benchmarks every trained point-history model saved in `models/point_history` against the dynamic finger-trajectory dataset `data/point_history.csv` built from MediaPipe.\n",
        "\n",
        "The workflow loads all `.tflite` and `.joblib` checkpoints (or trains new ones) and reports:\n",
        "\n",
        "- Accuracy, macro F1, per-class precision/recall/F1, and the full classification report.\n",
        "- Top-k accuracy, inference latency or throughput, disk size, estimated parameter count, and recorded training time.\n",
        "- Robustness to additive trajectory noise.\n",
        "- Confusion matrix visualizations, McNemar statistical tests, and focused analysis of circular versus linear gestures.\n",
        "\n",
        "Adjust the configuration cell below to point at a different dataset, tweak the noise magnitude, or provide known training times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*ENSURE all models are trained using the same dataset*\n",
        "\n",
        "This run assumes the 5-class point-history dataset:\n",
        "* 0 - Stop\n",
        "* 1 - Clockwise circle\n",
        "* 2 - Counter-clockwise circle\n",
        "* 3 - Move (linear)\n",
        "* 4 - Figure eight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "e75f65c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.1\n",
            "Repo root: c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\n",
            "Dataset: c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\data\\point_history.csv\n",
            "Models directory: c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\n",
            "Training enabled: True\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import datetime\n",
        "import re\n",
        "import time\n",
        "from itertools import count\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.stats import chi2\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_recall_fscore_support,\n",
        "    top_k_accuracy_score,\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "np.set_printoptions(linewidth=140, suppress=True)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"data\").is_dir() and (candidate / \"models\").is_dir():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate the project root (data/ and models/ folders not found).\")\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "DATA_FILE = REPO_ROOT / \"data\" / \"point_history.csv\"\n",
        "LABELS_FILE = REPO_ROOT / \"data\" / \"point_history_labels.csv\"\n",
        "MODEL_DIR = REPO_ROOT / \"models\" / \"point_history\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TIME_STEPS = 16\n",
        "DIMENSION = 2\n",
        "INPUT_DIM = TIME_STEPS * DIMENSION\n",
        "TOP_K = 3\n",
        "TOP_K_KEY = f\"top_{TOP_K}_accuracy\"\n",
        "NOISE_STD = 0.015\n",
        "RANDOM_SEED = 42\n",
        "GESTURE_GROUPS = {\n",
        "    \"Circular\": [1, 2, 4],\n",
        "    \"Static_vs_Move\": [0, 3],\n",
        "}\n",
        "\n",
        "TRAINING_TIME_OVERRIDES = {\n",
        "    # \"joblib::point_history_classifier_LogisticRegression.joblib\": 3.2,\n",
        "    # \"tflite::point_history_classifier_mlp.tflite\": 120.0,\n",
        "}\n",
        "\n",
        "TRAIN_MODELS = True\n",
        "TEST_SIZE = 0.25\n",
        "\n",
        "TRAINED_JOBLIB_MODELS = []\n",
        "TRAINED_TFLITE_MODELS = []\n",
        "TRAINED_KERAS_MODELS = []\n",
        "TRAINING_RUN_RECORDS = []\n",
        "ARTIFACT_COUNTER = count()\n",
        "\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "JOBLIB_MODELS = []\n",
        "TFLITE_MODELS = []\n",
        "KERAS_MODELS = []\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Dataset: {DATA_FILE}\")\n",
        "print(f\"Models directory: {MODEL_DIR}\")\n",
        "print(f\"Training enabled: {TRAIN_MODELS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "db82ec19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (7185, 33) (rows x columns)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>-0.003704</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>-0.003704</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>-0.005556</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>-0.016667</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>-0.018519</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>-0.016667</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>-0.016667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.014815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.005556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.007407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>-0.004167</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.004167</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.007407</td>\n",
              "      <td>-0.004167</td>\n",
              "      <td>0.007407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.005556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2         3         4         5         6         7         8   \\\n",
              "0   0  0.0  0.0 -0.002083 -0.003704 -0.002083 -0.003704 -0.003125 -0.005556   \n",
              "1   0  0.0  0.0  0.001042  0.000000  0.002083  0.000000  0.001042  0.001852   \n",
              "2   0  0.0  0.0  0.001042  0.000000  0.000000  0.001852  0.001042  0.001852   \n",
              "3   0  0.0  0.0 -0.001042  0.001852  0.000000  0.001852  0.000000  0.001852   \n",
              "4   0  0.0  0.0  0.001042  0.000000  0.001042  0.000000  0.001042  0.000000   \n",
              "\n",
              "         9   ...        23        24        25        26        27        28  \\\n",
              "0 -0.002083  ... -0.003125 -0.016667 -0.002083 -0.018519 -0.002083 -0.016667   \n",
              "1  0.002083  ... -0.001042  0.003704 -0.001042  0.003704 -0.001042  0.001852   \n",
              "2  0.001042  ... -0.002083  0.003704 -0.002083  0.001852 -0.003125  0.005556   \n",
              "3  0.000000  ... -0.003125  0.001852 -0.004167  0.005556 -0.004167  0.005556   \n",
              "4  0.002083  ... -0.003125  0.003704 -0.003125  0.003704 -0.002083  0.005556   \n",
              "\n",
              "         29        30        31        32  \n",
              "0 -0.001042 -0.016667  0.000000 -0.014815  \n",
              "1 -0.002083  0.005556 -0.002083  0.005556  \n",
              "2 -0.003125  0.005556 -0.002083  0.007407  \n",
              "3 -0.003125  0.007407 -0.004167  0.007407  \n",
              "4 -0.003125  0.005556 -0.002083  0.005556  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix: (7185, 32), Labels: (7185,), Unique classes: [0, 1, 2, 3, 4]\n",
            "Noise std applied to trajectories: 0.015\n"
          ]
        }
      ],
      "source": [
        "def load_label_map(path: Path):\n",
        "    mapping = {}\n",
        "    with path.open(encoding=\"utf-8-sig\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for idx, row in enumerate(reader):\n",
        "            if not row:\n",
        "                continue\n",
        "            raw = row[0].strip()\n",
        "            digits = \"\".join(ch for ch in raw if ch.isdigit())\n",
        "            label_id = int(digits) if digits else idx\n",
        "            label_name = raw[: len(raw) - len(digits)] or raw\n",
        "            mapping[label_id] = label_name\n",
        "    return mapping\n",
        "\n",
        "label_map = load_label_map(LABELS_FILE)\n",
        "label_ids_sorted = sorted(label_map.keys())\n",
        "label_names_sorted = [label_map[i] for i in label_ids_sorted]\n",
        "\n",
        "df = pd.read_csv(DATA_FILE, header=None)\n",
        "print(f\"Dataset shape: {df.shape} (rows x columns)\")\n",
        "display(df.head())\n",
        "\n",
        "X = df.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
        "y = df.iloc[:, 0].astype(int).to_numpy()\n",
        "\n",
        "X_noise = (X + rng.normal(loc=0.0, scale=NOISE_STD, size=X.shape)).astype(np.float32)\n",
        "\n",
        "print(f\"Feature matrix: {X.shape}, Labels: {y.shape}, Unique classes: {sorted(np.unique(y))}\")\n",
        "print(f\"Noise std applied to trajectories: {NOISE_STD}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "fe3dac75",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train split: (5388, 32), Validation split: (1797, 32)\n",
            "Class distribution (train): [1111  925  959  976 1417]\n",
            "Class distribution (val):   [370 309 320 326 472]\n"
          ]
        }
      ],
      "source": [
        "NUM_FEATURES = X.shape[1]\n",
        "if NUM_FEATURES != INPUT_DIM:\n",
        "    print(f\"Warning: expected {INPUT_DIM} features but found {NUM_FEATURES}\")\n",
        "NUM_CLASSES = len(label_ids_sorted)\n",
        "\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "print(f\"Train split: {X_train_split.shape}, Validation split: {X_val_split.shape}\")\n",
        "print(f\"Class distribution (train): {np.bincount(y_train_split, minlength=NUM_CLASSES)}\")\n",
        "print(f\"Class distribution (val):   {np.bincount(y_val_split, minlength=NUM_CLASSES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4b2989",
      "metadata": {},
      "source": [
        "## Unified Training Pipelines\n",
        "\n",
        "The next cells consolidate the end-to-end training flows from the dedicated notebooks:\n",
        "\n",
        "- `point_history_training.ipynb` (MLP + LSTM time-history classifiers)\n",
        "- `keypoint_models_comparison_analysis.ipynb` (classical baselines, TFLite/graph transformers, and XGBoost)\n",
        "\n",
        "Running this section retrains every model variant, saves the resulting checkpoints inside `models/point_history`, and records metrics so the comparison stage can immediately benchmark the fresh outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "e6bf7e6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def register_tflite_model(display_name, alias, keras_model, train_time, metrics=None, quantize=True, allow_select_tf_ops=False, disable_lower_tensor_list=False):\n",
        "    slug = slugify_name(alias or display_name)\n",
        "    suffix = next_artifact_suffix()\n",
        "    keras_path = MODEL_DIR / f\"point_history_classifier_{slug}_{suffix}.keras\"\n",
        "    keras_model.save(keras_path, include_optimizer=False)\n",
        "\n",
        "    def convert_model(enable_select_ops, disable_tensor_list, quantize_flag):\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "        if quantize_flag:\n",
        "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        if enable_select_ops:\n",
        "            converter.target_spec.supported_ops = [\n",
        "                tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "                tf.lite.OpsSet.SELECT_TF_OPS,\n",
        "            ]\n",
        "            converter.allow_custom_ops = True\n",
        "        if disable_tensor_list:\n",
        "            for attr in (\"_experimental_lower_tensor_list_ops\", \"experimental_lower_tensor_list_ops\"):\n",
        "                if hasattr(converter, attr):\n",
        "                    setattr(converter, attr, False)\n",
        "        if hasattr(converter, \"experimental_new_converter\"):\n",
        "            converter.experimental_new_converter = True\n",
        "        return converter.convert()\n",
        "\n",
        "    tflite_buffer = None\n",
        "    tflite_path = None\n",
        "    try:\n",
        "        tflite_buffer = convert_model(allow_select_tf_ops, disable_lower_tensor_list, quantize)\n",
        "    except Exception as convert_err:\n",
        "        needs_retry = not (allow_select_tf_ops and disable_lower_tensor_list)\n",
        "        if not needs_retry:\n",
        "            print(\"TFLite conversion failed even with Select TF ops enabled; skipping TFLite export for\", display_name)\n",
        "            print(convert_err)\n",
        "        else:\n",
        "            print(\"Retrying TFLite conversion with Select TF ops and tensor-list lowering disabled due to: {}\".format(convert_err))\n",
        "            try:\n",
        "                tflite_buffer = convert_model(True, True, False)\n",
        "            except Exception as retry_err:\n",
        "                print(\"Retry also failed; skipping TFLite export for\", display_name)\n",
        "                print(retry_err)\n",
        "            else:\n",
        "                tflite_path = MODEL_DIR / f\"point_history_classifier_{slug}_{suffix}.tflite\"\n",
        "    else:\n",
        "        tflite_path = MODEL_DIR / f\"point_history_classifier_{slug}_{suffix}.tflite\"\n",
        "\n",
        "    if tflite_buffer is not None and tflite_path is not None:\n",
        "        with tflite_path.open(\"wb\") as f:\n",
        "            f.write(tflite_buffer)\n",
        "        TRAINED_TFLITE_MODELS.append(tflite_path)\n",
        "        TRAINING_TIME_OVERRIDES[f\"tflite::{tflite_path.name}\"] = float(train_time)\n",
        "        log_training_record(display_name, \"tflite\", tflite_path, train_time, metrics)\n",
        "    else:\n",
        "        log_training_record(display_name, \"keras_only\", keras_path, train_time, metrics)\n",
        "\n",
        "    return keras_path, tflite_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "ed4a19d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def register_keras_model(display_name, alias, keras_model, train_time, metrics=None):\n",
        "    slug = slugify_name(alias or display_name)\n",
        "    suffix = next_artifact_suffix()\n",
        "    keras_path = MODEL_DIR / f\"point_history_classifier_{slug}_{suffix}.keras\"\n",
        "    keras_model.save(keras_path, include_optimizer=False)\n",
        "    TRAINED_KERAS_MODELS.append(keras_path)\n",
        "    TRAINING_TIME_OVERRIDES[f\"keras::{keras_path.name}\"] = float(train_time)\n",
        "    log_training_record(display_name, \"keras\", keras_path, train_time, metrics)\n",
        "    return keras_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "46d6b46d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classical_baselines(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training classical ML baselines...\")\n",
        "    models = {\n",
        "        \"LinearSVC\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", LinearSVC(\n",
        "                C=1.0,\n",
        "                loss=\"squared_hinge\",\n",
        "                random_state=RANDOM_SEED,\n",
        "                dual=False,\n",
        "            )),\n",
        "        ]),\n",
        "        \"LogisticRegression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", LogisticRegression(\n",
        "                multi_class=\"multinomial\",\n",
        "                solver=\"lbfgs\",\n",
        "                max_iter=1000,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )),\n",
        "        ]),\n",
        "        \"KNeighbors\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", KNeighborsClassifier(\n",
        "                n_neighbors=5,\n",
        "                weights=\"distance\",\n",
        "                metric=\"minkowski\",\n",
        "            )),\n",
        "        ]),\n",
        "        \"SVC_RBF\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", SVC(\n",
        "                kernel=\"rbf\",\n",
        "                C=10.0,\n",
        "                gamma=\"scale\",\n",
        "                probability=True,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )),\n",
        "        ]),\n",
        "        \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=None,\n",
        "            random_state=RANDOM_SEED,\n",
        "            n_jobs=-1,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    artifacts = []\n",
        "    for name, estimator in models.items():\n",
        "        start = time.perf_counter()\n",
        "        estimator.fit(X_train, y_train)\n",
        "        duration = time.perf_counter() - start\n",
        "        y_pred = estimator.predict(X_val)\n",
        "        acc, macro_f1 = evaluate_split_metrics(y_val, y_pred)\n",
        "        metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "        path = register_joblib_model(f\"Baseline::{name}\", name, estimator, duration, metrics)\n",
        "        print(f\"  {name:15s} | acc={acc:.4f} macro F1={macro_f1:.4f} | saved→{path.name}\")\n",
        "        artifacts.append((name, path))\n",
        "    return artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "0af071c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mlp_baseline(input_dim, num_classes):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Input((input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(24, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ], name=\"mlp_point_history_classifier\")\n",
        "\n",
        "def build_lstm_classifier(time_steps, point_dim, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input((time_steps * point_dim,)),\n",
        "        tf.keras.layers.Reshape((time_steps, point_dim)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.LSTM(16),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ], name=\"lstm_point_history_classifier\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_mlp_classifier(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training MLP baseline (Keras)...\")\n",
        "    reset_tf_session()\n",
        "    model = build_mlp_baseline(X_train.shape[1], NUM_CLASSES)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, verbose=1)\n",
        "    ]\n",
        "    start = time.perf_counter()\n",
        "    model.fit(\n",
        "        X_train.astype(np.float32),\n",
        "        y_train,\n",
        "        epochs=400,\n",
        "        batch_size=128,\n",
        "        validation_data=(X_val.astype(np.float32), y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "    duration = time.perf_counter() - start\n",
        "    preds = np.argmax(model.predict(X_val.astype(np.float32), verbose=0), axis=1)\n",
        "    acc, macro_f1 = evaluate_split_metrics(y_val, preds)\n",
        "    metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "    keras_path, tflite_path = register_tflite_model(\"MLP\", \"mlp\", model, duration, metrics)\n",
        "    print(f\"  MLP validation accuracy={acc:.4f}, macro F1={macro_f1:.4f} | saved→{tflite_path.name}\")\n",
        "    return keras_path, tflite_path\n",
        "\n",
        "\n",
        "def train_lstm_classifier(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training LSTM sequence model (Keras)...\")\n",
        "    reset_tf_session()\n",
        "    model = build_lstm_classifier(TIME_STEPS, DIMENSION, NUM_CLASSES)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, verbose=1)]\n",
        "    start = time.perf_counter()\n",
        "    model.fit(\n",
        "        X_train.astype(np.float32),\n",
        "        y_train,\n",
        "        epochs=400,\n",
        "        batch_size=128,\n",
        "        validation_data=(X_val.astype(np.float32), y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "    duration = time.perf_counter() - start\n",
        "    preds = np.argmax(model.predict(X_val.astype(np.float32), verbose=0), axis=1)\n",
        "    acc, macro_f1 = evaluate_split_metrics(y_val, preds)\n",
        "    metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "    keras_path = register_keras_model(\"LSTM\", \"lstm\", model, duration, metrics)\n",
        "    print(f\"  LSTM validation accuracy={acc:.4f}, macro F1={macro_f1:.4f} | saved→{keras_path.name}\")\n",
        "    return keras_path, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "3f1728b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "TIME_CONNECTIONS = [(i, i + 1) for i in range(TIME_STEPS - 1)]\n",
        "\n",
        "def build_adjacency_matrix(num_nodes, connections):\n",
        "    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
        "    for i, j in connections:\n",
        "        adj[i, j] = 1.0\n",
        "        adj[j, i] = 1.0\n",
        "    np.fill_diagonal(adj, 1.0)\n",
        "    return adj\n",
        "\n",
        "GRAPH_ADJ = build_adjacency_matrix(TIME_STEPS, TIME_CONNECTIONS)\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, adjacency_matrix, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        self._adjacency_matrix_for_config = adjacency_matrix\n",
        "        self.adj = tf.constant(adjacency_matrix, dtype=tf.float32)\n",
        "        self.graph_bias_weight = self.add_weight(\n",
        "            name=\"graph_bias_weight\",\n",
        "            shape=(),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        bias = self.adj * self.graph_bias_weight\n",
        "        bias = tf.expand_dims(bias, axis=0)\n",
        "        bias = tf.expand_dims(bias, axis=1)\n",
        "        return self.mha(x, x, attention_mask=bias)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"adjacency_matrix\": self._adjacency_matrix_for_config.tolist(),\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        config[\"adjacency_matrix\"] = np.array(config[\"adjacency_matrix\"], dtype=np.float32)\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "def build_graph_transformer(adjacency_matrix, num_steps, step_dim, num_classes,\n",
        "                            d_model=64, num_heads=4, ff_multiplier=2, num_layers=2,\n",
        "                            dropout_rate=0.1):\n",
        "    inputs = tf.keras.Input(shape=(num_steps * step_dim,), name=\"flat_input\")\n",
        "    x = tf.keras.layers.Reshape((num_steps, step_dim), name=\"reshape_tokens\")(inputs)\n",
        "    x = tf.keras.layers.Dense(d_model, name=\"token_projection\")(x)\n",
        "    step_ids = tf.range(num_steps)\n",
        "    step_emb = tf.keras.layers.Embedding(input_dim=num_steps, output_dim=d_model)(step_ids)\n",
        "    x = tf.keras.layers.Add(name=\"add_step_id\")([x, tf.expand_dims(step_emb, axis=0)])\n",
        "\n",
        "    for idx in range(num_layers):\n",
        "        res = x\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=f\"ln1_{idx}\")(x)\n",
        "        x = GraphAttentionLayer(num_heads=num_heads,\n",
        "                                key_dim=d_model // num_heads,\n",
        "                                adjacency_matrix=adjacency_matrix,\n",
        "                                name=f\"graph_attn_{idx}\")(x)\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = tf.keras.layers.Add(name=f\"res1_{idx}\")([res, x])\n",
        "\n",
        "        res = x\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=f\"ln2_{idx}\")(x)\n",
        "        x = tf.keras.layers.Dense(d_model * ff_multiplier, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = tf.keras.layers.Dense(d_model)(x)\n",
        "        x = tf.keras.layers.Add(name=f\"res2_{idx}\")([res, x])\n",
        "\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling1D(name=\"gap\")(x)\n",
        "    pooled = tf.keras.layers.Dropout(dropout_rate)(pooled)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"classification_head\")(pooled)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"TemporalGraphTransformer\")\n",
        "\n",
        "\n",
        "def train_graph_transformer(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training graph-aware Transformer...\")\n",
        "    reset_tf_session()\n",
        "    model = build_graph_transformer(GRAPH_ADJ, TIME_STEPS, DIMENSION, NUM_CLASSES)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)]\n",
        "    start = time.perf_counter()\n",
        "    model.fit(\n",
        "        X_train.astype(np.float32), y_train,\n",
        "        epochs=400,\n",
        "        batch_size=128,\n",
        "        validation_data=(X_val.astype(np.float32), y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "    duration = time.perf_counter() - start\n",
        "    preds = np.argmax(model.predict(X_val.astype(np.float32), verbose=0), axis=1)\n",
        "    acc, macro_f1 = evaluate_split_metrics(y_val, preds)\n",
        "    metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "    _, tflite_path = register_tflite_model(\"GraphTransformer\", \"graph_transformer\", model, duration, metrics)\n",
        "    print(f\"  Graph Transformer validation accuracy={acc:.4f}, macro F1={macro_f1:.4f} | saved→{tflite_path.name}\")\n",
        "    return tflite_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "05cbd2fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_transformer_classifier(num_steps, step_dim, num_classes,\n",
        "                                 d_model=64, num_heads=4, ff_multiplier=2,\n",
        "                                 num_layers=2, dropout_rate=0.1):\n",
        "    inputs = tf.keras.Input(shape=(num_steps * step_dim,))\n",
        "    x = tf.keras.layers.Reshape((num_steps, step_dim))(inputs)\n",
        "    x = tf.keras.layers.Dense(d_model)(x)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
        "        x = tf.keras.layers.Add()([x, attn])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        ff = tf.keras.layers.Dense(d_model * ff_multiplier, activation=\"relu\")(x)\n",
        "        ff = tf.keras.layers.Dense(d_model)(ff)\n",
        "        ff = tf.keras.layers.Dropout(dropout_rate)(ff)\n",
        "        x = tf.keras.layers.Add()([x, ff])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"transformer_self_attention\")\n",
        "\n",
        "\n",
        "def train_self_attention_transformer(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training Transformer self-attention model...\")\n",
        "    reset_tf_session()\n",
        "    model = build_transformer_classifier(TIME_STEPS, DIMENSION, NUM_CLASSES)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=1)]\n",
        "    start = time.perf_counter()\n",
        "    model.fit(\n",
        "        X_train.astype(np.float32), y_train,\n",
        "        epochs=400,\n",
        "        batch_size=128,\n",
        "        validation_data=(X_val.astype(np.float32), y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "    duration = time.perf_counter() - start\n",
        "    preds = np.argmax(model.predict(X_val.astype(np.float32), verbose=0), axis=1)\n",
        "    acc, macro_f1 = evaluate_split_metrics(y_val, preds)\n",
        "    metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "    _, tflite_path = register_tflite_model(\"TransformerSelfAttention\", \"transformer_self_attention\", model, duration, metrics)\n",
        "    print(f\"  Transformer validation accuracy={acc:.4f}, macro F1={macro_f1:.4f} | saved→{tflite_path.name}\")\n",
        "    return tflite_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "af7df39b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_xgboost_classifier(X_train, y_train, X_val, y_val):\n",
        "    print(\"Training XGBoost classifier with grid search...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    base_params = dict(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=num_classes,\n",
        "        eval_metric=\"mlogloss\",\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    param_grid = {\n",
        "        \"max_depth\": [2, 4],\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"learning_rate\": [0.05, 0.1],\n",
        "    }\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
        "    estimator = XGBClassifier(**base_params)\n",
        "    grid = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=param_grid,\n",
        "        scoring=\"accuracy\",\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "    )\n",
        "    start = time.perf_counter()\n",
        "    grid.fit(X_train, y_train)\n",
        "    duration = time.perf_counter() - start\n",
        "    best_model = grid.best_estimator_\n",
        "    print(f\"  Best params: {grid.best_params_} | CV accuracy={grid.best_score_:.4f}\")\n",
        "    y_pred = best_model.predict(X_val)\n",
        "    acc, macro_f1 = evaluate_split_metrics(y_val, y_pred)\n",
        "    metrics = {\"val_accuracy\": acc, \"val_macro_f1\": macro_f1}\n",
        "    path = register_joblib_model(\"XGBoost\", \"xgboost\", best_model, duration, metrics)\n",
        "    print(f\"  XGBoost validation accuracy={acc:.4f}, macro F1={macro_f1:.4f} | saved→{path.name}\")\n",
        "    return path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "a7ec7bd5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training classical ML baselines...\n",
            "  LinearSVC       | acc=0.2142 macro F1=0.0719 | saved→point_history_classifier_linearsvc_20251121_014943_00.joblib\n",
            "  LogisticRegression | acc=0.2159 macro F1=0.0734 | saved→point_history_classifier_logisticregression_20251121_014943_01.joblib\n",
            "  KNeighbors      | acc=0.9655 macro F1=0.9668 | saved→point_history_classifier_kneighbors_20251121_014943_02.joblib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SVC_RBF         | acc=0.9060 macro F1=0.9110 | saved→point_history_classifier_svc_rbf_20251121_014945_03.joblib\n",
            "  RandomForest    | acc=0.9839 macro F1=0.9846 | saved→point_history_classifier_randomforest_20251121_014946_04.joblib\n",
            "Training MLP baseline (Keras)...\n",
            "Epoch 1/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.1639 - loss: 1.6003 - val_accuracy: 0.2198 - val_loss: 1.5761\n",
            "Epoch 2/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2129 - loss: 1.5622 - val_accuracy: 0.2627 - val_loss: 1.5388\n",
            "Epoch 3/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2576 - loss: 1.5225 - val_accuracy: 0.2627 - val_loss: 1.4937\n",
            "Epoch 4/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2630 - loss: 1.4832 - val_accuracy: 0.2627 - val_loss: 1.4534\n",
            "Epoch 5/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3940 - loss: 1.4535 - val_accuracy: 0.4302 - val_loss: 1.4196\n",
            "Epoch 6/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4395 - loss: 1.4172 - val_accuracy: 0.4480 - val_loss: 1.3866\n",
            "Epoch 7/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4543 - loss: 1.3896 - val_accuracy: 0.4719 - val_loss: 1.3555\n",
            "Epoch 8/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4683 - loss: 1.3562 - val_accuracy: 0.4986 - val_loss: 1.3233\n",
            "Epoch 9/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4770 - loss: 1.3298 - val_accuracy: 0.5298 - val_loss: 1.2903\n",
            "Epoch 10/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5045 - loss: 1.2972 - val_accuracy: 0.5626 - val_loss: 1.2559\n",
            "Epoch 11/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5087 - loss: 1.2646 - val_accuracy: 0.5804 - val_loss: 1.2162\n",
            "Epoch 12/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5254 - loss: 1.2232 - val_accuracy: 0.6060 - val_loss: 1.1742\n",
            "Epoch 13/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5416 - loss: 1.1962 - val_accuracy: 0.6205 - val_loss: 1.1334\n",
            "Epoch 14/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5707 - loss: 1.1562 - val_accuracy: 0.6605 - val_loss: 1.0927\n",
            "Epoch 15/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5833 - loss: 1.1270 - val_accuracy: 0.7051 - val_loss: 1.0525\n",
            "Epoch 16/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5989 - loss: 1.0913 - val_accuracy: 0.7373 - val_loss: 1.0137\n",
            "Epoch 17/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6136 - loss: 1.0616 - val_accuracy: 0.7563 - val_loss: 0.9794\n",
            "Epoch 18/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6212 - loss: 1.0384 - val_accuracy: 0.7735 - val_loss: 0.9477\n",
            "Epoch 19/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6288 - loss: 1.0257 - val_accuracy: 0.7830 - val_loss: 0.9208\n",
            "Epoch 20/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6398 - loss: 1.0080 - val_accuracy: 0.7858 - val_loss: 0.8986\n",
            "Epoch 21/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6466 - loss: 0.9910 - val_accuracy: 0.7896 - val_loss: 0.8753\n",
            "Epoch 22/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6607 - loss: 0.9729 - val_accuracy: 0.7997 - val_loss: 0.8549\n",
            "Epoch 23/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6529 - loss: 0.9627 - val_accuracy: 0.8013 - val_loss: 0.8391\n",
            "Epoch 24/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.9355 - val_accuracy: 0.8114 - val_loss: 0.8157\n",
            "Epoch 25/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6787 - loss: 0.9199 - val_accuracy: 0.8119 - val_loss: 0.7977\n",
            "Epoch 26/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6758 - loss: 0.9161 - val_accuracy: 0.8214 - val_loss: 0.7817\n",
            "Epoch 27/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6808 - loss: 0.9075 - val_accuracy: 0.8152 - val_loss: 0.7690\n",
            "Epoch 28/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6876 - loss: 0.8940 - val_accuracy: 0.8114 - val_loss: 0.7574\n",
            "Epoch 29/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 0.8825 - val_accuracy: 0.8136 - val_loss: 0.7464\n",
            "Epoch 30/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6919 - loss: 0.8720 - val_accuracy: 0.8214 - val_loss: 0.7334\n",
            "Epoch 31/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6945 - loss: 0.8602 - val_accuracy: 0.8247 - val_loss: 0.7207\n",
            "Epoch 32/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7084 - loss: 0.8481 - val_accuracy: 0.8230 - val_loss: 0.7086\n",
            "Epoch 33/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7032 - loss: 0.8452 - val_accuracy: 0.8269 - val_loss: 0.6977\n",
            "Epoch 34/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7038 - loss: 0.8434 - val_accuracy: 0.8269 - val_loss: 0.6890\n",
            "Epoch 35/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7107 - loss: 0.8292 - val_accuracy: 0.8292 - val_loss: 0.6792\n",
            "Epoch 36/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7030 - loss: 0.8249 - val_accuracy: 0.8275 - val_loss: 0.6675\n",
            "Epoch 37/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7062 - loss: 0.8200 - val_accuracy: 0.8230 - val_loss: 0.6660\n",
            "Epoch 38/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7114 - loss: 0.8257 - val_accuracy: 0.8292 - val_loss: 0.6540\n",
            "Epoch 39/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7162 - loss: 0.8042 - val_accuracy: 0.8286 - val_loss: 0.6479\n",
            "Epoch 40/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.7999 - val_accuracy: 0.8336 - val_loss: 0.6383\n",
            "Epoch 41/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7287 - loss: 0.7902 - val_accuracy: 0.8303 - val_loss: 0.6342\n",
            "Epoch 42/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7223 - loss: 0.7929 - val_accuracy: 0.8325 - val_loss: 0.6281\n",
            "Epoch 43/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7209 - loss: 0.7782 - val_accuracy: 0.8347 - val_loss: 0.6180\n",
            "Epoch 44/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7253 - loss: 0.7782 - val_accuracy: 0.8353 - val_loss: 0.6112\n",
            "Epoch 45/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7327 - loss: 0.7680 - val_accuracy: 0.8370 - val_loss: 0.6072\n",
            "Epoch 46/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7261 - loss: 0.7729 - val_accuracy: 0.8403 - val_loss: 0.6003\n",
            "Epoch 47/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7220 - loss: 0.7746 - val_accuracy: 0.8381 - val_loss: 0.5976\n",
            "Epoch 48/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7279 - loss: 0.7642 - val_accuracy: 0.8386 - val_loss: 0.5910\n",
            "Epoch 49/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7235 - loss: 0.7711 - val_accuracy: 0.8392 - val_loss: 0.5885\n",
            "Epoch 50/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7287 - loss: 0.7647 - val_accuracy: 0.8347 - val_loss: 0.5892\n",
            "Epoch 51/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7298 - loss: 0.7592 - val_accuracy: 0.8370 - val_loss: 0.5845\n",
            "Epoch 52/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7357 - loss: 0.7425 - val_accuracy: 0.8364 - val_loss: 0.5791\n",
            "Epoch 53/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7385 - loss: 0.7499 - val_accuracy: 0.8347 - val_loss: 0.5796\n",
            "Epoch 54/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7303 - loss: 0.7542 - val_accuracy: 0.8403 - val_loss: 0.5711\n",
            "Epoch 55/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7409 - loss: 0.7447 - val_accuracy: 0.8403 - val_loss: 0.5661\n",
            "Epoch 56/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7355 - loss: 0.7409 - val_accuracy: 0.8442 - val_loss: 0.5616\n",
            "Epoch 57/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7316 - loss: 0.7433 - val_accuracy: 0.8436 - val_loss: 0.5585\n",
            "Epoch 58/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7418 - loss: 0.7350 - val_accuracy: 0.8392 - val_loss: 0.5576\n",
            "Epoch 59/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7340 - loss: 0.7433 - val_accuracy: 0.8392 - val_loss: 0.5604\n",
            "Epoch 60/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7389 - loss: 0.7375 - val_accuracy: 0.8425 - val_loss: 0.5541\n",
            "Epoch 61/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7294 - loss: 0.7384 - val_accuracy: 0.8436 - val_loss: 0.5528\n",
            "Epoch 62/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7389 - loss: 0.7352 - val_accuracy: 0.8436 - val_loss: 0.5525\n",
            "Epoch 63/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7392 - loss: 0.7290 - val_accuracy: 0.8453 - val_loss: 0.5479\n",
            "Epoch 64/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7398 - loss: 0.7236 - val_accuracy: 0.8459 - val_loss: 0.5468\n",
            "Epoch 65/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7379 - loss: 0.7290 - val_accuracy: 0.8442 - val_loss: 0.5432\n",
            "Epoch 66/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.7216 - val_accuracy: 0.8486 - val_loss: 0.5416\n",
            "Epoch 67/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7409 - loss: 0.7301 - val_accuracy: 0.8453 - val_loss: 0.5415\n",
            "Epoch 68/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 0.7141 - val_accuracy: 0.8447 - val_loss: 0.5397\n",
            "Epoch 69/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7472 - loss: 0.7172 - val_accuracy: 0.8392 - val_loss: 0.5456\n",
            "Epoch 70/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7357 - loss: 0.7247 - val_accuracy: 0.8447 - val_loss: 0.5344\n",
            "Epoch 71/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7407 - loss: 0.7135 - val_accuracy: 0.8420 - val_loss: 0.5348\n",
            "Epoch 72/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7392 - loss: 0.7180 - val_accuracy: 0.8436 - val_loss: 0.5311\n",
            "Epoch 73/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7498 - loss: 0.7042 - val_accuracy: 0.8420 - val_loss: 0.5291\n",
            "Epoch 74/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7476 - loss: 0.7081 - val_accuracy: 0.8453 - val_loss: 0.5244\n",
            "Epoch 75/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7472 - loss: 0.7002 - val_accuracy: 0.8459 - val_loss: 0.5218\n",
            "Epoch 76/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7424 - loss: 0.7134 - val_accuracy: 0.8481 - val_loss: 0.5205\n",
            "Epoch 77/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.7088 - val_accuracy: 0.8497 - val_loss: 0.5171\n",
            "Epoch 78/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7489 - loss: 0.7090 - val_accuracy: 0.8397 - val_loss: 0.5230\n",
            "Epoch 79/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7489 - loss: 0.6942 - val_accuracy: 0.8436 - val_loss: 0.5189\n",
            "Epoch 80/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7507 - loss: 0.6912 - val_accuracy: 0.8481 - val_loss: 0.5124\n",
            "Epoch 81/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7565 - loss: 0.7006 - val_accuracy: 0.8414 - val_loss: 0.5166\n",
            "Epoch 82/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7513 - loss: 0.6888 - val_accuracy: 0.8425 - val_loss: 0.5158\n",
            "Epoch 83/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7454 - loss: 0.7034 - val_accuracy: 0.8464 - val_loss: 0.5116\n",
            "Epoch 84/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7450 - loss: 0.7047 - val_accuracy: 0.8420 - val_loss: 0.5131\n",
            "Epoch 85/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.6977 - val_accuracy: 0.8397 - val_loss: 0.5183\n",
            "Epoch 86/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7472 - loss: 0.7021 - val_accuracy: 0.8414 - val_loss: 0.5092\n",
            "Epoch 87/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7524 - loss: 0.6978 - val_accuracy: 0.8464 - val_loss: 0.5072\n",
            "Epoch 88/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7558 - loss: 0.6922 - val_accuracy: 0.8464 - val_loss: 0.5057\n",
            "Epoch 89/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7532 - loss: 0.6909 - val_accuracy: 0.8392 - val_loss: 0.5096\n",
            "Epoch 90/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7513 - loss: 0.6977 - val_accuracy: 0.8420 - val_loss: 0.5058\n",
            "Epoch 91/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7526 - loss: 0.6980 - val_accuracy: 0.8425 - val_loss: 0.5031\n",
            "Epoch 92/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7537 - loss: 0.6795 - val_accuracy: 0.8425 - val_loss: 0.5006\n",
            "Epoch 93/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7548 - loss: 0.6912 - val_accuracy: 0.8486 - val_loss: 0.4988\n",
            "Epoch 94/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7541 - loss: 0.6828 - val_accuracy: 0.8403 - val_loss: 0.5058\n",
            "Epoch 95/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7598 - loss: 0.6683 - val_accuracy: 0.8459 - val_loss: 0.4971\n",
            "Epoch 96/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7476 - loss: 0.7062 - val_accuracy: 0.8403 - val_loss: 0.5080\n",
            "Epoch 97/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.6701 - val_accuracy: 0.8470 - val_loss: 0.5001\n",
            "Epoch 98/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7561 - loss: 0.6882 - val_accuracy: 0.8470 - val_loss: 0.4979\n",
            "Epoch 99/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7500 - loss: 0.6901 - val_accuracy: 0.8514 - val_loss: 0.4939\n",
            "Epoch 100/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7652 - loss: 0.6704 - val_accuracy: 0.8486 - val_loss: 0.4957\n",
            "Epoch 101/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7591 - loss: 0.6855 - val_accuracy: 0.8442 - val_loss: 0.4952\n",
            "Epoch 102/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7563 - loss: 0.6809 - val_accuracy: 0.8497 - val_loss: 0.4965\n",
            "Epoch 103/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7635 - loss: 0.6666 - val_accuracy: 0.8497 - val_loss: 0.4930\n",
            "Epoch 104/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7559 - loss: 0.6826 - val_accuracy: 0.8509 - val_loss: 0.4935\n",
            "Epoch 105/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7548 - loss: 0.6734 - val_accuracy: 0.8459 - val_loss: 0.4942\n",
            "Epoch 106/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7565 - loss: 0.6724 - val_accuracy: 0.8475 - val_loss: 0.4978\n",
            "Epoch 107/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7546 - loss: 0.6767 - val_accuracy: 0.8453 - val_loss: 0.4919\n",
            "Epoch 108/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7632 - loss: 0.6621 - val_accuracy: 0.8503 - val_loss: 0.4956\n",
            "Epoch 109/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7559 - loss: 0.6769 - val_accuracy: 0.8436 - val_loss: 0.5004\n",
            "Epoch 110/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7617 - loss: 0.6678 - val_accuracy: 0.8520 - val_loss: 0.4891\n",
            "Epoch 111/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7635 - loss: 0.6718 - val_accuracy: 0.8408 - val_loss: 0.5019\n",
            "Epoch 112/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7504 - loss: 0.6775 - val_accuracy: 0.8459 - val_loss: 0.4952\n",
            "Epoch 113/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7571 - loss: 0.6829 - val_accuracy: 0.8403 - val_loss: 0.4998\n",
            "Epoch 114/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7580 - loss: 0.6710 - val_accuracy: 0.8470 - val_loss: 0.4917\n",
            "Epoch 115/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7647 - loss: 0.6774 - val_accuracy: 0.8436 - val_loss: 0.4991\n",
            "Epoch 116/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7556 - loss: 0.6602 - val_accuracy: 0.8447 - val_loss: 0.4938\n",
            "Epoch 117/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.6609 - val_accuracy: 0.8475 - val_loss: 0.4902\n",
            "Epoch 118/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7550 - loss: 0.6777 - val_accuracy: 0.8492 - val_loss: 0.4888\n",
            "Epoch 119/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7632 - loss: 0.6611 - val_accuracy: 0.8425 - val_loss: 0.4903\n",
            "Epoch 120/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7598 - loss: 0.6746 - val_accuracy: 0.8464 - val_loss: 0.4897\n",
            "Epoch 121/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7530 - loss: 0.6794 - val_accuracy: 0.8514 - val_loss: 0.4871\n",
            "Epoch 122/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7615 - loss: 0.6589 - val_accuracy: 0.8525 - val_loss: 0.4813\n",
            "Epoch 123/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7606 - loss: 0.6642 - val_accuracy: 0.8470 - val_loss: 0.4877\n",
            "Epoch 124/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7582 - loss: 0.6638 - val_accuracy: 0.8514 - val_loss: 0.4827\n",
            "Epoch 125/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7567 - loss: 0.6661 - val_accuracy: 0.8509 - val_loss: 0.4775\n",
            "Epoch 126/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.6656 - val_accuracy: 0.8520 - val_loss: 0.4834\n",
            "Epoch 127/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7597 - loss: 0.6677 - val_accuracy: 0.8470 - val_loss: 0.4884\n",
            "Epoch 128/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7580 - loss: 0.6652 - val_accuracy: 0.8464 - val_loss: 0.4861\n",
            "Epoch 129/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7617 - loss: 0.6687 - val_accuracy: 0.8447 - val_loss: 0.4861\n",
            "Epoch 130/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 0.6820 - val_accuracy: 0.8442 - val_loss: 0.4847\n",
            "Epoch 131/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7617 - loss: 0.6665 - val_accuracy: 0.8564 - val_loss: 0.4777\n",
            "Epoch 132/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7615 - loss: 0.6673 - val_accuracy: 0.8559 - val_loss: 0.4792\n",
            "Epoch 133/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7567 - loss: 0.6637 - val_accuracy: 0.8308 - val_loss: 0.4952\n",
            "Epoch 134/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7634 - loss: 0.6592 - val_accuracy: 0.8509 - val_loss: 0.4821\n",
            "Epoch 135/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7593 - loss: 0.6628 - val_accuracy: 0.8453 - val_loss: 0.4870\n",
            "Epoch 136/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.6574 - val_accuracy: 0.8370 - val_loss: 0.4889\n",
            "Epoch 137/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.6608 - val_accuracy: 0.8447 - val_loss: 0.4870\n",
            "Epoch 138/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.6536 - val_accuracy: 0.8536 - val_loss: 0.4789\n",
            "Epoch 139/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7617 - loss: 0.6658 - val_accuracy: 0.8403 - val_loss: 0.4894\n",
            "Epoch 140/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7641 - loss: 0.6519 - val_accuracy: 0.8492 - val_loss: 0.4786\n",
            "Epoch 141/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7606 - loss: 0.6555 - val_accuracy: 0.8486 - val_loss: 0.4848\n",
            "Epoch 142/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7632 - loss: 0.6465 - val_accuracy: 0.8481 - val_loss: 0.4846\n",
            "Epoch 143/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7637 - loss: 0.6600 - val_accuracy: 0.8397 - val_loss: 0.4865\n",
            "Epoch 144/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.6590 - val_accuracy: 0.8464 - val_loss: 0.4826\n",
            "Epoch 145/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7621 - loss: 0.6595 - val_accuracy: 0.8486 - val_loss: 0.4807\n",
            "Epoch 146/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7598 - loss: 0.6549 - val_accuracy: 0.8509 - val_loss: 0.4799\n",
            "Epoch 147/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7647 - loss: 0.6476 - val_accuracy: 0.8497 - val_loss: 0.4749\n",
            "Epoch 148/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7632 - loss: 0.6537 - val_accuracy: 0.8364 - val_loss: 0.4878\n",
            "Epoch 149/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7630 - loss: 0.6556 - val_accuracy: 0.8470 - val_loss: 0.4779\n",
            "Epoch 150/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7624 - loss: 0.6716 - val_accuracy: 0.8442 - val_loss: 0.4817\n",
            "Epoch 151/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.6425 - val_accuracy: 0.8553 - val_loss: 0.4738\n",
            "Epoch 152/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7600 - loss: 0.6614 - val_accuracy: 0.8548 - val_loss: 0.4729\n",
            "Epoch 153/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7621 - loss: 0.6519 - val_accuracy: 0.8370 - val_loss: 0.4860\n",
            "Epoch 154/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7660 - loss: 0.6457 - val_accuracy: 0.8431 - val_loss: 0.4822\n",
            "Epoch 155/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7587 - loss: 0.6652 - val_accuracy: 0.8509 - val_loss: 0.4766\n",
            "Epoch 156/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7669 - loss: 0.6562 - val_accuracy: 0.8347 - val_loss: 0.4883\n",
            "Epoch 157/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7660 - loss: 0.6547 - val_accuracy: 0.8525 - val_loss: 0.4768\n",
            "Epoch 158/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7712 - loss: 0.6547 - val_accuracy: 0.8503 - val_loss: 0.4767\n",
            "Epoch 159/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7734 - loss: 0.6450 - val_accuracy: 0.8403 - val_loss: 0.4827\n",
            "Epoch 160/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7598 - loss: 0.6577 - val_accuracy: 0.8509 - val_loss: 0.4751\n",
            "Epoch 161/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7663 - loss: 0.6384 - val_accuracy: 0.8525 - val_loss: 0.4722\n",
            "Epoch 162/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7665 - loss: 0.6540 - val_accuracy: 0.8336 - val_loss: 0.4861\n",
            "Epoch 163/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7550 - loss: 0.6638 - val_accuracy: 0.8475 - val_loss: 0.4768\n",
            "Epoch 164/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7665 - loss: 0.6483 - val_accuracy: 0.8509 - val_loss: 0.4719\n",
            "Epoch 165/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7615 - loss: 0.6623 - val_accuracy: 0.8531 - val_loss: 0.4716\n",
            "Epoch 166/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7730 - loss: 0.6326 - val_accuracy: 0.8442 - val_loss: 0.4812\n",
            "Epoch 167/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7608 - loss: 0.6625 - val_accuracy: 0.8536 - val_loss: 0.4724\n",
            "Epoch 168/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7585 - loss: 0.6640 - val_accuracy: 0.8497 - val_loss: 0.4748\n",
            "Epoch 169/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7687 - loss: 0.6487 - val_accuracy: 0.8459 - val_loss: 0.4786\n",
            "Epoch 170/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7663 - loss: 0.6482 - val_accuracy: 0.8425 - val_loss: 0.4790\n",
            "Epoch 171/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7751 - loss: 0.6313 - val_accuracy: 0.8553 - val_loss: 0.4686\n",
            "Epoch 172/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7725 - loss: 0.6396 - val_accuracy: 0.8408 - val_loss: 0.4785\n",
            "Epoch 173/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7710 - loss: 0.6546 - val_accuracy: 0.8475 - val_loss: 0.4750\n",
            "Epoch 174/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.6421 - val_accuracy: 0.8520 - val_loss: 0.4710\n",
            "Epoch 175/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7665 - loss: 0.6414 - val_accuracy: 0.8587 - val_loss: 0.4652\n",
            "Epoch 176/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7643 - loss: 0.6642 - val_accuracy: 0.8470 - val_loss: 0.4751\n",
            "Epoch 177/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7734 - loss: 0.6362 - val_accuracy: 0.8587 - val_loss: 0.4631\n",
            "Epoch 178/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7643 - loss: 0.6483 - val_accuracy: 0.8531 - val_loss: 0.4680\n",
            "Epoch 179/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7669 - loss: 0.6397 - val_accuracy: 0.8592 - val_loss: 0.4645\n",
            "Epoch 180/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7678 - loss: 0.6392 - val_accuracy: 0.8509 - val_loss: 0.4715\n",
            "Epoch 181/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7693 - loss: 0.6432 - val_accuracy: 0.8319 - val_loss: 0.4821\n",
            "Epoch 182/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7595 - loss: 0.6500 - val_accuracy: 0.8592 - val_loss: 0.4639\n",
            "Epoch 183/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7600 - loss: 0.6498 - val_accuracy: 0.8464 - val_loss: 0.4737\n",
            "Epoch 184/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7652 - loss: 0.6548 - val_accuracy: 0.8447 - val_loss: 0.4744\n",
            "Epoch 185/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7634 - loss: 0.6445 - val_accuracy: 0.8397 - val_loss: 0.4771\n",
            "Epoch 186/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7697 - loss: 0.6434 - val_accuracy: 0.8425 - val_loss: 0.4737\n",
            "Epoch 187/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7695 - loss: 0.6404 - val_accuracy: 0.8531 - val_loss: 0.4657\n",
            "Epoch 188/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.6495 - val_accuracy: 0.8475 - val_loss: 0.4746\n",
            "Epoch 189/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7699 - loss: 0.6518 - val_accuracy: 0.8403 - val_loss: 0.4760\n",
            "Epoch 190/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7678 - loss: 0.6406 - val_accuracy: 0.8408 - val_loss: 0.4794\n",
            "Epoch 191/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.6475 - val_accuracy: 0.8459 - val_loss: 0.4746\n",
            "Epoch 192/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7712 - loss: 0.6391 - val_accuracy: 0.8397 - val_loss: 0.4764\n",
            "Epoch 193/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7674 - loss: 0.6422 - val_accuracy: 0.8509 - val_loss: 0.4706\n",
            "Epoch 194/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7736 - loss: 0.6369 - val_accuracy: 0.8381 - val_loss: 0.4794\n",
            "Epoch 195/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.6531 - val_accuracy: 0.8358 - val_loss: 0.4783\n",
            "Epoch 196/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7647 - loss: 0.6594 - val_accuracy: 0.8470 - val_loss: 0.4712\n",
            "Epoch 197/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7600 - loss: 0.6575 - val_accuracy: 0.8353 - val_loss: 0.4780\n",
            "Epoch 198/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7728 - loss: 0.6329 - val_accuracy: 0.8609 - val_loss: 0.4639\n",
            "Epoch 199/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.6443 - val_accuracy: 0.8514 - val_loss: 0.4682\n",
            "Epoch 200/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 0.6306 - val_accuracy: 0.8592 - val_loss: 0.4602\n",
            "Epoch 201/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7700 - loss: 0.6363 - val_accuracy: 0.8536 - val_loss: 0.4666\n",
            "Epoch 202/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7665 - loss: 0.6398 - val_accuracy: 0.8531 - val_loss: 0.4678\n",
            "Epoch 203/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7661 - loss: 0.6653 - val_accuracy: 0.8431 - val_loss: 0.4717\n",
            "Epoch 204/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7652 - loss: 0.6409 - val_accuracy: 0.8587 - val_loss: 0.4632\n",
            "Epoch 205/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7645 - loss: 0.6506 - val_accuracy: 0.8486 - val_loss: 0.4703\n",
            "Epoch 206/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7634 - loss: 0.6593 - val_accuracy: 0.8347 - val_loss: 0.4836\n",
            "Epoch 207/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.6372 - val_accuracy: 0.8570 - val_loss: 0.4634\n",
            "Epoch 208/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7719 - loss: 0.6425 - val_accuracy: 0.8598 - val_loss: 0.4613\n",
            "Epoch 209/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7661 - loss: 0.6450 - val_accuracy: 0.8408 - val_loss: 0.4771\n",
            "Epoch 210/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7689 - loss: 0.6261 - val_accuracy: 0.8408 - val_loss: 0.4732\n",
            "Epoch 211/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7656 - loss: 0.6501 - val_accuracy: 0.8542 - val_loss: 0.4672\n",
            "Epoch 212/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7741 - loss: 0.6397 - val_accuracy: 0.8631 - val_loss: 0.4565\n",
            "Epoch 213/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.6453 - val_accuracy: 0.8347 - val_loss: 0.4800\n",
            "Epoch 214/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.6398 - val_accuracy: 0.8431 - val_loss: 0.4718\n",
            "Epoch 215/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.6361 - val_accuracy: 0.8486 - val_loss: 0.4678\n",
            "Epoch 216/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7689 - loss: 0.6369 - val_accuracy: 0.8453 - val_loss: 0.4709\n",
            "Epoch 217/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7639 - loss: 0.6560 - val_accuracy: 0.8470 - val_loss: 0.4692\n",
            "Epoch 218/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7665 - loss: 0.6392 - val_accuracy: 0.8620 - val_loss: 0.4590\n",
            "Epoch 219/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.6258 - val_accuracy: 0.8631 - val_loss: 0.4538\n",
            "Epoch 220/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7754 - loss: 0.6316 - val_accuracy: 0.8381 - val_loss: 0.4773\n",
            "Epoch 221/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.6366 - val_accuracy: 0.8609 - val_loss: 0.4589\n",
            "Epoch 222/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7689 - loss: 0.6430 - val_accuracy: 0.8497 - val_loss: 0.4683\n",
            "Epoch 223/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7747 - loss: 0.6282 - val_accuracy: 0.8553 - val_loss: 0.4653\n",
            "Epoch 224/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7695 - loss: 0.6376 - val_accuracy: 0.8459 - val_loss: 0.4688\n",
            "Epoch 225/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.6339 - val_accuracy: 0.8481 - val_loss: 0.4679\n",
            "Epoch 226/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.6448 - val_accuracy: 0.8559 - val_loss: 0.4631\n",
            "Epoch 227/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7643 - loss: 0.6570 - val_accuracy: 0.8542 - val_loss: 0.4639\n",
            "Epoch 228/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7699 - loss: 0.6372 - val_accuracy: 0.8392 - val_loss: 0.4790\n",
            "Epoch 229/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7747 - loss: 0.6295 - val_accuracy: 0.8431 - val_loss: 0.4708\n",
            "Epoch 230/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7728 - loss: 0.6337 - val_accuracy: 0.8503 - val_loss: 0.4675\n",
            "Epoch 231/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7704 - loss: 0.6437 - val_accuracy: 0.8542 - val_loss: 0.4647\n",
            "Epoch 232/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7663 - loss: 0.6444 - val_accuracy: 0.8486 - val_loss: 0.4688\n",
            "Epoch 233/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7697 - loss: 0.6440 - val_accuracy: 0.8509 - val_loss: 0.4664\n",
            "Epoch 234/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.6229 - val_accuracy: 0.8548 - val_loss: 0.4606\n",
            "Epoch 235/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7665 - loss: 0.6391 - val_accuracy: 0.8475 - val_loss: 0.4686\n",
            "Epoch 236/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 0.6319 - val_accuracy: 0.8503 - val_loss: 0.4658\n",
            "Epoch 237/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7715 - loss: 0.6293 - val_accuracy: 0.8503 - val_loss: 0.4647\n",
            "Epoch 238/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7784 - loss: 0.6201 - val_accuracy: 0.8431 - val_loss: 0.4693\n",
            "Epoch 239/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7745 - loss: 0.6368 - val_accuracy: 0.8587 - val_loss: 0.4558\n",
            "Epoch 240/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.6521 - val_accuracy: 0.8587 - val_loss: 0.4584\n",
            "Epoch 241/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.6387 - val_accuracy: 0.8581 - val_loss: 0.4574\n",
            "Epoch 242/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7762 - loss: 0.6304 - val_accuracy: 0.8375 - val_loss: 0.4782\n",
            "Epoch 243/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7751 - loss: 0.6359 - val_accuracy: 0.8358 - val_loss: 0.4769\n",
            "Epoch 244/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7751 - loss: 0.6417 - val_accuracy: 0.8575 - val_loss: 0.4578\n",
            "Epoch 245/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7700 - loss: 0.6212 - val_accuracy: 0.8575 - val_loss: 0.4568\n",
            "Epoch 246/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7710 - loss: 0.6266 - val_accuracy: 0.8575 - val_loss: 0.4578\n",
            "Epoch 247/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7814 - loss: 0.6207 - val_accuracy: 0.8436 - val_loss: 0.4684\n",
            "Epoch 248/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7752 - loss: 0.6216 - val_accuracy: 0.8414 - val_loss: 0.4691\n",
            "Epoch 249/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.6325 - val_accuracy: 0.8447 - val_loss: 0.4661\n",
            "Epoch 249: early stopping\n",
            "Restoring model weights from the end of the best epoch: 219.\n",
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmpxctlx0dz\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmpxctlx0dz\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\brian\\AppData\\Local\\Temp\\tmpxctlx0dz'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2561579111952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579111184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579119440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579122128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579109648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579109264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  MLP validation accuracy=0.8631, macro F1=0.8656 | saved→point_history_classifier_mlp_20251121_015029_05.tflite\n",
            "Training LSTM sequence model (Keras)...\n",
            "Epoch 1/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.1960 - loss: 1.6022 - val_accuracy: 0.2627 - val_loss: 1.5887\n",
            "Epoch 2/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2602 - loss: 1.5718 - val_accuracy: 0.2627 - val_loss: 1.5335\n",
            "Epoch 3/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2598 - loss: 1.5038 - val_accuracy: 0.2627 - val_loss: 1.4705\n",
            "Epoch 4/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3296 - loss: 1.4654 - val_accuracy: 0.4068 - val_loss: 1.4369\n",
            "Epoch 5/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3992 - loss: 1.4321 - val_accuracy: 0.4023 - val_loss: 1.4103\n",
            "Epoch 6/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4053 - loss: 1.4042 - val_accuracy: 0.4040 - val_loss: 1.3776\n",
            "Epoch 7/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4074 - loss: 1.3827 - val_accuracy: 0.4162 - val_loss: 1.3487\n",
            "Epoch 8/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4196 - loss: 1.3641 - val_accuracy: 0.4263 - val_loss: 1.3305\n",
            "Epoch 9/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4369 - loss: 1.3377 - val_accuracy: 0.4430 - val_loss: 1.3110\n",
            "Epoch 10/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4508 - loss: 1.3186 - val_accuracy: 0.4602 - val_loss: 1.2761\n",
            "Epoch 11/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4666 - loss: 1.2965 - val_accuracy: 0.4819 - val_loss: 1.2497\n",
            "Epoch 12/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4840 - loss: 1.2655 - val_accuracy: 0.4930 - val_loss: 1.2194\n",
            "Epoch 13/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5022 - loss: 1.2292 - val_accuracy: 0.5303 - val_loss: 1.1685\n",
            "Epoch 14/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5265 - loss: 1.1931 - val_accuracy: 0.5509 - val_loss: 1.1330\n",
            "Epoch 15/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5471 - loss: 1.1643 - val_accuracy: 0.5988 - val_loss: 1.0867\n",
            "Epoch 16/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5520 - loss: 1.1331 - val_accuracy: 0.6260 - val_loss: 1.0568\n",
            "Epoch 17/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5622 - loss: 1.1198 - val_accuracy: 0.6255 - val_loss: 1.0384\n",
            "Epoch 18/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5789 - loss: 1.0861 - val_accuracy: 0.6316 - val_loss: 1.0104\n",
            "Epoch 19/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5869 - loss: 1.0605 - val_accuracy: 0.6550 - val_loss: 0.9838\n",
            "Epoch 20/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5783 - loss: 1.0604 - val_accuracy: 0.6505 - val_loss: 0.9760\n",
            "Epoch 21/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5995 - loss: 1.0359 - val_accuracy: 0.6706 - val_loss: 0.9523\n",
            "Epoch 22/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6106 - loss: 1.0240 - val_accuracy: 0.6811 - val_loss: 0.9362\n",
            "Epoch 23/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6219 - loss: 0.9885 - val_accuracy: 0.6845 - val_loss: 0.8981\n",
            "Epoch 24/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6284 - loss: 0.9833 - val_accuracy: 0.6867 - val_loss: 0.8823\n",
            "Epoch 25/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6331 - loss: 0.9600 - val_accuracy: 0.6923 - val_loss: 0.8702\n",
            "Epoch 26/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6540 - loss: 0.9319 - val_accuracy: 0.7234 - val_loss: 0.8243\n",
            "Epoch 27/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6715 - loss: 0.9068 - val_accuracy: 0.7362 - val_loss: 0.8208\n",
            "Epoch 28/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6748 - loss: 0.8927 - val_accuracy: 0.7357 - val_loss: 0.7815\n",
            "Epoch 29/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6795 - loss: 0.8759 - val_accuracy: 0.7362 - val_loss: 0.8066\n",
            "Epoch 30/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6914 - loss: 0.8521 - val_accuracy: 0.7602 - val_loss: 0.7202\n",
            "Epoch 31/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7029 - loss: 0.8329 - val_accuracy: 0.7752 - val_loss: 0.7196\n",
            "Epoch 32/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7090 - loss: 0.8105 - val_accuracy: 0.7641 - val_loss: 0.6864\n",
            "Epoch 33/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7071 - loss: 0.7937 - val_accuracy: 0.7769 - val_loss: 0.7100\n",
            "Epoch 34/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7088 - loss: 0.7990 - val_accuracy: 0.7819 - val_loss: 0.6577\n",
            "Epoch 35/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7192 - loss: 0.7788 - val_accuracy: 0.7835 - val_loss: 0.6390\n",
            "Epoch 36/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7248 - loss: 0.7562 - val_accuracy: 0.7858 - val_loss: 0.6244\n",
            "Epoch 37/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7264 - loss: 0.7543 - val_accuracy: 0.7913 - val_loss: 0.6150\n",
            "Epoch 38/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7313 - loss: 0.7405 - val_accuracy: 0.7997 - val_loss: 0.6011\n",
            "Epoch 39/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7381 - loss: 0.7268 - val_accuracy: 0.7919 - val_loss: 0.5987\n",
            "Epoch 40/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7400 - loss: 0.7196 - val_accuracy: 0.8041 - val_loss: 0.5806\n",
            "Epoch 41/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7457 - loss: 0.7023 - val_accuracy: 0.8024 - val_loss: 0.5785\n",
            "Epoch 42/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7342 - loss: 0.7202 - val_accuracy: 0.8030 - val_loss: 0.5735\n",
            "Epoch 43/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7485 - loss: 0.7074 - val_accuracy: 0.8141 - val_loss: 0.5481\n",
            "Epoch 44/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7509 - loss: 0.7016 - val_accuracy: 0.8169 - val_loss: 0.5503\n",
            "Epoch 45/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7545 - loss: 0.6810 - val_accuracy: 0.8186 - val_loss: 0.5348\n",
            "Epoch 46/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7556 - loss: 0.6819 - val_accuracy: 0.8258 - val_loss: 0.5150\n",
            "Epoch 47/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7615 - loss: 0.6541 - val_accuracy: 0.8386 - val_loss: 0.5058\n",
            "Epoch 48/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.6540 - val_accuracy: 0.8347 - val_loss: 0.5053\n",
            "Epoch 49/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7643 - loss: 0.6596 - val_accuracy: 0.8375 - val_loss: 0.4972\n",
            "Epoch 50/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7725 - loss: 0.6380 - val_accuracy: 0.8381 - val_loss: 0.4871\n",
            "Epoch 51/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7702 - loss: 0.6410 - val_accuracy: 0.8403 - val_loss: 0.4825\n",
            "Epoch 52/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7760 - loss: 0.6365 - val_accuracy: 0.8503 - val_loss: 0.4671\n",
            "Epoch 53/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7825 - loss: 0.6232 - val_accuracy: 0.8503 - val_loss: 0.4600\n",
            "Epoch 54/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7834 - loss: 0.6026 - val_accuracy: 0.8548 - val_loss: 0.4721\n",
            "Epoch 55/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7860 - loss: 0.6099 - val_accuracy: 0.8503 - val_loss: 0.4662\n",
            "Epoch 56/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.6149 - val_accuracy: 0.8464 - val_loss: 0.4671\n",
            "Epoch 57/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7886 - loss: 0.5986 - val_accuracy: 0.8581 - val_loss: 0.4371\n",
            "Epoch 58/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7895 - loss: 0.5983 - val_accuracy: 0.8587 - val_loss: 0.4353\n",
            "Epoch 59/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7945 - loss: 0.5950 - val_accuracy: 0.8642 - val_loss: 0.4269\n",
            "Epoch 60/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7983 - loss: 0.5897 - val_accuracy: 0.8453 - val_loss: 0.4656\n",
            "Epoch 61/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8016 - loss: 0.5796 - val_accuracy: 0.8642 - val_loss: 0.4205\n",
            "Epoch 62/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7875 - loss: 0.5964 - val_accuracy: 0.8670 - val_loss: 0.4238\n",
            "Epoch 63/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7905 - loss: 0.5813 - val_accuracy: 0.8581 - val_loss: 0.4317\n",
            "Epoch 64/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7983 - loss: 0.5685 - val_accuracy: 0.8631 - val_loss: 0.4154\n",
            "Epoch 65/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7997 - loss: 0.5818 - val_accuracy: 0.8681 - val_loss: 0.4130\n",
            "Epoch 66/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7977 - loss: 0.5821 - val_accuracy: 0.8648 - val_loss: 0.4149\n",
            "Epoch 67/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8111 - loss: 0.5592 - val_accuracy: 0.8692 - val_loss: 0.4088\n",
            "Epoch 68/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7994 - loss: 0.5701 - val_accuracy: 0.8687 - val_loss: 0.4074\n",
            "Epoch 69/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8085 - loss: 0.5447 - val_accuracy: 0.8748 - val_loss: 0.3952\n",
            "Epoch 70/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8062 - loss: 0.5645 - val_accuracy: 0.8676 - val_loss: 0.3981\n",
            "Epoch 71/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.5486 - val_accuracy: 0.8642 - val_loss: 0.4104\n",
            "Epoch 72/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8153 - loss: 0.5463 - val_accuracy: 0.8737 - val_loss: 0.3867\n",
            "Epoch 73/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8014 - loss: 0.5680 - val_accuracy: 0.8709 - val_loss: 0.3940\n",
            "Epoch 74/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.5522 - val_accuracy: 0.8748 - val_loss: 0.3973\n",
            "Epoch 75/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8018 - loss: 0.5518 - val_accuracy: 0.8759 - val_loss: 0.3913\n",
            "Epoch 76/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8127 - loss: 0.5374 - val_accuracy: 0.8742 - val_loss: 0.3871\n",
            "Epoch 77/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8118 - loss: 0.5339 - val_accuracy: 0.8737 - val_loss: 0.3938\n",
            "Epoch 78/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8116 - loss: 0.5557 - val_accuracy: 0.8792 - val_loss: 0.3873\n",
            "Epoch 79/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8226 - loss: 0.5400 - val_accuracy: 0.8687 - val_loss: 0.3977\n",
            "Epoch 80/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8198 - loss: 0.5282 - val_accuracy: 0.8726 - val_loss: 0.3866\n",
            "Epoch 81/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8216 - loss: 0.5376 - val_accuracy: 0.8687 - val_loss: 0.3915\n",
            "Epoch 82/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8164 - loss: 0.5331 - val_accuracy: 0.8753 - val_loss: 0.3763\n",
            "Epoch 83/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8216 - loss: 0.5269 - val_accuracy: 0.8781 - val_loss: 0.3827\n",
            "Epoch 84/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8233 - loss: 0.5215 - val_accuracy: 0.8742 - val_loss: 0.3784\n",
            "Epoch 85/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8114 - loss: 0.5465 - val_accuracy: 0.8804 - val_loss: 0.3677\n",
            "Epoch 86/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8233 - loss: 0.5207 - val_accuracy: 0.8809 - val_loss: 0.3696\n",
            "Epoch 87/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8257 - loss: 0.5222 - val_accuracy: 0.8742 - val_loss: 0.3758\n",
            "Epoch 88/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.5330 - val_accuracy: 0.8820 - val_loss: 0.3652\n",
            "Epoch 89/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8280 - loss: 0.5146 - val_accuracy: 0.8809 - val_loss: 0.3682\n",
            "Epoch 90/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8228 - loss: 0.5168 - val_accuracy: 0.8748 - val_loss: 0.3864\n",
            "Epoch 91/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8177 - loss: 0.5262 - val_accuracy: 0.8759 - val_loss: 0.3756\n",
            "Epoch 92/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8192 - loss: 0.5255 - val_accuracy: 0.8748 - val_loss: 0.3704\n",
            "Epoch 93/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8268 - loss: 0.5181 - val_accuracy: 0.8854 - val_loss: 0.3593\n",
            "Epoch 94/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8189 - loss: 0.5296 - val_accuracy: 0.8759 - val_loss: 0.3877\n",
            "Epoch 95/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8254 - loss: 0.5149 - val_accuracy: 0.8798 - val_loss: 0.3642\n",
            "Epoch 96/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8268 - loss: 0.5142 - val_accuracy: 0.8798 - val_loss: 0.3742\n",
            "Epoch 97/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8304 - loss: 0.5086 - val_accuracy: 0.8798 - val_loss: 0.3611\n",
            "Epoch 98/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8289 - loss: 0.5081 - val_accuracy: 0.8815 - val_loss: 0.3566\n",
            "Epoch 99/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8246 - loss: 0.5113 - val_accuracy: 0.8826 - val_loss: 0.3585\n",
            "Epoch 100/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8281 - loss: 0.4942 - val_accuracy: 0.8815 - val_loss: 0.3566\n",
            "Epoch 101/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8289 - loss: 0.5031 - val_accuracy: 0.8759 - val_loss: 0.3726\n",
            "Epoch 102/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8229 - loss: 0.5173 - val_accuracy: 0.8837 - val_loss: 0.3591\n",
            "Epoch 103/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8318 - loss: 0.5024 - val_accuracy: 0.8809 - val_loss: 0.3600\n",
            "Epoch 104/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8276 - loss: 0.5033 - val_accuracy: 0.8848 - val_loss: 0.3561\n",
            "Epoch 105/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8309 - loss: 0.4946 - val_accuracy: 0.8870 - val_loss: 0.3477\n",
            "Epoch 106/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8322 - loss: 0.4962 - val_accuracy: 0.8887 - val_loss: 0.3470\n",
            "Epoch 107/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8304 - loss: 0.4985 - val_accuracy: 0.8876 - val_loss: 0.3569\n",
            "Epoch 108/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8293 - loss: 0.5032 - val_accuracy: 0.8893 - val_loss: 0.3492\n",
            "Epoch 109/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8333 - loss: 0.5040 - val_accuracy: 0.8837 - val_loss: 0.3573\n",
            "Epoch 110/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8320 - loss: 0.4950 - val_accuracy: 0.8826 - val_loss: 0.3516\n",
            "Epoch 111/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8328 - loss: 0.4999 - val_accuracy: 0.8792 - val_loss: 0.3587\n",
            "Epoch 112/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8395 - loss: 0.4802 - val_accuracy: 0.8887 - val_loss: 0.3476\n",
            "Epoch 113/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8404 - loss: 0.4824 - val_accuracy: 0.8870 - val_loss: 0.3451\n",
            "Epoch 114/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8357 - loss: 0.4942 - val_accuracy: 0.8843 - val_loss: 0.3511\n",
            "Epoch 115/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8330 - loss: 0.4962 - val_accuracy: 0.8854 - val_loss: 0.3497\n",
            "Epoch 116/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8354 - loss: 0.4756 - val_accuracy: 0.8865 - val_loss: 0.3521\n",
            "Epoch 117/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8374 - loss: 0.4743 - val_accuracy: 0.8876 - val_loss: 0.3473\n",
            "Epoch 118/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8380 - loss: 0.4849 - val_accuracy: 0.8876 - val_loss: 0.3435\n",
            "Epoch 119/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8387 - loss: 0.4898 - val_accuracy: 0.8837 - val_loss: 0.3559\n",
            "Epoch 120/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8313 - loss: 0.4944 - val_accuracy: 0.8893 - val_loss: 0.3441\n",
            "Epoch 121/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8396 - loss: 0.4838 - val_accuracy: 0.8893 - val_loss: 0.3466\n",
            "Epoch 122/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8393 - loss: 0.4799 - val_accuracy: 0.8859 - val_loss: 0.3482\n",
            "Epoch 123/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8343 - loss: 0.4827 - val_accuracy: 0.8859 - val_loss: 0.3487\n",
            "Epoch 124/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8376 - loss: 0.4845 - val_accuracy: 0.8870 - val_loss: 0.3418\n",
            "Epoch 125/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8343 - loss: 0.4851 - val_accuracy: 0.8893 - val_loss: 0.3499\n",
            "Epoch 126/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8382 - loss: 0.4873 - val_accuracy: 0.8887 - val_loss: 0.3395\n",
            "Epoch 127/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8385 - loss: 0.4767 - val_accuracy: 0.8854 - val_loss: 0.3469\n",
            "Epoch 128/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8400 - loss: 0.4702 - val_accuracy: 0.8887 - val_loss: 0.3409\n",
            "Epoch 129/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8372 - loss: 0.4882 - val_accuracy: 0.8843 - val_loss: 0.3532\n",
            "Epoch 130/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8363 - loss: 0.4771 - val_accuracy: 0.8854 - val_loss: 0.3444\n",
            "Epoch 131/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8348 - loss: 0.4860 - val_accuracy: 0.8898 - val_loss: 0.3398\n",
            "Epoch 132/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8415 - loss: 0.4674 - val_accuracy: 0.8898 - val_loss: 0.3336\n",
            "Epoch 133/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8300 - loss: 0.4893 - val_accuracy: 0.8898 - val_loss: 0.3403\n",
            "Epoch 134/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8404 - loss: 0.4761 - val_accuracy: 0.8815 - val_loss: 0.3481\n",
            "Epoch 135/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8465 - loss: 0.4808 - val_accuracy: 0.8859 - val_loss: 0.3549\n",
            "Epoch 136/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8389 - loss: 0.4760 - val_accuracy: 0.8870 - val_loss: 0.3434\n",
            "Epoch 137/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8387 - loss: 0.4751 - val_accuracy: 0.8920 - val_loss: 0.3465\n",
            "Epoch 138/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8370 - loss: 0.4843 - val_accuracy: 0.8870 - val_loss: 0.3423\n",
            "Epoch 139/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8344 - loss: 0.4820 - val_accuracy: 0.8920 - val_loss: 0.3385\n",
            "Epoch 140/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8450 - loss: 0.4573 - val_accuracy: 0.8904 - val_loss: 0.3330\n",
            "Epoch 141/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8448 - loss: 0.4602 - val_accuracy: 0.8898 - val_loss: 0.3340\n",
            "Epoch 142/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8359 - loss: 0.4775 - val_accuracy: 0.8920 - val_loss: 0.3395\n",
            "Epoch 143/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8372 - loss: 0.4663 - val_accuracy: 0.8887 - val_loss: 0.3359\n",
            "Epoch 144/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8422 - loss: 0.4601 - val_accuracy: 0.8898 - val_loss: 0.3343\n",
            "Epoch 145/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8391 - loss: 0.4643 - val_accuracy: 0.8854 - val_loss: 0.3430\n",
            "Epoch 146/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8439 - loss: 0.4603 - val_accuracy: 0.8926 - val_loss: 0.3342\n",
            "Epoch 147/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8406 - loss: 0.4734 - val_accuracy: 0.8887 - val_loss: 0.3380\n",
            "Epoch 148/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8389 - loss: 0.4687 - val_accuracy: 0.8937 - val_loss: 0.3284\n",
            "Epoch 149/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8428 - loss: 0.4569 - val_accuracy: 0.8909 - val_loss: 0.3342\n",
            "Epoch 150/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8391 - loss: 0.4700 - val_accuracy: 0.8909 - val_loss: 0.3300\n",
            "Epoch 151/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8439 - loss: 0.4595 - val_accuracy: 0.8937 - val_loss: 0.3287\n",
            "Epoch 152/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8402 - loss: 0.4707 - val_accuracy: 0.8920 - val_loss: 0.3284\n",
            "Epoch 153/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8428 - loss: 0.4608 - val_accuracy: 0.8904 - val_loss: 0.3321\n",
            "Epoch 154/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8467 - loss: 0.4461 - val_accuracy: 0.8943 - val_loss: 0.3303\n",
            "Epoch 155/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8495 - loss: 0.4524 - val_accuracy: 0.8898 - val_loss: 0.3302\n",
            "Epoch 156/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8480 - loss: 0.4557 - val_accuracy: 0.8915 - val_loss: 0.3311\n",
            "Epoch 157/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8441 - loss: 0.4688 - val_accuracy: 0.8965 - val_loss: 0.3278\n",
            "Epoch 158/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8413 - loss: 0.4682 - val_accuracy: 0.8870 - val_loss: 0.3363\n",
            "Epoch 159/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8445 - loss: 0.4630 - val_accuracy: 0.8876 - val_loss: 0.3405\n",
            "Epoch 160/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8458 - loss: 0.4613 - val_accuracy: 0.8915 - val_loss: 0.3320\n",
            "Epoch 161/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8460 - loss: 0.4521 - val_accuracy: 0.8926 - val_loss: 0.3247\n",
            "Epoch 162/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8502 - loss: 0.4558 - val_accuracy: 0.8898 - val_loss: 0.3341\n",
            "Epoch 163/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8424 - loss: 0.4652 - val_accuracy: 0.8943 - val_loss: 0.3243\n",
            "Epoch 164/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8471 - loss: 0.4481 - val_accuracy: 0.8926 - val_loss: 0.3221\n",
            "Epoch 165/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8473 - loss: 0.4598 - val_accuracy: 0.8943 - val_loss: 0.3376\n",
            "Epoch 166/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8391 - loss: 0.4719 - val_accuracy: 0.8887 - val_loss: 0.3356\n",
            "Epoch 167/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8441 - loss: 0.4525 - val_accuracy: 0.8904 - val_loss: 0.3310\n",
            "Epoch 168/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8463 - loss: 0.4577 - val_accuracy: 0.8920 - val_loss: 0.3244\n",
            "Epoch 169/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8445 - loss: 0.4608 - val_accuracy: 0.8909 - val_loss: 0.3246\n",
            "Epoch 170/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.4563 - val_accuracy: 0.8904 - val_loss: 0.3253\n",
            "Epoch 171/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8463 - loss: 0.4673 - val_accuracy: 0.8887 - val_loss: 0.3285\n",
            "Epoch 172/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8495 - loss: 0.4509 - val_accuracy: 0.8954 - val_loss: 0.3261\n",
            "Epoch 173/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8521 - loss: 0.4420 - val_accuracy: 0.8870 - val_loss: 0.3365\n",
            "Epoch 174/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8556 - loss: 0.4377 - val_accuracy: 0.8870 - val_loss: 0.3382\n",
            "Epoch 175/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8491 - loss: 0.4501 - val_accuracy: 0.8915 - val_loss: 0.3277\n",
            "Epoch 176/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8530 - loss: 0.4475 - val_accuracy: 0.8943 - val_loss: 0.3201\n",
            "Epoch 177/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8486 - loss: 0.4504 - val_accuracy: 0.8920 - val_loss: 0.3304\n",
            "Epoch 178/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8524 - loss: 0.4326 - val_accuracy: 0.8898 - val_loss: 0.3391\n",
            "Epoch 179/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8508 - loss: 0.4533 - val_accuracy: 0.8870 - val_loss: 0.3361\n",
            "Epoch 180/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8563 - loss: 0.4389 - val_accuracy: 0.8932 - val_loss: 0.3246\n",
            "Epoch 181/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8576 - loss: 0.4350 - val_accuracy: 0.8937 - val_loss: 0.3169\n",
            "Epoch 182/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8526 - loss: 0.4358 - val_accuracy: 0.8932 - val_loss: 0.3243\n",
            "Epoch 183/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8517 - loss: 0.4463 - val_accuracy: 0.8937 - val_loss: 0.3213\n",
            "Epoch 184/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8465 - loss: 0.4616 - val_accuracy: 0.8948 - val_loss: 0.3275\n",
            "Epoch 185/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8502 - loss: 0.4546 - val_accuracy: 0.8937 - val_loss: 0.3253\n",
            "Epoch 186/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8534 - loss: 0.4367 - val_accuracy: 0.8932 - val_loss: 0.3211\n",
            "Epoch 187/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8460 - loss: 0.4406 - val_accuracy: 0.8926 - val_loss: 0.3183\n",
            "Epoch 188/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8487 - loss: 0.4429 - val_accuracy: 0.8959 - val_loss: 0.3205\n",
            "Epoch 189/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8504 - loss: 0.4538 - val_accuracy: 0.8898 - val_loss: 0.3303\n",
            "Epoch 190/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8532 - loss: 0.4374 - val_accuracy: 0.8932 - val_loss: 0.3221\n",
            "Epoch 191/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8510 - loss: 0.4434 - val_accuracy: 0.8926 - val_loss: 0.3276\n",
            "Epoch 192/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8530 - loss: 0.4332 - val_accuracy: 0.8932 - val_loss: 0.3224\n",
            "Epoch 193/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8560 - loss: 0.4320 - val_accuracy: 0.8898 - val_loss: 0.3248\n",
            "Epoch 194/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8458 - loss: 0.4548 - val_accuracy: 0.8965 - val_loss: 0.3193\n",
            "Epoch 195/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8567 - loss: 0.4336 - val_accuracy: 0.8909 - val_loss: 0.3241\n",
            "Epoch 196/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8513 - loss: 0.4429 - val_accuracy: 0.8932 - val_loss: 0.3181\n",
            "Epoch 197/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8565 - loss: 0.4288 - val_accuracy: 0.8926 - val_loss: 0.3223\n",
            "Epoch 198/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8604 - loss: 0.4207 - val_accuracy: 0.8932 - val_loss: 0.3206\n",
            "Epoch 199/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8549 - loss: 0.4300 - val_accuracy: 0.8954 - val_loss: 0.3185\n",
            "Epoch 200/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.4246 - val_accuracy: 0.8937 - val_loss: 0.3247\n",
            "Epoch 201/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8541 - loss: 0.4312 - val_accuracy: 0.8948 - val_loss: 0.3116\n",
            "Epoch 202/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8508 - loss: 0.4482 - val_accuracy: 0.8893 - val_loss: 0.3246\n",
            "Epoch 203/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8519 - loss: 0.4306 - val_accuracy: 0.8904 - val_loss: 0.3219\n",
            "Epoch 204/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8617 - loss: 0.4154 - val_accuracy: 0.8959 - val_loss: 0.3133\n",
            "Epoch 205/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8541 - loss: 0.4343 - val_accuracy: 0.8971 - val_loss: 0.3102\n",
            "Epoch 206/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8580 - loss: 0.4244 - val_accuracy: 0.8959 - val_loss: 0.3134\n",
            "Epoch 207/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8521 - loss: 0.4293 - val_accuracy: 0.8926 - val_loss: 0.3155\n",
            "Epoch 208/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8576 - loss: 0.4217 - val_accuracy: 0.8948 - val_loss: 0.3138\n",
            "Epoch 209/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8556 - loss: 0.4327 - val_accuracy: 0.8904 - val_loss: 0.3311\n",
            "Epoch 210/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8521 - loss: 0.4359 - val_accuracy: 0.8920 - val_loss: 0.3211\n",
            "Epoch 211/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8591 - loss: 0.4262 - val_accuracy: 0.8943 - val_loss: 0.3169\n",
            "Epoch 212/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8497 - loss: 0.4358 - val_accuracy: 0.8937 - val_loss: 0.3206\n",
            "Epoch 213/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8532 - loss: 0.4256 - val_accuracy: 0.8943 - val_loss: 0.3172\n",
            "Epoch 214/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8560 - loss: 0.4360 - val_accuracy: 0.8887 - val_loss: 0.3213\n",
            "Epoch 215/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8562 - loss: 0.4387 - val_accuracy: 0.8932 - val_loss: 0.3140\n",
            "Epoch 216/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8632 - loss: 0.4347 - val_accuracy: 0.8926 - val_loss: 0.3148\n",
            "Epoch 217/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8604 - loss: 0.4150 - val_accuracy: 0.8954 - val_loss: 0.3108\n",
            "Epoch 218/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8573 - loss: 0.4223 - val_accuracy: 0.8943 - val_loss: 0.3117\n",
            "Epoch 219/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8589 - loss: 0.4261 - val_accuracy: 0.8909 - val_loss: 0.3176\n",
            "Epoch 220/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8567 - loss: 0.4176 - val_accuracy: 0.8937 - val_loss: 0.3141\n",
            "Epoch 221/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8586 - loss: 0.4291 - val_accuracy: 0.8959 - val_loss: 0.3104\n",
            "Epoch 222/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8627 - loss: 0.4153 - val_accuracy: 0.8909 - val_loss: 0.3200\n",
            "Epoch 223/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8610 - loss: 0.4176 - val_accuracy: 0.8932 - val_loss: 0.3171\n",
            "Epoch 224/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8513 - loss: 0.4304 - val_accuracy: 0.8943 - val_loss: 0.3161\n",
            "Epoch 225/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8578 - loss: 0.4221 - val_accuracy: 0.8971 - val_loss: 0.3198\n",
            "Epoch 226/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8615 - loss: 0.4212 - val_accuracy: 0.8943 - val_loss: 0.3076\n",
            "Epoch 227/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8591 - loss: 0.4141 - val_accuracy: 0.8959 - val_loss: 0.3155\n",
            "Epoch 228/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8563 - loss: 0.4338 - val_accuracy: 0.8920 - val_loss: 0.3219\n",
            "Epoch 229/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8601 - loss: 0.4169 - val_accuracy: 0.8965 - val_loss: 0.3096\n",
            "Epoch 230/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8588 - loss: 0.4169 - val_accuracy: 0.8965 - val_loss: 0.3061\n",
            "Epoch 231/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8601 - loss: 0.4288 - val_accuracy: 0.8954 - val_loss: 0.3098\n",
            "Epoch 232/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8619 - loss: 0.4086 - val_accuracy: 0.8948 - val_loss: 0.3094\n",
            "Epoch 233/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8597 - loss: 0.4199 - val_accuracy: 0.8954 - val_loss: 0.3087\n",
            "Epoch 234/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8593 - loss: 0.4194 - val_accuracy: 0.8904 - val_loss: 0.3209\n",
            "Epoch 235/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8604 - loss: 0.4161 - val_accuracy: 0.8954 - val_loss: 0.3087\n",
            "Epoch 236/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8651 - loss: 0.4128 - val_accuracy: 0.8881 - val_loss: 0.3209\n",
            "Epoch 237/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8606 - loss: 0.4153 - val_accuracy: 0.8948 - val_loss: 0.3054\n",
            "Epoch 238/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8621 - loss: 0.4088 - val_accuracy: 0.8932 - val_loss: 0.3152\n",
            "Epoch 239/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8641 - loss: 0.4117 - val_accuracy: 0.8971 - val_loss: 0.3058\n",
            "Epoch 240/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8586 - loss: 0.4134 - val_accuracy: 0.8982 - val_loss: 0.3062\n",
            "Epoch 241/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8649 - loss: 0.4083 - val_accuracy: 0.8965 - val_loss: 0.3048\n",
            "Epoch 242/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8604 - loss: 0.4149 - val_accuracy: 0.8937 - val_loss: 0.3189\n",
            "Epoch 243/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8604 - loss: 0.4181 - val_accuracy: 0.8954 - val_loss: 0.3050\n",
            "Epoch 244/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8640 - loss: 0.4153 - val_accuracy: 0.8932 - val_loss: 0.3113\n",
            "Epoch 245/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8460 - loss: 0.4712 - val_accuracy: 0.8954 - val_loss: 0.3194\n",
            "Epoch 246/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8608 - loss: 0.4102 - val_accuracy: 0.8993 - val_loss: 0.3037\n",
            "Epoch 247/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8654 - loss: 0.4121 - val_accuracy: 0.8943 - val_loss: 0.3115\n",
            "Epoch 248/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8632 - loss: 0.4201 - val_accuracy: 0.9021 - val_loss: 0.3056\n",
            "Epoch 249/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8701 - loss: 0.4097 - val_accuracy: 0.8987 - val_loss: 0.3026\n",
            "Epoch 250/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.4107 - val_accuracy: 0.8948 - val_loss: 0.3115\n",
            "Epoch 251/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8615 - loss: 0.4165 - val_accuracy: 0.8948 - val_loss: 0.3178\n",
            "Epoch 252/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8719 - loss: 0.4067 - val_accuracy: 0.8993 - val_loss: 0.3022\n",
            "Epoch 253/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8656 - loss: 0.4070 - val_accuracy: 0.8948 - val_loss: 0.3121\n",
            "Epoch 254/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8673 - loss: 0.4060 - val_accuracy: 0.8926 - val_loss: 0.3179\n",
            "Epoch 255/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8625 - loss: 0.4100 - val_accuracy: 0.8971 - val_loss: 0.3082\n",
            "Epoch 256/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8667 - loss: 0.4028 - val_accuracy: 0.8987 - val_loss: 0.2993\n",
            "Epoch 257/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8653 - loss: 0.4034 - val_accuracy: 0.8943 - val_loss: 0.3079\n",
            "Epoch 258/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8664 - loss: 0.4062 - val_accuracy: 0.8887 - val_loss: 0.3247\n",
            "Epoch 259/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8725 - loss: 0.4048 - val_accuracy: 0.8959 - val_loss: 0.3022\n",
            "Epoch 260/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8660 - loss: 0.4112 - val_accuracy: 0.8959 - val_loss: 0.3041\n",
            "Epoch 261/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8643 - loss: 0.4093 - val_accuracy: 0.8998 - val_loss: 0.3021\n",
            "Epoch 262/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8667 - loss: 0.4061 - val_accuracy: 0.8976 - val_loss: 0.2991\n",
            "Epoch 263/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8684 - loss: 0.3988 - val_accuracy: 0.8954 - val_loss: 0.3044\n",
            "Epoch 264/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8669 - loss: 0.4085 - val_accuracy: 0.9037 - val_loss: 0.2942\n",
            "Epoch 265/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8667 - loss: 0.4026 - val_accuracy: 0.8982 - val_loss: 0.3037\n",
            "Epoch 266/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8682 - loss: 0.3939 - val_accuracy: 0.8932 - val_loss: 0.3111\n",
            "Epoch 267/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8697 - loss: 0.3939 - val_accuracy: 0.8971 - val_loss: 0.2990\n",
            "Epoch 268/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8677 - loss: 0.4027 - val_accuracy: 0.8965 - val_loss: 0.2993\n",
            "Epoch 269/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8710 - loss: 0.3955 - val_accuracy: 0.8993 - val_loss: 0.2952\n",
            "Epoch 270/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8682 - loss: 0.4015 - val_accuracy: 0.8993 - val_loss: 0.2976\n",
            "Epoch 271/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8660 - loss: 0.4098 - val_accuracy: 0.8954 - val_loss: 0.3075\n",
            "Epoch 272/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8688 - loss: 0.4015 - val_accuracy: 0.8954 - val_loss: 0.3039\n",
            "Epoch 273/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8710 - loss: 0.3977 - val_accuracy: 0.8943 - val_loss: 0.3070\n",
            "Epoch 274/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8731 - loss: 0.3919 - val_accuracy: 0.8959 - val_loss: 0.3053\n",
            "Epoch 275/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8677 - loss: 0.3965 - val_accuracy: 0.8948 - val_loss: 0.3126\n",
            "Epoch 276/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8649 - loss: 0.4051 - val_accuracy: 0.8943 - val_loss: 0.3068\n",
            "Epoch 277/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8701 - loss: 0.4031 - val_accuracy: 0.8959 - val_loss: 0.3059\n",
            "Epoch 278/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8712 - loss: 0.3998 - val_accuracy: 0.8998 - val_loss: 0.2936\n",
            "Epoch 279/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8703 - loss: 0.3925 - val_accuracy: 0.8987 - val_loss: 0.2964\n",
            "Epoch 280/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8614 - loss: 0.4110 - val_accuracy: 0.8987 - val_loss: 0.2931\n",
            "Epoch 281/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8666 - loss: 0.3981 - val_accuracy: 0.8982 - val_loss: 0.2988\n",
            "Epoch 282/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8651 - loss: 0.3975 - val_accuracy: 0.8998 - val_loss: 0.2958\n",
            "Epoch 283/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8712 - loss: 0.3920 - val_accuracy: 0.9032 - val_loss: 0.2928\n",
            "Epoch 284/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8688 - loss: 0.3985 - val_accuracy: 0.8987 - val_loss: 0.2992\n",
            "Epoch 285/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8706 - loss: 0.4029 - val_accuracy: 0.9015 - val_loss: 0.2907\n",
            "Epoch 286/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8627 - loss: 0.3936 - val_accuracy: 0.8987 - val_loss: 0.2969\n",
            "Epoch 287/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8677 - loss: 0.3954 - val_accuracy: 0.8965 - val_loss: 0.3055\n",
            "Epoch 288/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8738 - loss: 0.3878 - val_accuracy: 0.8965 - val_loss: 0.2960\n",
            "Epoch 289/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8751 - loss: 0.3829 - val_accuracy: 0.8915 - val_loss: 0.3145\n",
            "Epoch 290/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8693 - loss: 0.3956 - val_accuracy: 0.8965 - val_loss: 0.3019\n",
            "Epoch 291/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8692 - loss: 0.3909 - val_accuracy: 0.8982 - val_loss: 0.3040\n",
            "Epoch 292/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8682 - loss: 0.4016 - val_accuracy: 0.8937 - val_loss: 0.3105\n",
            "Epoch 293/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8721 - loss: 0.3860 - val_accuracy: 0.8954 - val_loss: 0.3013\n",
            "Epoch 294/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8597 - loss: 0.4263 - val_accuracy: 0.8948 - val_loss: 0.3086\n",
            "Epoch 295/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8684 - loss: 0.3918 - val_accuracy: 0.8976 - val_loss: 0.2961\n",
            "Epoch 296/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8734 - loss: 0.3944 - val_accuracy: 0.9009 - val_loss: 0.2918\n",
            "Epoch 297/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8727 - loss: 0.3895 - val_accuracy: 0.9054 - val_loss: 0.2869\n",
            "Epoch 298/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.3738 - val_accuracy: 0.9009 - val_loss: 0.2937\n",
            "Epoch 299/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8697 - loss: 0.3866 - val_accuracy: 0.8959 - val_loss: 0.3048\n",
            "Epoch 300/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8718 - loss: 0.3915 - val_accuracy: 0.8971 - val_loss: 0.2961\n",
            "Epoch 301/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8706 - loss: 0.3950 - val_accuracy: 0.8965 - val_loss: 0.2963\n",
            "Epoch 302/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8736 - loss: 0.3977 - val_accuracy: 0.9015 - val_loss: 0.2936\n",
            "Epoch 303/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8760 - loss: 0.3810 - val_accuracy: 0.8993 - val_loss: 0.2928\n",
            "Epoch 304/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8716 - loss: 0.3875 - val_accuracy: 0.9032 - val_loss: 0.2892\n",
            "Epoch 305/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8744 - loss: 0.3820 - val_accuracy: 0.8982 - val_loss: 0.3012\n",
            "Epoch 306/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8723 - loss: 0.3921 - val_accuracy: 0.8998 - val_loss: 0.2964\n",
            "Epoch 307/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8658 - loss: 0.3926 - val_accuracy: 0.8993 - val_loss: 0.2888\n",
            "Epoch 308/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8781 - loss: 0.3796 - val_accuracy: 0.8987 - val_loss: 0.2932\n",
            "Epoch 309/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8769 - loss: 0.3760 - val_accuracy: 0.8959 - val_loss: 0.3022\n",
            "Epoch 310/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8788 - loss: 0.3770 - val_accuracy: 0.9043 - val_loss: 0.2844\n",
            "Epoch 311/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8760 - loss: 0.3836 - val_accuracy: 0.9009 - val_loss: 0.2930\n",
            "Epoch 312/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8747 - loss: 0.3847 - val_accuracy: 0.9026 - val_loss: 0.2875\n",
            "Epoch 313/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8669 - loss: 0.3967 - val_accuracy: 0.8932 - val_loss: 0.2938\n",
            "Epoch 314/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8716 - loss: 0.3841 - val_accuracy: 0.8993 - val_loss: 0.2935\n",
            "Epoch 315/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8794 - loss: 0.3757 - val_accuracy: 0.9004 - val_loss: 0.2904\n",
            "Epoch 316/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8756 - loss: 0.3774 - val_accuracy: 0.9004 - val_loss: 0.2919\n",
            "Epoch 317/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8771 - loss: 0.3834 - val_accuracy: 0.9032 - val_loss: 0.2883\n",
            "Epoch 318/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.3856 - val_accuracy: 0.9026 - val_loss: 0.2865\n",
            "Epoch 319/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8786 - loss: 0.3691 - val_accuracy: 0.8982 - val_loss: 0.3035\n",
            "Epoch 320/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8647 - loss: 0.4021 - val_accuracy: 0.9021 - val_loss: 0.2898\n",
            "Epoch 321/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8721 - loss: 0.3859 - val_accuracy: 0.8987 - val_loss: 0.2931\n",
            "Epoch 322/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8719 - loss: 0.3786 - val_accuracy: 0.8993 - val_loss: 0.2948\n",
            "Epoch 323/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8762 - loss: 0.3874 - val_accuracy: 0.9065 - val_loss: 0.2849\n",
            "Epoch 324/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8727 - loss: 0.3850 - val_accuracy: 0.9032 - val_loss: 0.2875\n",
            "Epoch 325/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8729 - loss: 0.3801 - val_accuracy: 0.8982 - val_loss: 0.3017\n",
            "Epoch 326/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8721 - loss: 0.3883 - val_accuracy: 0.8920 - val_loss: 0.3030\n",
            "Epoch 327/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8753 - loss: 0.3885 - val_accuracy: 0.8998 - val_loss: 0.2889\n",
            "Epoch 328/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8786 - loss: 0.3667 - val_accuracy: 0.9037 - val_loss: 0.2815\n",
            "Epoch 329/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8768 - loss: 0.3732 - val_accuracy: 0.9043 - val_loss: 0.2839\n",
            "Epoch 330/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8729 - loss: 0.3783 - val_accuracy: 0.9021 - val_loss: 0.2909\n",
            "Epoch 331/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8710 - loss: 0.3844 - val_accuracy: 0.9021 - val_loss: 0.2909\n",
            "Epoch 332/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8790 - loss: 0.3637 - val_accuracy: 0.8993 - val_loss: 0.2949\n",
            "Epoch 333/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8762 - loss: 0.3756 - val_accuracy: 0.9043 - val_loss: 0.2852\n",
            "Epoch 334/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8844 - loss: 0.3603 - val_accuracy: 0.9032 - val_loss: 0.2843\n",
            "Epoch 335/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8781 - loss: 0.3722 - val_accuracy: 0.9004 - val_loss: 0.2889\n",
            "Epoch 336/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8779 - loss: 0.3682 - val_accuracy: 0.9043 - val_loss: 0.2881\n",
            "Epoch 337/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8810 - loss: 0.3775 - val_accuracy: 0.9037 - val_loss: 0.2780\n",
            "Epoch 338/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8838 - loss: 0.3620 - val_accuracy: 0.8959 - val_loss: 0.2950\n",
            "Epoch 339/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8775 - loss: 0.3785 - val_accuracy: 0.9009 - val_loss: 0.2875\n",
            "Epoch 340/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8808 - loss: 0.3696 - val_accuracy: 0.9026 - val_loss: 0.2919\n",
            "Epoch 341/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8734 - loss: 0.3749 - val_accuracy: 0.9021 - val_loss: 0.2857\n",
            "Epoch 342/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8792 - loss: 0.3709 - val_accuracy: 0.8937 - val_loss: 0.3101\n",
            "Epoch 343/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8758 - loss: 0.3663 - val_accuracy: 0.9021 - val_loss: 0.2845\n",
            "Epoch 344/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8807 - loss: 0.3697 - val_accuracy: 0.9054 - val_loss: 0.2815\n",
            "Epoch 345/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8794 - loss: 0.3721 - val_accuracy: 0.8998 - val_loss: 0.2914\n",
            "Epoch 346/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8814 - loss: 0.3587 - val_accuracy: 0.9009 - val_loss: 0.2858\n",
            "Epoch 347/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8831 - loss: 0.3721 - val_accuracy: 0.8998 - val_loss: 0.2825\n",
            "Epoch 348/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8797 - loss: 0.3721 - val_accuracy: 0.9021 - val_loss: 0.2801\n",
            "Epoch 349/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8790 - loss: 0.3693 - val_accuracy: 0.9043 - val_loss: 0.2769\n",
            "Epoch 350/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.3730 - val_accuracy: 0.9009 - val_loss: 0.2817\n",
            "Epoch 351/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8714 - loss: 0.3832 - val_accuracy: 0.9009 - val_loss: 0.2858\n",
            "Epoch 352/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8764 - loss: 0.3746 - val_accuracy: 0.9032 - val_loss: 0.2850\n",
            "Epoch 353/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.3682 - val_accuracy: 0.9076 - val_loss: 0.2752\n",
            "Epoch 354/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8827 - loss: 0.3656 - val_accuracy: 0.9043 - val_loss: 0.2771\n",
            "Epoch 355/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8768 - loss: 0.3736 - val_accuracy: 0.8987 - val_loss: 0.2864\n",
            "Epoch 356/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8762 - loss: 0.3778 - val_accuracy: 0.9048 - val_loss: 0.2780\n",
            "Epoch 357/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8755 - loss: 0.3806 - val_accuracy: 0.9009 - val_loss: 0.2848\n",
            "Epoch 358/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8812 - loss: 0.3624 - val_accuracy: 0.9015 - val_loss: 0.2909\n",
            "Epoch 359/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8777 - loss: 0.3709 - val_accuracy: 0.9037 - val_loss: 0.2736\n",
            "Epoch 360/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8825 - loss: 0.3583 - val_accuracy: 0.8982 - val_loss: 0.2978\n",
            "Epoch 361/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8760 - loss: 0.3779 - val_accuracy: 0.9054 - val_loss: 0.2740\n",
            "Epoch 362/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8810 - loss: 0.3642 - val_accuracy: 0.8943 - val_loss: 0.2973\n",
            "Epoch 363/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8745 - loss: 0.3719 - val_accuracy: 0.9054 - val_loss: 0.2756\n",
            "Epoch 364/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8768 - loss: 0.3699 - val_accuracy: 0.8965 - val_loss: 0.2970\n",
            "Epoch 365/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8795 - loss: 0.3733 - val_accuracy: 0.9032 - val_loss: 0.2849\n",
            "Epoch 366/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8784 - loss: 0.3688 - val_accuracy: 0.9060 - val_loss: 0.2823\n",
            "Epoch 367/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8810 - loss: 0.3612 - val_accuracy: 0.9026 - val_loss: 0.2830\n",
            "Epoch 368/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8760 - loss: 0.3655 - val_accuracy: 0.9037 - val_loss: 0.2775\n",
            "Epoch 369/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8734 - loss: 0.3890 - val_accuracy: 0.9043 - val_loss: 0.2800\n",
            "Epoch 370/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8784 - loss: 0.3745 - val_accuracy: 0.9037 - val_loss: 0.2883\n",
            "Epoch 371/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8808 - loss: 0.3649 - val_accuracy: 0.9071 - val_loss: 0.2767\n",
            "Epoch 372/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8840 - loss: 0.3701 - val_accuracy: 0.8998 - val_loss: 0.2974\n",
            "Epoch 373/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8821 - loss: 0.3687 - val_accuracy: 0.9060 - val_loss: 0.2765\n",
            "Epoch 374/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8857 - loss: 0.3576 - val_accuracy: 0.8987 - val_loss: 0.2969\n",
            "Epoch 375/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8812 - loss: 0.3694 - val_accuracy: 0.9082 - val_loss: 0.2808\n",
            "Epoch 376/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8833 - loss: 0.3566 - val_accuracy: 0.9082 - val_loss: 0.2749\n",
            "Epoch 377/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8818 - loss: 0.3647 - val_accuracy: 0.8987 - val_loss: 0.2906\n",
            "Epoch 378/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8827 - loss: 0.3518 - val_accuracy: 0.9048 - val_loss: 0.2743\n",
            "Epoch 379/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8803 - loss: 0.3621 - val_accuracy: 0.9071 - val_loss: 0.2762\n",
            "Epoch 380/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8797 - loss: 0.3764 - val_accuracy: 0.9004 - val_loss: 0.2945\n",
            "Epoch 381/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8827 - loss: 0.3629 - val_accuracy: 0.8965 - val_loss: 0.3013\n",
            "Epoch 382/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8805 - loss: 0.3627 - val_accuracy: 0.9082 - val_loss: 0.2762\n",
            "Epoch 383/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8799 - loss: 0.3637 - val_accuracy: 0.9037 - val_loss: 0.2805\n",
            "Epoch 384/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8812 - loss: 0.3592 - val_accuracy: 0.9043 - val_loss: 0.2760\n",
            "Epoch 385/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8820 - loss: 0.3557 - val_accuracy: 0.8904 - val_loss: 0.3045\n",
            "Epoch 386/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8781 - loss: 0.3641 - val_accuracy: 0.9065 - val_loss: 0.2737\n",
            "Epoch 387/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8788 - loss: 0.3665 - val_accuracy: 0.9037 - val_loss: 0.2838\n",
            "Epoch 388/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8818 - loss: 0.3549 - val_accuracy: 0.9054 - val_loss: 0.2850\n",
            "Epoch 389/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8803 - loss: 0.3653 - val_accuracy: 0.9043 - val_loss: 0.2836\n",
            "Epoch 389: early stopping\n",
            "Restoring model weights from the end of the best epoch: 359.\n",
            "  LSTM validation accuracy=0.9037, macro F1=0.9098 | saved→point_history_classifier_lstm_20251121_015250_06.keras\n",
            "Training graph-aware Transformer...\n",
            "Epoch 1/400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:870: UserWarning: Gradients do not exist for variables ['graph_attn_0/graph_bias_weight', 'graph_attn_1/graph_bias_weight'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.4761 - loss: 1.2706 - val_accuracy: 0.5927 - val_loss: 0.9664\n",
            "Epoch 2/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5980 - loss: 0.9636 - val_accuracy: 0.6138 - val_loss: 0.8733\n",
            "Epoch 3/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6295 - loss: 0.8692 - val_accuracy: 0.6316 - val_loss: 0.8238\n",
            "Epoch 4/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6505 - loss: 0.8193 - val_accuracy: 0.6850 - val_loss: 0.7396\n",
            "Epoch 5/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6832 - loss: 0.7556 - val_accuracy: 0.7129 - val_loss: 0.6956\n",
            "Epoch 6/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7027 - loss: 0.7234 - val_accuracy: 0.7040 - val_loss: 0.7016\n",
            "Epoch 7/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7313 - loss: 0.6742 - val_accuracy: 0.6639 - val_loss: 0.8483\n",
            "Epoch 8/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7322 - loss: 0.6766 - val_accuracy: 0.7201 - val_loss: 0.6832\n",
            "Epoch 9/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7509 - loss: 0.6265 - val_accuracy: 0.7702 - val_loss: 0.5998\n",
            "Epoch 10/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7509 - loss: 0.6398 - val_accuracy: 0.7657 - val_loss: 0.6016\n",
            "Epoch 11/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7537 - loss: 0.6148 - val_accuracy: 0.7635 - val_loss: 0.6123\n",
            "Epoch 12/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7775 - loss: 0.5793 - val_accuracy: 0.7835 - val_loss: 0.5789\n",
            "Epoch 13/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7845 - loss: 0.5675 - val_accuracy: 0.7835 - val_loss: 0.5596\n",
            "Epoch 14/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7914 - loss: 0.5508 - val_accuracy: 0.8052 - val_loss: 0.5238\n",
            "Epoch 15/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7873 - loss: 0.5593 - val_accuracy: 0.7974 - val_loss: 0.5437\n",
            "Epoch 16/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8012 - loss: 0.5407 - val_accuracy: 0.7952 - val_loss: 0.5136\n",
            "Epoch 17/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8157 - loss: 0.5045 - val_accuracy: 0.8114 - val_loss: 0.5062\n",
            "Epoch 18/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8140 - loss: 0.5054 - val_accuracy: 0.8075 - val_loss: 0.5159\n",
            "Epoch 19/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8114 - loss: 0.5111 - val_accuracy: 0.8191 - val_loss: 0.4969\n",
            "Epoch 20/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8248 - loss: 0.4816 - val_accuracy: 0.7841 - val_loss: 0.5760\n",
            "Epoch 21/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8398 - loss: 0.4642 - val_accuracy: 0.8470 - val_loss: 0.4336\n",
            "Epoch 22/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8361 - loss: 0.4524 - val_accuracy: 0.8542 - val_loss: 0.4286\n",
            "Epoch 23/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8552 - loss: 0.4230 - val_accuracy: 0.8286 - val_loss: 0.5068\n",
            "Epoch 24/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8619 - loss: 0.4141 - val_accuracy: 0.8681 - val_loss: 0.3825\n",
            "Epoch 25/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8632 - loss: 0.4050 - val_accuracy: 0.8848 - val_loss: 0.3651\n",
            "Epoch 26/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8758 - loss: 0.3780 - val_accuracy: 0.8815 - val_loss: 0.3558\n",
            "Epoch 27/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8786 - loss: 0.3637 - val_accuracy: 0.8887 - val_loss: 0.3394\n",
            "Epoch 28/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8896 - loss: 0.3410 - val_accuracy: 0.9037 - val_loss: 0.3063\n",
            "Epoch 29/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8836 - loss: 0.3540 - val_accuracy: 0.8843 - val_loss: 0.3728\n",
            "Epoch 30/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8903 - loss: 0.3348 - val_accuracy: 0.8904 - val_loss: 0.3388\n",
            "Epoch 31/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8925 - loss: 0.3208 - val_accuracy: 0.8887 - val_loss: 0.3446\n",
            "Epoch 32/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8818 - loss: 0.3722 - val_accuracy: 0.8976 - val_loss: 0.3222\n",
            "Epoch 33/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9050 - loss: 0.3066 - val_accuracy: 0.9110 - val_loss: 0.2754\n",
            "Epoch 34/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8981 - loss: 0.3184 - val_accuracy: 0.8976 - val_loss: 0.3208\n",
            "Epoch 35/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8977 - loss: 0.3180 - val_accuracy: 0.9043 - val_loss: 0.3016\n",
            "Epoch 36/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8872 - loss: 0.3485 - val_accuracy: 0.9021 - val_loss: 0.3079\n",
            "Epoch 37/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8944 - loss: 0.3203 - val_accuracy: 0.9060 - val_loss: 0.3040\n",
            "Epoch 38/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9048 - loss: 0.2910 - val_accuracy: 0.9137 - val_loss: 0.2831\n",
            "Epoch 39/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9078 - loss: 0.2883 - val_accuracy: 0.9115 - val_loss: 0.2738\n",
            "Epoch 40/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9052 - loss: 0.2829 - val_accuracy: 0.9043 - val_loss: 0.3103\n",
            "Epoch 41/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9109 - loss: 0.2855 - val_accuracy: 0.9132 - val_loss: 0.2678\n",
            "Epoch 42/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9139 - loss: 0.2625 - val_accuracy: 0.9087 - val_loss: 0.2917\n",
            "Epoch 43/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9079 - loss: 0.2809 - val_accuracy: 0.9115 - val_loss: 0.2729\n",
            "Epoch 44/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9133 - loss: 0.2682 - val_accuracy: 0.9126 - val_loss: 0.2836\n",
            "Epoch 45/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9126 - loss: 0.2669 - val_accuracy: 0.9071 - val_loss: 0.3020\n",
            "Epoch 46/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9100 - loss: 0.2707 - val_accuracy: 0.9176 - val_loss: 0.2665\n",
            "Epoch 47/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9135 - loss: 0.2674 - val_accuracy: 0.9154 - val_loss: 0.2654\n",
            "Epoch 48/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9163 - loss: 0.2629 - val_accuracy: 0.9193 - val_loss: 0.2574\n",
            "Epoch 49/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9139 - loss: 0.2685 - val_accuracy: 0.9165 - val_loss: 0.2649\n",
            "Epoch 50/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9198 - loss: 0.2532 - val_accuracy: 0.9215 - val_loss: 0.2589\n",
            "Epoch 51/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9219 - loss: 0.2529 - val_accuracy: 0.9210 - val_loss: 0.2509\n",
            "Epoch 52/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9102 - loss: 0.2905 - val_accuracy: 0.9149 - val_loss: 0.2671\n",
            "Epoch 53/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9189 - loss: 0.2475 - val_accuracy: 0.9160 - val_loss: 0.2558\n",
            "Epoch 54/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9196 - loss: 0.2573 - val_accuracy: 0.9249 - val_loss: 0.2473\n",
            "Epoch 55/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9193 - loss: 0.2470 - val_accuracy: 0.9249 - val_loss: 0.2431\n",
            "Epoch 56/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9076 - loss: 0.2802 - val_accuracy: 0.9282 - val_loss: 0.2362\n",
            "Epoch 57/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9226 - loss: 0.2423 - val_accuracy: 0.9282 - val_loss: 0.2377\n",
            "Epoch 58/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9206 - loss: 0.2446 - val_accuracy: 0.9204 - val_loss: 0.2440\n",
            "Epoch 59/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9211 - loss: 0.2442 - val_accuracy: 0.9282 - val_loss: 0.2378\n",
            "Epoch 60/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9259 - loss: 0.2370 - val_accuracy: 0.9199 - val_loss: 0.2432\n",
            "Epoch 61/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9265 - loss: 0.2404 - val_accuracy: 0.9243 - val_loss: 0.2397\n",
            "Epoch 62/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9211 - loss: 0.2425 - val_accuracy: 0.9110 - val_loss: 0.2842\n",
            "Epoch 63/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9219 - loss: 0.2401 - val_accuracy: 0.9215 - val_loss: 0.2490\n",
            "Epoch 64/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9284 - loss: 0.2321 - val_accuracy: 0.9221 - val_loss: 0.2482\n",
            "Epoch 65/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9258 - loss: 0.2411 - val_accuracy: 0.9226 - val_loss: 0.2513\n",
            "Epoch 66/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9219 - loss: 0.2383 - val_accuracy: 0.9271 - val_loss: 0.2379\n",
            "Epoch 67/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9258 - loss: 0.2372 - val_accuracy: 0.9037 - val_loss: 0.2945\n",
            "Epoch 68/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9135 - loss: 0.2676 - val_accuracy: 0.9204 - val_loss: 0.2638\n",
            "Epoch 69/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9323 - loss: 0.2241 - val_accuracy: 0.9254 - val_loss: 0.2282\n",
            "Epoch 70/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9313 - loss: 0.2245 - val_accuracy: 0.9288 - val_loss: 0.2317\n",
            "Epoch 71/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9302 - loss: 0.2239 - val_accuracy: 0.9193 - val_loss: 0.2398\n",
            "Epoch 72/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9230 - loss: 0.2445 - val_accuracy: 0.9204 - val_loss: 0.2549\n",
            "Epoch 73/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9319 - loss: 0.2153 - val_accuracy: 0.9299 - val_loss: 0.2306\n",
            "Epoch 74/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9245 - loss: 0.2380 - val_accuracy: 0.9154 - val_loss: 0.2675\n",
            "Epoch 75/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9252 - loss: 0.2276 - val_accuracy: 0.9176 - val_loss: 0.2688\n",
            "Epoch 76/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9154 - loss: 0.2588 - val_accuracy: 0.9176 - val_loss: 0.2602\n",
            "Epoch 77/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9245 - loss: 0.2361 - val_accuracy: 0.9226 - val_loss: 0.2488\n",
            "Epoch 78/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9310 - loss: 0.2161 - val_accuracy: 0.9154 - val_loss: 0.2717\n",
            "Epoch 79/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9272 - loss: 0.2275 - val_accuracy: 0.9304 - val_loss: 0.2337\n",
            "Epoch 80/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9339 - loss: 0.2145 - val_accuracy: 0.9310 - val_loss: 0.2159\n",
            "Epoch 81/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9324 - loss: 0.2134 - val_accuracy: 0.9349 - val_loss: 0.2149\n",
            "Epoch 82/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9306 - loss: 0.2174 - val_accuracy: 0.9299 - val_loss: 0.2391\n",
            "Epoch 83/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9332 - loss: 0.2107 - val_accuracy: 0.9299 - val_loss: 0.2179\n",
            "Epoch 84/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9332 - loss: 0.2139 - val_accuracy: 0.9310 - val_loss: 0.2182\n",
            "Epoch 85/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9315 - loss: 0.2122 - val_accuracy: 0.9282 - val_loss: 0.2345\n",
            "Epoch 86/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9298 - loss: 0.2164 - val_accuracy: 0.9210 - val_loss: 0.2554\n",
            "Epoch 87/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9356 - loss: 0.2066 - val_accuracy: 0.9210 - val_loss: 0.2423\n",
            "Epoch 88/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9117 - loss: 0.2642 - val_accuracy: 0.9304 - val_loss: 0.2210\n",
            "Epoch 89/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9345 - loss: 0.2073 - val_accuracy: 0.9327 - val_loss: 0.2227\n",
            "Epoch 90/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9343 - loss: 0.2075 - val_accuracy: 0.9316 - val_loss: 0.2185\n",
            "Epoch 91/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9345 - loss: 0.2142 - val_accuracy: 0.9327 - val_loss: 0.2110\n",
            "Epoch 92/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9345 - loss: 0.2019 - val_accuracy: 0.9354 - val_loss: 0.2219\n",
            "Epoch 93/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9336 - loss: 0.2053 - val_accuracy: 0.9165 - val_loss: 0.2554\n",
            "Epoch 94/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9165 - loss: 0.2554 - val_accuracy: 0.9304 - val_loss: 0.2313\n",
            "Epoch 95/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9315 - loss: 0.2125 - val_accuracy: 0.9332 - val_loss: 0.2231\n",
            "Epoch 96/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9341 - loss: 0.2095 - val_accuracy: 0.9282 - val_loss: 0.2356\n",
            "Epoch 97/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9375 - loss: 0.2046 - val_accuracy: 0.9260 - val_loss: 0.2224\n",
            "Epoch 98/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9371 - loss: 0.2042 - val_accuracy: 0.9304 - val_loss: 0.2262\n",
            "Epoch 99/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9354 - loss: 0.2037 - val_accuracy: 0.9260 - val_loss: 0.2463\n",
            "Epoch 100/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9308 - loss: 0.2093 - val_accuracy: 0.9316 - val_loss: 0.2117\n",
            "Epoch 101/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9362 - loss: 0.1965 - val_accuracy: 0.9321 - val_loss: 0.2175\n",
            "Epoch 102/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9113 - loss: 0.2756 - val_accuracy: 0.9316 - val_loss: 0.2310\n",
            "Epoch 103/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9347 - loss: 0.2061 - val_accuracy: 0.9349 - val_loss: 0.2095\n",
            "Epoch 104/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9337 - loss: 0.2046 - val_accuracy: 0.9332 - val_loss: 0.2095\n",
            "Epoch 105/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9389 - loss: 0.1933 - val_accuracy: 0.9366 - val_loss: 0.2108\n",
            "Epoch 106/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9389 - loss: 0.1930 - val_accuracy: 0.9249 - val_loss: 0.2512\n",
            "Epoch 107/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9326 - loss: 0.2085 - val_accuracy: 0.9299 - val_loss: 0.2338\n",
            "Epoch 108/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9386 - loss: 0.1957 - val_accuracy: 0.9332 - val_loss: 0.2183\n",
            "Epoch 109/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9375 - loss: 0.2023 - val_accuracy: 0.9293 - val_loss: 0.2245\n",
            "Epoch 110/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9239 - loss: 0.2317 - val_accuracy: 0.9115 - val_loss: 0.2600\n",
            "Epoch 111/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9375 - loss: 0.1967 - val_accuracy: 0.9343 - val_loss: 0.2336\n",
            "Epoch 112/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9371 - loss: 0.1911 - val_accuracy: 0.9388 - val_loss: 0.2006\n",
            "Epoch 113/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9408 - loss: 0.1915 - val_accuracy: 0.9316 - val_loss: 0.2211\n",
            "Epoch 114/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9399 - loss: 0.1965 - val_accuracy: 0.9254 - val_loss: 0.2444\n",
            "Epoch 115/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9317 - loss: 0.2159 - val_accuracy: 0.9282 - val_loss: 0.2358\n",
            "Epoch 116/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9367 - loss: 0.1961 - val_accuracy: 0.9327 - val_loss: 0.2232\n",
            "Epoch 117/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9410 - loss: 0.1835 - val_accuracy: 0.9215 - val_loss: 0.2460\n",
            "Epoch 118/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9259 - loss: 0.2299 - val_accuracy: 0.9316 - val_loss: 0.2270\n",
            "Epoch 119/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9388 - loss: 0.1948 - val_accuracy: 0.9288 - val_loss: 0.2264\n",
            "Epoch 120/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9367 - loss: 0.2008 - val_accuracy: 0.9366 - val_loss: 0.2190\n",
            "Epoch 121/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9441 - loss: 0.1799 - val_accuracy: 0.9338 - val_loss: 0.2103\n",
            "Epoch 122/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9417 - loss: 0.1843 - val_accuracy: 0.9343 - val_loss: 0.2097\n",
            "Epoch 123/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9371 - loss: 0.1937 - val_accuracy: 0.9327 - val_loss: 0.2121\n",
            "Epoch 124/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9436 - loss: 0.1767 - val_accuracy: 0.9338 - val_loss: 0.2179\n",
            "Epoch 125/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9373 - loss: 0.1940 - val_accuracy: 0.9282 - val_loss: 0.2375\n",
            "Epoch 126/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9393 - loss: 0.1878 - val_accuracy: 0.9360 - val_loss: 0.2141\n",
            "Epoch 127/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9447 - loss: 0.1799 - val_accuracy: 0.9321 - val_loss: 0.2214\n",
            "Epoch 127: early stopping\n",
            "Restoring model weights from the end of the best epoch: 112.\n",
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmplqq0zmy6\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmplqq0zmy6\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\brian\\AppData\\Local\\Temp\\tmplqq0zmy6'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32), dtype=tf.float32, name='flat_input')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2561579116176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579120400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579119632: TensorSpec(shape=(1, 16, 64), dtype=tf.float32, name=None)\n",
            "  2561579122320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579115408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579121744: TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)\n",
            "  2561780320976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780334032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780327696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780326544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780325200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780327504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780326160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780325008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780327888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780325968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780333456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780322128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780320400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561780325392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740421840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740422416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740406864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740422608: TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)\n",
            "  2561740422992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740422224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740422800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740421072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561740421264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652950800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652949840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652951184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652950608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652951760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652949072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652951376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652950032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652950992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652952336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652950224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652952912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  Graph Transformer validation accuracy=0.9388, macro F1=0.9444 | saved→point_history_classifier_graph_transformer_20251121_015515_07.tflite\n",
            "Training Transformer self-attention model...\n",
            "Epoch 1/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.4220 - loss: 1.5080 - val_accuracy: 0.6349 - val_loss: 1.0021\n",
            "Epoch 2/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.6464 - loss: 0.8776 - val_accuracy: 0.7067 - val_loss: 0.7073\n",
            "Epoch 3/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.7068 - loss: 0.7095 - val_accuracy: 0.7718 - val_loss: 0.5980\n",
            "Epoch 4/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.7674 - loss: 0.5902 - val_accuracy: 0.7346 - val_loss: 0.5999\n",
            "Epoch 5/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.7847 - loss: 0.5395 - val_accuracy: 0.8386 - val_loss: 0.4445\n",
            "Epoch 6/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8157 - loss: 0.4758 - val_accuracy: 0.8453 - val_loss: 0.4052\n",
            "Epoch 7/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8305 - loss: 0.4472 - val_accuracy: 0.8870 - val_loss: 0.3507\n",
            "Epoch 8/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8530 - loss: 0.4058 - val_accuracy: 0.8726 - val_loss: 0.3606\n",
            "Epoch 9/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8558 - loss: 0.4021 - val_accuracy: 0.8926 - val_loss: 0.3463\n",
            "Epoch 10/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8762 - loss: 0.3634 - val_accuracy: 0.8709 - val_loss: 0.3371\n",
            "Epoch 11/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8736 - loss: 0.3472 - val_accuracy: 0.8893 - val_loss: 0.3243\n",
            "Epoch 12/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8840 - loss: 0.3284 - val_accuracy: 0.8815 - val_loss: 0.3108\n",
            "Epoch 13/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.8695 - loss: 0.3483 - val_accuracy: 0.8887 - val_loss: 0.2799\n",
            "Epoch 14/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8792 - loss: 0.3241 - val_accuracy: 0.8971 - val_loss: 0.2579\n",
            "Epoch 15/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8966 - loss: 0.2854 - val_accuracy: 0.8692 - val_loss: 0.3006\n",
            "Epoch 16/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9000 - loss: 0.2671 - val_accuracy: 0.8659 - val_loss: 0.3159\n",
            "Epoch 17/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8979 - loss: 0.2718 - val_accuracy: 0.9154 - val_loss: 0.2173\n",
            "Epoch 18/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9079 - loss: 0.2485 - val_accuracy: 0.9366 - val_loss: 0.2128\n",
            "Epoch 19/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9135 - loss: 0.2373 - val_accuracy: 0.9427 - val_loss: 0.1874\n",
            "Epoch 20/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9224 - loss: 0.2132 - val_accuracy: 0.9349 - val_loss: 0.2097\n",
            "Epoch 21/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9182 - loss: 0.2161 - val_accuracy: 0.9098 - val_loss: 0.2110\n",
            "Epoch 22/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9159 - loss: 0.2250 - val_accuracy: 0.9093 - val_loss: 0.2015\n",
            "Epoch 23/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9211 - loss: 0.2136 - val_accuracy: 0.9254 - val_loss: 0.1961\n",
            "Epoch 24/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9272 - loss: 0.1917 - val_accuracy: 0.9494 - val_loss: 0.1726\n",
            "Epoch 25/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9462 - loss: 0.1628 - val_accuracy: 0.9193 - val_loss: 0.2197\n",
            "Epoch 26/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9226 - loss: 0.1996 - val_accuracy: 0.9488 - val_loss: 0.1557\n",
            "Epoch 27/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9373 - loss: 0.1845 - val_accuracy: 0.9494 - val_loss: 0.1521\n",
            "Epoch 28/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9430 - loss: 0.1586 - val_accuracy: 0.9377 - val_loss: 0.1797\n",
            "Epoch 29/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9445 - loss: 0.1510 - val_accuracy: 0.9460 - val_loss: 0.1673\n",
            "Epoch 30/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9376 - loss: 0.1775 - val_accuracy: 0.9494 - val_loss: 0.1430\n",
            "Epoch 31/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9451 - loss: 0.1581 - val_accuracy: 0.9527 - val_loss: 0.1415\n",
            "Epoch 32/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9575 - loss: 0.1294 - val_accuracy: 0.9588 - val_loss: 0.1344\n",
            "Epoch 33/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9523 - loss: 0.1358 - val_accuracy: 0.9544 - val_loss: 0.1441\n",
            "Epoch 34/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9547 - loss: 0.1419 - val_accuracy: 0.9499 - val_loss: 0.1480\n",
            "Epoch 35/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9499 - loss: 0.1427 - val_accuracy: 0.9527 - val_loss: 0.1345\n",
            "Epoch 36/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9556 - loss: 0.1324 - val_accuracy: 0.9521 - val_loss: 0.1420\n",
            "Epoch 37/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9555 - loss: 0.1337 - val_accuracy: 0.9627 - val_loss: 0.1181\n",
            "Epoch 38/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9595 - loss: 0.1211 - val_accuracy: 0.9688 - val_loss: 0.1333\n",
            "Epoch 39/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9625 - loss: 0.1188 - val_accuracy: 0.9627 - val_loss: 0.1172\n",
            "Epoch 40/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9538 - loss: 0.1338 - val_accuracy: 0.9488 - val_loss: 0.1583\n",
            "Epoch 41/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9534 - loss: 0.1382 - val_accuracy: 0.9622 - val_loss: 0.1193\n",
            "Epoch 42/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9646 - loss: 0.1071 - val_accuracy: 0.9622 - val_loss: 0.1265\n",
            "Epoch 43/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9655 - loss: 0.1027 - val_accuracy: 0.9260 - val_loss: 0.1409\n",
            "Epoch 44/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9671 - loss: 0.1009 - val_accuracy: 0.9655 - val_loss: 0.1012\n",
            "Epoch 45/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9627 - loss: 0.1086 - val_accuracy: 0.9738 - val_loss: 0.1062\n",
            "Epoch 46/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9620 - loss: 0.1111 - val_accuracy: 0.9560 - val_loss: 0.1303\n",
            "Epoch 47/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9569 - loss: 0.1246 - val_accuracy: 0.9338 - val_loss: 0.2012\n",
            "Epoch 48/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9579 - loss: 0.1206 - val_accuracy: 0.9627 - val_loss: 0.1214\n",
            "Epoch 49/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9633 - loss: 0.1090 - val_accuracy: 0.9627 - val_loss: 0.1359\n",
            "Epoch 50/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9627 - loss: 0.1085 - val_accuracy: 0.9711 - val_loss: 0.0978\n",
            "Epoch 51/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9697 - loss: 0.0921 - val_accuracy: 0.9722 - val_loss: 0.0912\n",
            "Epoch 52/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9677 - loss: 0.0942 - val_accuracy: 0.9688 - val_loss: 0.1061\n",
            "Epoch 53/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9664 - loss: 0.0928 - val_accuracy: 0.9661 - val_loss: 0.1096\n",
            "Epoch 54/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9610 - loss: 0.1127 - val_accuracy: 0.9633 - val_loss: 0.1108\n",
            "Epoch 55/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9644 - loss: 0.1009 - val_accuracy: 0.9666 - val_loss: 0.1167\n",
            "Epoch 56/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9703 - loss: 0.0859 - val_accuracy: 0.9627 - val_loss: 0.1098\n",
            "Epoch 57/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9718 - loss: 0.0868 - val_accuracy: 0.9738 - val_loss: 0.0955\n",
            "Epoch 58/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9577 - loss: 0.1131 - val_accuracy: 0.9677 - val_loss: 0.1061\n",
            "Epoch 59/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9530 - loss: 0.1312 - val_accuracy: 0.9638 - val_loss: 0.1130\n",
            "Epoch 60/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9499 - loss: 0.1512 - val_accuracy: 0.9727 - val_loss: 0.0925\n",
            "Epoch 61/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9710 - loss: 0.0861 - val_accuracy: 0.9666 - val_loss: 0.1254\n",
            "Epoch 62/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9757 - loss: 0.0746 - val_accuracy: 0.9750 - val_loss: 0.0781\n",
            "Epoch 63/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9779 - loss: 0.0667 - val_accuracy: 0.9638 - val_loss: 0.1304\n",
            "Epoch 64/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9636 - loss: 0.1008 - val_accuracy: 0.9482 - val_loss: 0.1401\n",
            "Epoch 65/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9688 - loss: 0.0867 - val_accuracy: 0.9683 - val_loss: 0.0907\n",
            "Epoch 66/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9688 - loss: 0.0879 - val_accuracy: 0.9677 - val_loss: 0.1150\n",
            "Epoch 67/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9671 - loss: 0.0948 - val_accuracy: 0.9761 - val_loss: 0.0857\n",
            "Epoch 68/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9781 - loss: 0.0642 - val_accuracy: 0.9766 - val_loss: 0.0886\n",
            "Epoch 69/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9785 - loss: 0.0620 - val_accuracy: 0.9733 - val_loss: 0.0912\n",
            "Epoch 70/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9738 - loss: 0.0763 - val_accuracy: 0.9583 - val_loss: 0.1327\n",
            "Epoch 71/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9642 - loss: 0.1006 - val_accuracy: 0.9699 - val_loss: 0.1000\n",
            "Epoch 72/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9683 - loss: 0.0989 - val_accuracy: 0.9733 - val_loss: 0.0901\n",
            "Epoch 73/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9757 - loss: 0.0702 - val_accuracy: 0.9705 - val_loss: 0.1050\n",
            "Epoch 74/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9761 - loss: 0.0727 - val_accuracy: 0.9716 - val_loss: 0.0888\n",
            "Epoch 75/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9757 - loss: 0.0700 - val_accuracy: 0.9722 - val_loss: 0.0957\n",
            "Epoch 76/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9800 - loss: 0.0599 - val_accuracy: 0.9805 - val_loss: 0.0730\n",
            "Epoch 77/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9770 - loss: 0.0691 - val_accuracy: 0.9761 - val_loss: 0.0844\n",
            "Epoch 78/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9770 - loss: 0.0681 - val_accuracy: 0.9727 - val_loss: 0.0966\n",
            "Epoch 79/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9729 - loss: 0.0786 - val_accuracy: 0.9811 - val_loss: 0.0746\n",
            "Epoch 80/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9671 - loss: 0.0909 - val_accuracy: 0.9599 - val_loss: 0.1694\n",
            "Epoch 81/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9720 - loss: 0.0820 - val_accuracy: 0.9699 - val_loss: 0.0975\n",
            "Epoch 82/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9761 - loss: 0.0682 - val_accuracy: 0.9616 - val_loss: 0.1195\n",
            "Epoch 83/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9722 - loss: 0.0764 - val_accuracy: 0.9683 - val_loss: 0.0983\n",
            "Epoch 84/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9783 - loss: 0.0665 - val_accuracy: 0.9683 - val_loss: 0.0893\n",
            "Epoch 85/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9814 - loss: 0.0569 - val_accuracy: 0.9716 - val_loss: 0.0916\n",
            "Epoch 86/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9831 - loss: 0.0535 - val_accuracy: 0.9672 - val_loss: 0.1062\n",
            "Epoch 87/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9775 - loss: 0.0644 - val_accuracy: 0.9744 - val_loss: 0.0968\n",
            "Epoch 88/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9775 - loss: 0.0674 - val_accuracy: 0.9750 - val_loss: 0.0853\n",
            "Epoch 89/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9731 - loss: 0.0715 - val_accuracy: 0.9649 - val_loss: 0.1020\n",
            "Epoch 90/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9792 - loss: 0.0588 - val_accuracy: 0.9727 - val_loss: 0.1003\n",
            "Epoch 91/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9781 - loss: 0.0660 - val_accuracy: 0.9421 - val_loss: 0.1466\n",
            "Epoch 92/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9746 - loss: 0.0710 - val_accuracy: 0.9649 - val_loss: 0.1183\n",
            "Epoch 93/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9401 - loss: 0.1917 - val_accuracy: 0.9349 - val_loss: 0.2116\n",
            "Epoch 94/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.9523 - loss: 0.1374 - val_accuracy: 0.9627 - val_loss: 0.1260\n",
            "Epoch 95/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9694 - loss: 0.0882 - val_accuracy: 0.9677 - val_loss: 0.1014\n",
            "Epoch 96/400\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9772 - loss: 0.0682 - val_accuracy: 0.9727 - val_loss: 0.0871\n",
            "Epoch 96: early stopping\n",
            "Restoring model weights from the end of the best epoch: 76.\n",
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmpaalrplpe\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\brian\\AppData\\Local\\Temp\\tmpaalrplpe\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\brian\\AppData\\Local\\Temp\\tmpaalrplpe'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2561579113488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579114640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579115024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579115216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579114832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561579114448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652954064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652953680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652956560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652954256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652955600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652954832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652951952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652954640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652955408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652953296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652955984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652949264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652955216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652953488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652953872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652954448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652955792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652949648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652949456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561652956176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373697040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373696080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373699728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373696464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373698768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373697808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373696656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373699344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373697616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2561373699920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  Transformer validation accuracy=0.9805, macro F1=0.9795 | saved→point_history_classifier_transformer_self_attention_20251121_015810_08.tflite\n",
            "Training XGBoost classifier with grid search...\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "  Best params: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200} | CV accuracy=0.9651\n",
            "  XGBoost validation accuracy=0.9766, macro F1=0.9771 | saved→point_history_classifier_xgboost_20251121_015824_09.joblib\n",
            "Finished training all model families.\n"
          ]
        }
      ],
      "source": [
        "if TRAIN_MODELS:\n",
        "    TRAINED_JOBLIB_MODELS.clear()\n",
        "    TRAINED_TFLITE_MODELS.clear()\n",
        "    TRAINING_RUN_RECORDS.clear()\n",
        "\n",
        "    baseline_artifacts = train_classical_baselines(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "    mlp_artifacts = train_mlp_classifier(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "    lstm_artifact = train_lstm_classifier(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "    graph_artifact = train_graph_transformer(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "    self_attn_artifact = train_self_attention_transformer(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "    xgb_artifact = train_xgboost_classifier(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "\n",
        "    print(\"Finished training all model families.\")\n",
        "else:\n",
        "    print(\"Training disabled via TRAIN_MODELS flag – skipping model retraining.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "155e5ab2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>artifact_type</th>\n",
              "      <th>artifact_path</th>\n",
              "      <th>train_time_s</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Baseline::RandomForest</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>0.682276</td>\n",
              "      <td>0.983862</td>\n",
              "      <td>0.984557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TransformerSelfAttention</td>\n",
              "      <td>tflite</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_transformer_self_attention_20251121_015810_08.tflite</td>\n",
              "      <td>171.304270</td>\n",
              "      <td>0.980523</td>\n",
              "      <td>0.979472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_xgboost_20251121_015824_09.joblib</td>\n",
              "      <td>11.714263</td>\n",
              "      <td>0.976628</td>\n",
              "      <td>0.977133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Baseline::KNeighbors</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_kneighbors_20251121_014943_02.joblib</td>\n",
              "      <td>0.002980</td>\n",
              "      <td>0.965498</td>\n",
              "      <td>0.966845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GraphTransformer</td>\n",
              "      <td>tflite</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_graph_transformer_20251121_015515_07.tflite</td>\n",
              "      <td>143.676344</td>\n",
              "      <td>0.938787</td>\n",
              "      <td>0.944362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Baseline::SVC_RBF</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_svc_rbf_20251121_014945_03.joblib</td>\n",
              "      <td>1.964263</td>\n",
              "      <td>0.905954</td>\n",
              "      <td>0.910973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>keras</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_lstm_20251121_015250_06.keras</td>\n",
              "      <td>139.059242</td>\n",
              "      <td>0.903728</td>\n",
              "      <td>0.909771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MLP</td>\n",
              "      <td>tflite</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_mlp_20251121_015029_05.tflite</td>\n",
              "      <td>41.974416</td>\n",
              "      <td>0.863105</td>\n",
              "      <td>0.865581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Baseline::LogisticRegression</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_logisticregression_20251121_014943_01.joblib</td>\n",
              "      <td>0.067984</td>\n",
              "      <td>0.215915</td>\n",
              "      <td>0.073381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline::LinearSVC</td>\n",
              "      <td>joblib</td>\n",
              "      <td>c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_linearsvc_20251121_014943_00.joblib</td>\n",
              "      <td>0.101448</td>\n",
              "      <td>0.214246</td>\n",
              "      <td>0.071912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     model_name artifact_type  \\\n",
              "4        Baseline::RandomForest        joblib   \n",
              "8      TransformerSelfAttention        tflite   \n",
              "9                       XGBoost        joblib   \n",
              "2          Baseline::KNeighbors        joblib   \n",
              "7              GraphTransformer        tflite   \n",
              "3             Baseline::SVC_RBF        joblib   \n",
              "6                          LSTM         keras   \n",
              "5                           MLP        tflite   \n",
              "1  Baseline::LogisticRegression        joblib   \n",
              "0           Baseline::LinearSVC        joblib   \n",
              "\n",
              "                                                                                                                                                   artifact_path  \\\n",
              "4                c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "8  c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_transformer_self_attention_20251121_015810_08.tflite   \n",
              "9                     c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_xgboost_20251121_015824_09.joblib   \n",
              "2                  c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_kneighbors_20251121_014943_02.joblib   \n",
              "7           c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_graph_transformer_20251121_015515_07.tflite   \n",
              "3                     c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_svc_rbf_20251121_014945_03.joblib   \n",
              "6                         c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_lstm_20251121_015250_06.keras   \n",
              "5                         c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_mlp_20251121_015029_05.tflite   \n",
              "1          c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_logisticregression_20251121_014943_01.joblib   \n",
              "0                   c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\models\\point_history\\point_history_classifier_linearsvc_20251121_014943_00.joblib   \n",
              "\n",
              "   train_time_s  val_accuracy  val_macro_f1  \n",
              "4      0.682276      0.983862      0.984557  \n",
              "8    171.304270      0.980523      0.979472  \n",
              "9     11.714263      0.976628      0.977133  \n",
              "2      0.002980      0.965498      0.966845  \n",
              "7    143.676344      0.938787      0.944362  \n",
              "3      1.964263      0.905954      0.910973  \n",
              "6    139.059242      0.903728      0.909771  \n",
              "5     41.974416      0.863105      0.865581  \n",
              "1      0.067984      0.215915      0.073381  \n",
              "0      0.101448      0.214246      0.071912  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if TRAINING_RUN_RECORDS:\n",
        "    training_df = pd.DataFrame(TRAINING_RUN_RECORDS)\n",
        "    display(training_df.sort_values(by=[\"val_macro_f1\", \"val_accuracy\"], ascending=False, na_position=\"last\"))\n",
        "else:\n",
        "    print(\"No new training records captured in this session.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "d5abe9e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 6 joblib, 3 TFLite, and 1 Keras models for evaluation.\n"
          ]
        }
      ],
      "source": [
        "if TRAINED_JOBLIB_MODELS or TRAINED_TFLITE_MODELS or TRAINED_KERAS_MODELS:\n",
        "    JOBLIB_MODELS = TRAINED_JOBLIB_MODELS\n",
        "    TFLITE_MODELS = TRAINED_TFLITE_MODELS\n",
        "    KERAS_MODELS = TRAINED_KERAS_MODELS\n",
        "else:\n",
        "    JOBLIB_MODELS = sorted(MODEL_DIR.glob(\"*.joblib\"))\n",
        "    TFLITE_MODELS = sorted(MODEL_DIR.glob(\"*.tflite\"))\n",
        "    keras_candidates = sorted(MODEL_DIR.glob(\"*.keras\"))\n",
        "    KERAS_MODELS = []\n",
        "    for path_candidate in keras_candidates:\n",
        "        has_tflite = path_candidate.with_suffix(\".tflite\").exists()\n",
        "        if not has_tflite:\n",
        "            KERAS_MODELS.append(path_candidate)\n",
        "\n",
        "print(f\"Detected {len(JOBLIB_MODELS)} joblib, {len(TFLITE_MODELS)} TFLite, and {len(KERAS_MODELS)} Keras models for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "7280eed3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_label(label_id):\n",
        "    return f\"{label_id}: {label_map.get(label_id, str(label_id))}\"\n",
        "\n",
        "def format_labels(label_ids):\n",
        "    return [describe_label(idx) for idx in label_ids]\n",
        "\n",
        "def compute_custom_metrics(y_true, y_pred, similar_group):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    relaxed_correct = []\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        if t in similar_group:\n",
        "            relaxed_correct.append(p in similar_group)\n",
        "        else:\n",
        "            relaxed_correct.append(p == t)\n",
        "    relaxed_accuracy = float(np.mean(relaxed_correct))\n",
        "    return relaxed_accuracy\n",
        "\n",
        "def mcnemars_test(y_true, y_pred_a, y_pred_b):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred_a = np.asarray(y_pred_a)\n",
        "    y_pred_b = np.asarray(y_pred_b)\n",
        "\n",
        "    correct_a = y_pred_a == y_true\n",
        "    correct_b = y_pred_b == y_true\n",
        "\n",
        "    b = np.sum(correct_a & ~correct_b)\n",
        "    c = np.sum(~correct_a & correct_b)\n",
        "    n = b + c\n",
        "    if n == 0:\n",
        "        return np.nan, np.nan, b, c\n",
        "\n",
        "    chi2_stat = (abs(b - c) - 1) ** 2 / n\n",
        "    p_value = chi2.sf(chi2_stat, df=1)\n",
        "    return chi2_stat, p_value, b, c\n",
        "\n",
        "def plot_confusion_matrix(cm, label_strings, title=\"Confusion Matrix\"):\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    im = ax.imshow(cm, cmap=\"Blues\")\n",
        "    ax.set_xticks(range(len(label_strings)))\n",
        "    ax.set_yticks(range(len(label_strings)))\n",
        "    ax.set_xticklabels(label_strings, rotation=45, ha=\"right\")\n",
        "    ax.set_yticklabels(label_strings)\n",
        "    ax.set_ylabel(\"True label\")\n",
        "    ax.set_xlabel(\"Predicted label\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "    thresh = cm.max() / 2.0 if cm.any() else 0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(\n",
        "                j,\n",
        "                i,\n",
        "                cm[i, j],\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "            )\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_two_confusion_matrices(cm_a, labels_a, name_a, cm_b, labels_b, name_b):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    for ax, cm, labels, title in zip(\n",
        "        axes,\n",
        "        [cm_a, cm_b],\n",
        "        [labels_a, labels_b],\n",
        "        [name_a, name_b],\n",
        "    ):\n",
        "        im = ax.imshow(cm, cmap=\"Blues\")\n",
        "        ax.set_xticks(range(len(labels)))\n",
        "        ax.set_yticks(range(len(labels)))\n",
        "        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
        "        ax.set_yticklabels(labels)\n",
        "        ax.set_ylabel(\"True label\")\n",
        "        ax.set_xlabel(\"Predicted label\")\n",
        "        ax.set_title(title)\n",
        "        thresh = cm.max() / 2.0 if cm.any() else 0\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(\n",
        "                    j,\n",
        "                    i,\n",
        "                    cm[i, j],\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                )\n",
        "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def estimate_parameter_count(model):\n",
        "    seen = set()\n",
        "    total = 0\n",
        "    stack = [model]\n",
        "    while stack:\n",
        "        obj = stack.pop()\n",
        "        if obj is None:\n",
        "            continue\n",
        "        obj_id = id(obj)\n",
        "        if obj_id in seen:\n",
        "            continue\n",
        "        seen.add(obj_id)\n",
        "\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            total += obj.size\n",
        "            continue\n",
        "        if isinstance(obj, (list, tuple, set)):\n",
        "            stack.extend(obj)\n",
        "            continue\n",
        "        if isinstance(obj, dict):\n",
        "            stack.extend(obj.values())\n",
        "            continue\n",
        "\n",
        "        for attr in (\n",
        "            \"coef_\",\n",
        "            \"intercept_\",\n",
        "            \"support_vectors_\",\n",
        "            \"dual_coef_\",\n",
        "            \"rho_\",\n",
        "            \"feature_importances_\",\n",
        "            \"estimators_\",\n",
        "            \"tree_\",\n",
        "            \"value\",\n",
        "        ):\n",
        "            if hasattr(obj, attr):\n",
        "                stack.append(getattr(obj, attr))\n",
        "\n",
        "        if hasattr(obj, \"__dict__\"):\n",
        "            stack.extend(obj.__dict__.values())\n",
        "    return int(total) if total else np.nan\n",
        "\n",
        "def measure_joblib_training_time(model, X_train, y_train):\n",
        "    try:\n",
        "        estimator = clone(model)\n",
        "    except Exception:\n",
        "        return np.nan, \"clone_failed\"\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        estimator.fit(X_train, y_train)\n",
        "    except Exception:\n",
        "        return np.nan, \"fit_failed\"\n",
        "    elapsed = time.perf_counter() - start\n",
        "    return float(elapsed), \"measured_fit\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "6b3538d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_top_k_accuracy(y_true, score_matrix, score_labels, k):\n",
        "    if score_matrix is None or score_labels is None:\n",
        "        return np.nan\n",
        "    score_labels = np.asarray(score_labels)\n",
        "    if score_matrix.shape[1] != len(score_labels):\n",
        "        return np.nan\n",
        "    y_true = np.asarray(y_true)\n",
        "    mask = np.isin(y_true, score_labels)\n",
        "    if not np.any(mask):\n",
        "        return np.nan\n",
        "    filtered_scores = score_matrix[mask]\n",
        "    filtered_true = y_true[mask]\n",
        "    return top_k_accuracy_score(filtered_true, filtered_scores, k=k, labels=score_labels)\n",
        "\n",
        "def get_joblib_scores(model, X_data, y_true, labels, top_k):\n",
        "    start = time.perf_counter()\n",
        "    y_pred = model.predict(X_data)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    latency_ms = (elapsed / len(X_data)) * 1000.0\n",
        "    samples_per_second = len(X_data) / elapsed if elapsed > 0 else np.inf\n",
        "\n",
        "    y_proba = None\n",
        "    proba_labels = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_data)\n",
        "        proba_labels = getattr(model, \"classes_\", None)\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_data)\n",
        "        scores = np.asarray(scores)\n",
        "        if scores.ndim == 1:\n",
        "            scores = np.vstack([-scores, scores]).T\n",
        "        exp_scores = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
        "        y_proba = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
        "        proba_labels = getattr(model, \"classes_\", None)\n",
        "\n",
        "    topk_acc = safe_top_k_accuracy(y_true, y_proba, proba_labels, top_k)\n",
        "    return y_pred, y_proba, topk_acc, latency_ms, samples_per_second\n",
        "\n",
        "def evaluate_joblib_model(model_path, X_eval, X_eval_noise, y_eval):\n",
        "    name = f\"joblib::{model_path.name}\"\n",
        "    print(\"\")\n",
        "    print(f\"📦 Evaluating {name}\")\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    labels = np.array(sorted(np.unique(y_eval)))\n",
        "    label_strings = format_labels(labels)\n",
        "\n",
        "    y_pred, y_proba, topk_acc, latency_ms, sps = get_joblib_scores(\n",
        "        model, X_eval, y_eval, labels, TOP_K\n",
        "    )\n",
        "    y_pred_noise, _, _, _, _ = get_joblib_scores(\n",
        "        model, X_eval_noise, y_eval, labels, TOP_K\n",
        "    )\n",
        "\n",
        "    acc = accuracy_score(y_eval, y_pred)\n",
        "    macro_f1 = f1_score(y_eval, y_pred, average=\"macro\")\n",
        "    prec, rec, f1_per_class, support = precision_recall_fscore_support(\n",
        "        y_eval, y_pred, labels=labels, zero_division=0\n",
        "    )\n",
        "    cls_report = classification_report(\n",
        "        y_eval,\n",
        "        y_pred,\n",
        "        labels=labels,\n",
        "        target_names=label_strings,\n",
        "        digits=4,\n",
        "        zero_division=0,\n",
        "    )\n",
        "    cm = confusion_matrix(y_eval, y_pred, labels=labels)\n",
        "\n",
        "    acc_noise = accuracy_score(y_eval, y_pred_noise)\n",
        "    macro_f1_noise = f1_score(y_eval, y_pred_noise, average=\"macro\")\n",
        "\n",
        "    group_metrics = {}\n",
        "    for group_name, group_ids in GESTURE_GROUPS.items():\n",
        "        relaxed_acc = compute_custom_metrics(y_eval, y_pred, group_ids)\n",
        "        group_metrics[f\"relaxed_acc_{group_name}\"] = relaxed_acc\n",
        "\n",
        "    model_disk_mb = model_path.stat().st_size / (1024**2)\n",
        "    param_count = estimate_parameter_count(model)\n",
        "\n",
        "    training_time_s, training_time_source = measure_joblib_training_time(model, X, y)\n",
        "    if np.isnan(training_time_s):\n",
        "        training_time_s = TRAINING_TIME_OVERRIDES.get(name, np.nan)\n",
        "        if not np.isnan(training_time_s):\n",
        "            training_time_source = \"override\"\n",
        "\n",
        "    result = {\n",
        "        \"model_name\": name,\n",
        "        \"model_type\": \"joblib\",\n",
        "        \"y_true\": y_eval,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"y_pred_noise\": y_pred_noise,\n",
        "        \"y_proba\": y_proba,\n",
        "        \"labels\": labels,\n",
        "        \"label_strings\": label_strings,\n",
        "        \"cm\": cm,\n",
        "        \"classification_report\": cls_report,\n",
        "        \"model_disk_mb\": model_disk_mb,\n",
        "        \"model_parameter_count\": param_count,\n",
        "        \"training_time_s\": training_time_s,\n",
        "        \"training_time_source\": training_time_source,\n",
        "        \"latency_ms_per_sample\": latency_ms,\n",
        "        \"samples_per_second\": sps,\n",
        "        \"accuracy\": acc,\n",
        "        TOP_K_KEY: topk_acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"per_class_precision\": prec,\n",
        "        \"per_class_recall\": rec,\n",
        "        \"per_class_f1\": f1_per_class,\n",
        "        \"support\": support,\n",
        "        \"accuracy_with_noise\": acc_noise,\n",
        "        \"macro_f1_with_noise\": macro_f1_noise,\n",
        "        **group_metrics,\n",
        "    }\n",
        "\n",
        "    topk_display = \"N/A\" if np.isnan(topk_acc) else f\"{topk_acc:.4f}\"\n",
        "    param_display = \"N/A\" if np.isnan(param_count) else f\"{int(param_count):,}\"\n",
        "\n",
        "    print(f\"  Accuracy={acc:.4f} | Macro F1={macro_f1:.4f} | Top-{TOP_K} Acc={topk_display}\")\n",
        "    print(f\"  Latency={latency_ms:.4f} ms/sample | Speed={sps:.2f} samples/s\")\n",
        "    print(f\"  Disk size={model_disk_mb:.3f} MB | Parameters≈{param_display}\")\n",
        "    print(f\"  Noise ({NOISE_STD} sigma) -> acc={acc_noise:.4f}, macro F1={macro_f1_noise:.4f}\")\n",
        "    for group_name in GESTURE_GROUPS:\n",
        "        print(f\"  Relaxed Acc ({group_name})={group_metrics[f'relaxed_acc_{group_name}']:.4f}\")\n",
        "\n",
        "    return name, result\n",
        "\n",
        "def evaluate_tflite_model(model_path, X_eval, X_eval_noise, y_eval):\n",
        "    name = f\"tflite::{model_path.name}\"\n",
        "    print(\"\")\n",
        "    print(f\"📦 Evaluating {name}\")\n",
        "    interpreter = tf.lite.Interpreter(model_path=str(model_path))\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()[0]\n",
        "    output_details = interpreter.get_output_details()[0]\n",
        "    input_index = input_details[\"index\"]\n",
        "    output_index = output_details[\"index\"]\n",
        "    input_dtype = input_details[\"dtype\"]\n",
        "\n",
        "    def run_inference(X_data):\n",
        "        preds = []\n",
        "        outputs = []\n",
        "        start = time.perf_counter()\n",
        "        for row in X_data:\n",
        "            tensor = np.expand_dims(row, axis=0).astype(input_dtype)\n",
        "            interpreter.set_tensor(input_index, tensor)\n",
        "            interpreter.invoke()\n",
        "            out = interpreter.get_tensor(output_index)\n",
        "            outputs.append(out[0])\n",
        "            preds.append(int(np.argmax(out)))\n",
        "        elapsed = time.perf_counter() - start\n",
        "        latency_ms = (elapsed / len(X_data)) * 1000.0\n",
        "        samples_per_second = len(X_data) / elapsed if elapsed > 0 else np.inf\n",
        "        outputs = np.asarray(outputs, dtype=np.float32)\n",
        "        exp_scores = np.exp(outputs - outputs.max(axis=1, keepdims=True))\n",
        "        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
        "        return np.array(preds), probs, latency_ms, samples_per_second\n",
        "\n",
        "    y_pred, y_proba, latency_ms, sps = run_inference(X_eval)\n",
        "    y_pred_noise, _, _, _ = run_inference(X_eval_noise)\n",
        "\n",
        "    labels = np.array(sorted(np.unique(y_eval)))\n",
        "    label_strings = format_labels(labels)\n",
        "\n",
        "    acc = accuracy_score(y_eval, y_pred)\n",
        "    macro_f1 = f1_score(y_eval, y_pred, average=\"macro\")\n",
        "    prec, rec, f1_per_class, support = precision_recall_fscore_support(\n",
        "        y_eval, y_pred, labels=labels, zero_division=0\n",
        "    )\n",
        "    cls_report = classification_report(\n",
        "        y_eval,\n",
        "        y_pred,\n",
        "        labels=labels,\n",
        "        target_names=label_strings,\n",
        "        digits=4,\n",
        "        zero_division=0,\n",
        "    )\n",
        "    cm = confusion_matrix(y_eval, y_pred, labels=labels)\n",
        "\n",
        "    acc_noise = accuracy_score(y_eval, y_pred_noise)\n",
        "    macro_f1_noise = f1_score(y_eval, y_pred_noise, average=\"macro\")\n",
        "\n",
        "    group_metrics = {}\n",
        "    for group_name, group_ids in GESTURE_GROUPS.items():\n",
        "        relaxed_acc = compute_custom_metrics(y_eval, y_pred, group_ids)\n",
        "        group_metrics[f\"relaxed_acc_{group_name}\"] = relaxed_acc\n",
        "\n",
        "    topk_acc = safe_top_k_accuracy(y_eval, y_proba, labels, TOP_K)\n",
        "    model_disk_mb = model_path.stat().st_size / (1024**2)\n",
        "    param_count = np.nan\n",
        "    training_time_s = TRAINING_TIME_OVERRIDES.get(name, np.nan)\n",
        "    training_time_source = \"override\" if name in TRAINING_TIME_OVERRIDES else \"unknown\"\n",
        "\n",
        "    result = {\n",
        "        \"model_name\": name,\n",
        "        \"model_type\": \"tflite\",\n",
        "        \"y_true\": y_eval,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"y_pred_noise\": y_pred_noise,\n",
        "        \"y_proba\": y_proba,\n",
        "        \"labels\": labels,\n",
        "        \"label_strings\": label_strings,\n",
        "        \"cm\": cm,\n",
        "        \"classification_report\": cls_report,\n",
        "        \"model_disk_mb\": model_disk_mb,\n",
        "        \"model_parameter_count\": param_count,\n",
        "        \"training_time_s\": training_time_s,\n",
        "        \"training_time_source\": training_time_source,\n",
        "        \"latency_ms_per_sample\": latency_ms,\n",
        "        \"samples_per_second\": sps,\n",
        "        \"accuracy\": acc,\n",
        "        TOP_K_KEY: topk_acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"per_class_precision\": prec,\n",
        "        \"per_class_recall\": rec,\n",
        "        \"per_class_f1\": f1_per_class,\n",
        "        \"support\": support,\n",
        "        \"accuracy_with_noise\": acc_noise,\n",
        "        \"macro_f1_with_noise\": macro_f1_noise,\n",
        "        **group_metrics,\n",
        "    }\n",
        "\n",
        "    topk_display = \"N/A\" if np.isnan(topk_acc) else f\"{topk_acc:.4f}\"\n",
        "\n",
        "    print(f\"  Accuracy={acc:.4f} | Macro F1={macro_f1:.4f} | Top-{TOP_K} Acc={topk_display}\")\n",
        "    print(f\"  Latency={latency_ms:.4f} ms/sample | Speed={sps:.2f} samples/s\")\n",
        "    print(f\"  Disk size={model_disk_mb:.3f} MB\")\n",
        "    print(f\"  Noise ({NOISE_STD} sigma) -> acc={acc_noise:.4f}, macro F1={macro_f1_noise:.4f}\")\n",
        "    for group_name in GESTURE_GROUPS:\n",
        "        print(f\"  Relaxed Acc ({group_name})={group_metrics[f'relaxed_acc_{group_name}']:.4f}\")\n",
        "\n",
        "    return name, result\n",
        "\n",
        "\n",
        "def evaluate_keras_model(model_path, X_eval, X_eval_noise, y_eval):\n",
        "    name = f\"keras::{model_path.name}\"\n",
        "    print(\"\")\n",
        "    print(f\"📦 Evaluating {name}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    def run_inference(X_data):\n",
        "        start = time.perf_counter()\n",
        "        outputs = model.predict(X_data.astype(np.float32), batch_size=256, verbose=0)\n",
        "        elapsed = time.perf_counter() - start\n",
        "        latency_ms = (elapsed / len(X_data)) * 1000.0\n",
        "        samples_per_second = len(X_data) / elapsed if elapsed > 0 else np.inf\n",
        "        return outputs, latency_ms, samples_per_second\n",
        "\n",
        "    logits, latency_ms, sps = run_inference(X_eval)\n",
        "    logits_noise, _, _ = run_inference(X_eval_noise)\n",
        "\n",
        "    y_pred = np.argmax(logits, axis=1)\n",
        "    y_pred_noise = np.argmax(logits_noise, axis=1)\n",
        "    y_proba = tf.nn.softmax(logits, axis=1).numpy()\n",
        "\n",
        "    labels = np.array(sorted(np.unique(y_eval)))\n",
        "    label_strings = format_labels(labels)\n",
        "\n",
        "    acc = accuracy_score(y_eval, y_pred)\n",
        "    macro_f1 = f1_score(y_eval, y_pred, average=\"macro\")\n",
        "    prec, rec, f1_per_class, support = precision_recall_fscore_support(\n",
        "        y_eval, y_pred, labels=labels, zero_division=0\n",
        "    )\n",
        "    cls_report = classification_report(\n",
        "        y_eval,\n",
        "        y_pred,\n",
        "        labels=labels,\n",
        "        target_names=label_strings,\n",
        "        digits=4,\n",
        "        zero_division=0,\n",
        "    )\n",
        "    cm = confusion_matrix(y_eval, y_pred, labels=labels)\n",
        "\n",
        "    acc_noise = accuracy_score(y_eval, y_pred_noise)\n",
        "    macro_f1_noise = f1_score(y_eval, y_pred_noise, average=\"macro\")\n",
        "\n",
        "    group_metrics = {}\n",
        "    for group_name, group_ids in GESTURE_GROUPS.items():\n",
        "        relaxed_acc = compute_custom_metrics(y_eval, y_pred, group_ids)\n",
        "        group_metrics[f\"relaxed_acc_{group_name}\"] = relaxed_acc\n",
        "\n",
        "    topk_acc = safe_top_k_accuracy(y_eval, y_proba, labels, TOP_K)\n",
        "    model_disk_mb = model_path.stat().st_size / (1024**2)\n",
        "    try:\n",
        "        param_count = model.count_params()\n",
        "    except Exception:\n",
        "        param_count = np.nan\n",
        "    training_time_key = f\"keras::{model_path.name}\"\n",
        "    training_time_s = TRAINING_TIME_OVERRIDES.get(training_time_key, np.nan)\n",
        "    training_time_source = \"override\" if training_time_key in TRAINING_TIME_OVERRIDES else \"unknown\"\n",
        "\n",
        "    result = {\n",
        "        \"model_name\": name,\n",
        "        \"model_type\": \"keras\",\n",
        "        \"y_true\": y_eval,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"y_pred_noise\": y_pred_noise,\n",
        "        \"y_proba\": y_proba,\n",
        "        \"labels\": labels,\n",
        "        \"label_strings\": label_strings,\n",
        "        \"cm\": cm,\n",
        "        \"classification_report\": cls_report,\n",
        "        \"model_disk_mb\": model_disk_mb,\n",
        "        \"model_parameter_count\": param_count,\n",
        "        \"training_time_s\": training_time_s,\n",
        "        \"training_time_source\": training_time_source,\n",
        "        \"latency_ms_per_sample\": latency_ms,\n",
        "        \"samples_per_second\": sps,\n",
        "        \"accuracy\": acc,\n",
        "        TOP_K_KEY: topk_acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"per_class_precision\": prec,\n",
        "        \"per_class_recall\": rec,\n",
        "        \"per_class_f1\": f1_per_class,\n",
        "        \"support\": support,\n",
        "        \"accuracy_with_noise\": acc_noise,\n",
        "        \"macro_f1_with_noise\": macro_f1_noise,\n",
        "        **group_metrics,\n",
        "    }\n",
        "\n",
        "    topk_display = \"N/A\" if np.isnan(topk_acc) else f\"{topk_acc:.4f}\"\n",
        "    print(f\"  Accuracy={acc:.4f} | Macro F1={macro_f1:.4f} | Top-{TOP_K} Acc={topk_display}\")\n",
        "    print(f\"  Latency={latency_ms:.4f} ms/sample | Speed={sps:.2f} samples/s\")\n",
        "    print(f\"  Disk size={model_disk_mb:.3f} MB | Parameters≈{param_count if not np.isnan(param_count) else 'N/A'}\")\n",
        "    print(f\"  Noise ({NOISE_STD} sigma) -> acc={acc_noise:.4f}, macro F1={macro_f1_noise:.4f}\")\n",
        "    for group_name in GESTURE_GROUPS:\n",
        "        print(f\"  Relaxed Acc ({group_name})={group_metrics[f'relaxed_acc_{group_name}']:.4f}\")\n",
        "\n",
        "    return name, result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "8afcd6c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Evaluating joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib\n",
            "  Accuracy=0.2283 | Macro F1=0.0773 | Top-3 Acc=0.6644\n",
            "  Latency=0.0011 ms/sample | Speed=892081.15 samples/s\n",
            "  Disk size=0.003 MB | Parameters≈266\n",
            "  Noise (0.015 sigma) -> acc=0.2239, macro F1=0.0752\n",
            "  Relaxed Acc (Circular)=0.5786\n",
            "  Relaxed Acc (Static_vs_Move)=0.2316\n",
            "\n",
            "📦 Evaluating joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib\n",
            "  Accuracy=0.2299 | Macro F1=0.0782 | Top-3 Acc=0.6668\n",
            "  Latency=0.0005 ms/sample | Speed=2117470.24 samples/s\n",
            "  Disk size=0.003 MB | Parameters≈267\n",
            "  Noise (0.015 sigma) -> acc=0.2263, macro F1=0.0768\n",
            "  Relaxed Acc (Circular)=0.5804\n",
            "  Relaxed Acc (Static_vs_Move)=0.2328\n",
            "\n",
            "📦 Evaluating joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy=0.9879 | Macro F1=0.9885 | Top-3 Acc=0.9993\n",
            "  Latency=0.0092 ms/sample | Speed=108228.86 samples/s\n",
            "  Disk size=0.701 MB | Parameters≈177,905\n",
            "  Noise (0.015 sigma) -> acc=0.9457, macro F1=0.9505\n",
            "  Relaxed Acc (Circular)=0.9900\n",
            "  Relaxed Acc (Static_vs_Move)=0.9879\n",
            "\n",
            "📦 Evaluating joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib\n",
            "  Accuracy=0.9271 | Macro F1=0.9317 | Top-3 Acc=0.9976\n",
            "  Latency=0.1592 ms/sample | Speed=6281.29 samples/s\n",
            "  Disk size=0.604 MB | Parameters≈79,916\n",
            "  Noise (0.015 sigma) -> acc=0.9265, macro F1=0.9308\n",
            "  Relaxed Acc (Circular)=0.9356\n",
            "  Relaxed Acc (Static_vs_Move)=0.9351\n",
            "\n",
            "📦 Evaluating joblib::point_history_classifier_randomforest_20251121_014946_04.joblib\n",
            "  Accuracy=0.9928 | Macro F1=0.9931 | Top-3 Acc=1.0000\n",
            "  Latency=0.0165 ms/sample | Speed=60460.51 samples/s\n",
            "  Disk size=17.629 MB | Parameters≈4,304\n",
            "  Noise (0.015 sigma) -> acc=0.8327, macro F1=0.8299\n",
            "  Relaxed Acc (Circular)=0.9942\n",
            "  Relaxed Acc (Static_vs_Move)=0.9928\n",
            "\n",
            "📦 Evaluating joblib::point_history_classifier_xgboost_20251121_015824_09.joblib\n",
            "  Accuracy=0.9894 | Macro F1=0.9899 | Top-3 Acc=1.0000\n",
            "  Latency=0.0025 ms/sample | Speed=397132.47 samples/s\n",
            "  Disk size=1.307 MB | Parameters≈37\n",
            "  Noise (0.015 sigma) -> acc=0.7390, macro F1=0.6986\n",
            "  Relaxed Acc (Circular)=0.9912\n",
            "  Relaxed Acc (Static_vs_Move)=0.9894\n",
            "\n",
            "📦 Evaluating tflite::point_history_classifier_mlp_20251121_015029_05.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy=0.8628 | Macro F1=0.8651 | Top-3 Acc=0.9992\n",
            "  Latency=0.0119 ms/sample | Speed=84099.91 samples/s\n",
            "  Disk size=0.006 MB\n",
            "  Noise (0.015 sigma) -> acc=0.8333, macro F1=0.8358\n",
            "  Relaxed Acc (Circular)=0.8724\n",
            "  Relaxed Acc (Static_vs_Move)=0.8667\n",
            "\n",
            "📦 Evaluating tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy=0.9464 | Macro F1=0.9515 | Top-3 Acc=0.9996\n",
            "  Latency=0.0623 ms/sample | Speed=16056.25 samples/s\n",
            "  Disk size=0.109 MB\n",
            "  Noise (0.015 sigma) -> acc=0.8905, macro F1=0.8934\n",
            "  Relaxed Acc (Circular)=0.9495\n",
            "  Relaxed Acc (Static_vs_Move)=0.9509\n",
            "\n",
            "📦 Evaluating tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brian\\Documents\\GitHub\\mediapipe-hand-recognition\\.conda\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy=0.9866 | Macro F1=0.9863 | Top-3 Acc=0.9997\n",
            "  Latency=0.0737 ms/sample | Speed=13562.13 samples/s\n",
            "  Disk size=0.216 MB\n",
            "  Noise (0.015 sigma) -> acc=0.6017, macro F1=0.5517\n",
            "  Relaxed Acc (Circular)=0.9926\n",
            "  Relaxed Acc (Static_vs_Move)=0.9866\n",
            "\n",
            "📦 Evaluating keras::point_history_classifier_lstm_20251121_015250_06.keras\n",
            "  Accuracy=0.9123 | Macro F1=0.9180 | Top-3 Acc=0.9997\n",
            "  Latency=0.0912 ms/sample | Speed=10963.08 samples/s\n",
            "  Disk size=0.050 MB | Parameters≈1441\n",
            "  Noise (0.015 sigma) -> acc=0.8029, macro F1=0.8011\n",
            "  Relaxed Acc (Circular)=0.9239\n",
            "  Relaxed Acc (Static_vs_Move)=0.9168\n"
          ]
        }
      ],
      "source": [
        "all_results = {}\n",
        "all_predictions = {}\n",
        "\n",
        "for model_path in JOBLIB_MODELS:\n",
        "    name, res = evaluate_joblib_model(model_path, X, X_noise, y)\n",
        "    all_results[name] = res\n",
        "    all_predictions[name] = res[\"y_pred\"]\n",
        "\n",
        "for model_path in TFLITE_MODELS:\n",
        "    name, res = evaluate_tflite_model(model_path, X, X_noise, y)\n",
        "    all_results[name] = res\n",
        "    all_predictions[name] = res[\"y_pred\"]\n",
        "\n",
        "for model_path in KERAS_MODELS:\n",
        "    name, res = evaluate_keras_model(model_path, X, X_noise, y)\n",
        "    all_results[name] = res\n",
        "    all_predictions[name] = res[\"y_pred\"]\n",
        "\n",
        "if not all_results:\n",
        "    raise RuntimeError(\"No models were evaluated. Please add .joblib, .tflite, or .keras files to models/point_history.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Model comparison summary (sorted by macro F1):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_type</th>\n",
              "      <th>model_disk_mb</th>\n",
              "      <th>model_parameter_count</th>\n",
              "      <th>training_time_s</th>\n",
              "      <th>training_time_source</th>\n",
              "      <th>latency_ms_per_sample</th>\n",
              "      <th>samples_per_second</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_3_accuracy</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>accuracy_with_noise</th>\n",
              "      <th>macro_f1_with_noise</th>\n",
              "      <th>relaxed_acc_Circular</th>\n",
              "      <th>relaxed_acc_Static_vs_Move</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>17.628541</td>\n",
              "      <td>4304.0</td>\n",
              "      <td>1.129883</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.016540</td>\n",
              "      <td>6.046051e+04</td>\n",
              "      <td>0.992763</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993138</td>\n",
              "      <td>0.832707</td>\n",
              "      <td>0.829916</td>\n",
              "      <td>0.994154</td>\n",
              "      <td>0.992763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>joblib::point_history_classifier_xgboost_20251121_015824_09.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>1.306925</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1.716947</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.002518</td>\n",
              "      <td>3.971325e+05</td>\n",
              "      <td>0.989422</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.989920</td>\n",
              "      <td>0.739040</td>\n",
              "      <td>0.698629</td>\n",
              "      <td>0.991232</td>\n",
              "      <td>0.989422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>0.700834</td>\n",
              "      <td>177905.0</td>\n",
              "      <td>0.005723</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.009240</td>\n",
              "      <td>1.082289e+05</td>\n",
              "      <td>0.987891</td>\n",
              "      <td>0.999304</td>\n",
              "      <td>0.988483</td>\n",
              "      <td>0.945720</td>\n",
              "      <td>0.950460</td>\n",
              "      <td>0.989979</td>\n",
              "      <td>0.987891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite</td>\n",
              "      <td>tflite</td>\n",
              "      <td>0.216469</td>\n",
              "      <td>NaN</td>\n",
              "      <td>171.304270</td>\n",
              "      <td>override</td>\n",
              "      <td>0.073735</td>\n",
              "      <td>1.356213e+04</td>\n",
              "      <td>0.986639</td>\n",
              "      <td>0.999722</td>\n",
              "      <td>0.986303</td>\n",
              "      <td>0.601670</td>\n",
              "      <td>0.551718</td>\n",
              "      <td>0.992624</td>\n",
              "      <td>0.986639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite</td>\n",
              "      <td>tflite</td>\n",
              "      <td>0.108757</td>\n",
              "      <td>NaN</td>\n",
              "      <td>143.676344</td>\n",
              "      <td>override</td>\n",
              "      <td>0.062281</td>\n",
              "      <td>1.605625e+04</td>\n",
              "      <td>0.946416</td>\n",
              "      <td>0.999582</td>\n",
              "      <td>0.951543</td>\n",
              "      <td>0.890466</td>\n",
              "      <td>0.893427</td>\n",
              "      <td>0.949478</td>\n",
              "      <td>0.950870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>0.604142</td>\n",
              "      <td>79916.0</td>\n",
              "      <td>4.661995</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.159203</td>\n",
              "      <td>6.281291e+03</td>\n",
              "      <td>0.927070</td>\n",
              "      <td>0.997634</td>\n",
              "      <td>0.931733</td>\n",
              "      <td>0.926514</td>\n",
              "      <td>0.930845</td>\n",
              "      <td>0.935560</td>\n",
              "      <td>0.935143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>keras::point_history_classifier_lstm_20251121_015250_06.keras</td>\n",
              "      <td>keras</td>\n",
              "      <td>0.050328</td>\n",
              "      <td>1441.0</td>\n",
              "      <td>139.059242</td>\n",
              "      <td>override</td>\n",
              "      <td>0.091215</td>\n",
              "      <td>1.096308e+04</td>\n",
              "      <td>0.912317</td>\n",
              "      <td>0.999722</td>\n",
              "      <td>0.917978</td>\n",
              "      <td>0.802923</td>\n",
              "      <td>0.801075</td>\n",
              "      <td>0.923869</td>\n",
              "      <td>0.916771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tflite::point_history_classifier_mlp_20251121_015029_05.tflite</td>\n",
              "      <td>tflite</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.974416</td>\n",
              "      <td>override</td>\n",
              "      <td>0.011891</td>\n",
              "      <td>8.409991e+04</td>\n",
              "      <td>0.862770</td>\n",
              "      <td>0.999165</td>\n",
              "      <td>0.865130</td>\n",
              "      <td>0.833264</td>\n",
              "      <td>0.835780</td>\n",
              "      <td>0.872373</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>0.003266</td>\n",
              "      <td>267.0</td>\n",
              "      <td>0.131376</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>2.117470e+06</td>\n",
              "      <td>0.229923</td>\n",
              "      <td>0.666806</td>\n",
              "      <td>0.078233</td>\n",
              "      <td>0.226305</td>\n",
              "      <td>0.076849</td>\n",
              "      <td>0.580376</td>\n",
              "      <td>0.232846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib</td>\n",
              "      <td>joblib</td>\n",
              "      <td>0.003156</td>\n",
              "      <td>266.0</td>\n",
              "      <td>0.129830</td>\n",
              "      <td>measured_fit</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>8.920811e+05</td>\n",
              "      <td>0.228253</td>\n",
              "      <td>0.664440</td>\n",
              "      <td>0.077285</td>\n",
              "      <td>0.223939</td>\n",
              "      <td>0.075160</td>\n",
              "      <td>0.578566</td>\n",
              "      <td>0.231594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                              model_name  \\\n",
              "0                joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "1                     joblib::point_history_classifier_xgboost_20251121_015824_09.joblib   \n",
              "2                  joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib   \n",
              "3  tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite   \n",
              "4           tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite   \n",
              "5                     joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib   \n",
              "6                          keras::point_history_classifier_lstm_20251121_015250_06.keras   \n",
              "7                         tflite::point_history_classifier_mlp_20251121_015029_05.tflite   \n",
              "8          joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib   \n",
              "9                   joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib   \n",
              "\n",
              "  model_type  model_disk_mb  model_parameter_count  training_time_s  \\\n",
              "0     joblib      17.628541                 4304.0         1.129883   \n",
              "1     joblib       1.306925                   37.0         1.716947   \n",
              "2     joblib       0.700834               177905.0         0.005723   \n",
              "3     tflite       0.216469                    NaN       171.304270   \n",
              "4     tflite       0.108757                    NaN       143.676344   \n",
              "5     joblib       0.604142                79916.0         4.661995   \n",
              "6      keras       0.050328                 1441.0       139.059242   \n",
              "7     tflite       0.006382                    NaN        41.974416   \n",
              "8     joblib       0.003266                  267.0         0.131376   \n",
              "9     joblib       0.003156                  266.0         0.129830   \n",
              "\n",
              "  training_time_source  latency_ms_per_sample  samples_per_second  accuracy  \\\n",
              "0         measured_fit               0.016540        6.046051e+04  0.992763   \n",
              "1         measured_fit               0.002518        3.971325e+05  0.989422   \n",
              "2         measured_fit               0.009240        1.082289e+05  0.987891   \n",
              "3             override               0.073735        1.356213e+04  0.986639   \n",
              "4             override               0.062281        1.605625e+04  0.946416   \n",
              "5         measured_fit               0.159203        6.281291e+03  0.927070   \n",
              "6             override               0.091215        1.096308e+04  0.912317   \n",
              "7             override               0.011891        8.409991e+04  0.862770   \n",
              "8         measured_fit               0.000472        2.117470e+06  0.229923   \n",
              "9         measured_fit               0.001121        8.920811e+05  0.228253   \n",
              "\n",
              "   top_3_accuracy  macro_f1  accuracy_with_noise  macro_f1_with_noise  \\\n",
              "0        1.000000  0.993138             0.832707             0.829916   \n",
              "1        1.000000  0.989920             0.739040             0.698629   \n",
              "2        0.999304  0.988483             0.945720             0.950460   \n",
              "3        0.999722  0.986303             0.601670             0.551718   \n",
              "4        0.999582  0.951543             0.890466             0.893427   \n",
              "5        0.997634  0.931733             0.926514             0.930845   \n",
              "6        0.999722  0.917978             0.802923             0.801075   \n",
              "7        0.999165  0.865130             0.833264             0.835780   \n",
              "8        0.666806  0.078233             0.226305             0.076849   \n",
              "9        0.664440  0.077285             0.223939             0.075160   \n",
              "\n",
              "   relaxed_acc_Circular  relaxed_acc_Static_vs_Move  \n",
              "0              0.994154                    0.992763  \n",
              "1              0.991232                    0.989422  \n",
              "2              0.989979                    0.987891  \n",
              "3              0.992624                    0.986639  \n",
              "4              0.949478                    0.950870  \n",
              "5              0.935560                    0.935143  \n",
              "6              0.923869                    0.916771  \n",
              "7              0.872373                    0.866667  \n",
              "8              0.580376                    0.232846  \n",
              "9              0.578566                    0.231594  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "summary_rows = []\n",
        "for name, res in all_results.items():\n",
        "    row = {\n",
        "        \"model_name\": name,\n",
        "        \"model_type\": res[\"model_type\"],\n",
        "        \"model_disk_mb\": res[\"model_disk_mb\"],\n",
        "        \"model_parameter_count\": res[\"model_parameter_count\"],\n",
        "        \"training_time_s\": res[\"training_time_s\"],\n",
        "        \"training_time_source\": res.get(\"training_time_source\"),\n",
        "        \"latency_ms_per_sample\": res[\"latency_ms_per_sample\"],\n",
        "        \"samples_per_second\": res[\"samples_per_second\"],\n",
        "        \"accuracy\": res[\"accuracy\"],\n",
        "        TOP_K_KEY: res[TOP_K_KEY],\n",
        "        \"macro_f1\": res[\"macro_f1\"],\n",
        "        \"accuracy_with_noise\": res[\"accuracy_with_noise\"],\n",
        "        \"macro_f1_with_noise\": res[\"macro_f1_with_noise\"],\n",
        "    }\n",
        "    for group_name in GESTURE_GROUPS:\n",
        "        key = f\"relaxed_acc_{group_name}\"\n",
        "        row[key] = res[key]\n",
        "    summary_rows.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df = summary_df.sort_values(by=\"macro_f1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"📊 Model comparison summary (sorted by macro F1):\")\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model by macro F1: joblib::point_history_classifier_randomforest_20251121_014946_04.joblib\n",
            "Per-class precision / recall / F1:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label_id</th>\n",
              "      <th>label_name</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0: Stop</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998650</td>\n",
              "      <td>0.999324</td>\n",
              "      <td>1481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1: Clockwise</td>\n",
              "      <td>0.994346</td>\n",
              "      <td>0.997569</td>\n",
              "      <td>0.995955</td>\n",
              "      <td>1234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2: Counter Clockwise</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997654</td>\n",
              "      <td>0.998826</td>\n",
              "      <td>1279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3: Move</td>\n",
              "      <td>0.997634</td>\n",
              "      <td>0.971582</td>\n",
              "      <td>0.984436</td>\n",
              "      <td>1302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4: Eight</td>\n",
              "      <td>0.978170</td>\n",
              "      <td>0.996294</td>\n",
              "      <td>0.987149</td>\n",
              "      <td>1889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label_id            label_name  precision    recall        f1  support\n",
              "0         0               0: Stop   1.000000  0.998650  0.999324     1481\n",
              "1         1          1: Clockwise   0.994346  0.997569  0.995955     1234\n",
              "2         2  2: Counter Clockwise   1.000000  0.997654  0.998826     1279\n",
              "3         3               3: Move   0.997634  0.971582  0.984436     1302\n",
              "4         4              4: Eight   0.978170  0.996294  0.987149     1889"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "             0: Stop     1.0000    0.9986    0.9993      1481\n",
            "        1: Clockwise     0.9943    0.9976    0.9960      1234\n",
            "2: Counter Clockwise     1.0000    0.9977    0.9988      1279\n",
            "             3: Move     0.9976    0.9716    0.9844      1302\n",
            "            4: Eight     0.9782    0.9963    0.9871      1889\n",
            "\n",
            "            accuracy                         0.9928      7185\n",
            "           macro avg     0.9940    0.9923    0.9931      7185\n",
            "        weighted avg     0.9929    0.9928    0.9928      7185\n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_model_name = summary_df[\"model_name\"].iloc[0]\n",
        "print(f\"Best model by macro F1: {best_model_name}\")\n",
        "\n",
        "best_res = all_results[best_model_name]\n",
        "\n",
        "per_class_df = pd.DataFrame({\n",
        "    \"label_id\": best_res[\"labels\"],\n",
        "    \"label_name\": best_res[\"label_strings\"],\n",
        "    \"precision\": best_res[\"per_class_precision\"],\n",
        "    \"recall\": best_res[\"per_class_recall\"],\n",
        "    \"f1\": best_res[\"per_class_f1\"],\n",
        "    \"support\": best_res[\"support\"],\n",
        "})\n",
        "\n",
        "print(\"Per-class precision / recall / F1:\")\n",
        "display(per_class_df)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Classification report:\")\n",
        "print(best_res[\"classification_report\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model A: joblib::point_history_classifier_randomforest_20251121_014946_04.joblib\n",
            "Model B: joblib::point_history_classifier_xgboost_20251121_015824_09.joblib\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_2256\\1998907906.py:92: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  fig.tight_layout()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_a = summary_df[\"model_name\"].iloc[0]\n",
        "model_b = summary_df[\"model_name\"].iloc[1] if len(summary_df) > 1 else None\n",
        "\n",
        "print(f\"Model A: {model_a}\")\n",
        "print(f\"Model B: {model_b}\")\n",
        "\n",
        "res_a = all_results[model_a]\n",
        "plot_confusion_matrix(res_a[\"cm\"], res_a[\"label_strings\"], title=f\"Confusion Matrix – {model_a}\")\n",
        "\n",
        "if model_b is not None:\n",
        "    res_b = all_results[model_b]\n",
        "    if np.array_equal(res_a[\"labels\"], res_b[\"labels\"]):\n",
        "        plot_two_confusion_matrices(\n",
        "            cm_a=res_a[\"cm\"],\n",
        "            labels_a=res_a[\"label_strings\"],\n",
        "            name_a=model_a,\n",
        "            cm_b=res_b[\"cm\"],\n",
        "            labels_b=res_b[\"label_strings\"],\n",
        "            name_b=model_b,\n",
        "        )\n",
        "    else:\n",
        "        print(\"Labels differ, skip side-by-side comparison.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📈 McNemar's test (difference vs best model):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>vs_best_model</th>\n",
              "      <th>chi2_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>b_best_correct_other_wrong</th>\n",
              "      <th>c_best_wrong_other_correct</th>\n",
              "      <th>significant_at_0.05</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>5477.040857</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5500</td>\n",
              "      <td>7</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>5465.040946</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5488</td>\n",
              "      <td>7</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tflite::point_history_classifier_mlp_20251121_015029_05.tflite</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>914.379202</td>\n",
              "      <td>7.344841e-201</td>\n",
              "      <td>943</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>keras::point_history_classifier_lstm_20251121_015250_06.keras</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>564.286441</td>\n",
              "      <td>9.821556e-125</td>\n",
              "      <td>584</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>462.168750</td>\n",
              "      <td>1.621572e-102</td>\n",
              "      <td>476</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>315.828080</td>\n",
              "      <td>1.173962e-70</td>\n",
              "      <td>341</td>\n",
              "      <td>8</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>25.688889</td>\n",
              "      <td>4.011299e-07</td>\n",
              "      <td>40</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>23.705128</td>\n",
              "      <td>1.122829e-06</td>\n",
              "      <td>61</td>\n",
              "      <td>17</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>joblib::point_history_classifier_xgboost_20251121_015824_09.joblib</td>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>16.531250</td>\n",
              "      <td>4.785484e-05</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                              model_name  \\\n",
              "0                   joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib   \n",
              "1          joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib   \n",
              "5                         tflite::point_history_classifier_mlp_20251121_015029_05.tflite   \n",
              "8                          keras::point_history_classifier_lstm_20251121_015250_06.keras   \n",
              "3                     joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib   \n",
              "6           tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite   \n",
              "2                  joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib   \n",
              "7  tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite   \n",
              "4                     joblib::point_history_classifier_xgboost_20251121_015824_09.joblib   \n",
              "\n",
              "                                                             vs_best_model  \\\n",
              "0  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "1  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "5  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "8  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "3  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "6  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "2  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "7  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "4  joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "\n",
              "     chi2_stat        p_value  b_best_correct_other_wrong  \\\n",
              "0  5477.040857   0.000000e+00                        5500   \n",
              "1  5465.040946   0.000000e+00                        5488   \n",
              "5   914.379202  7.344841e-201                         943   \n",
              "8   564.286441  9.821556e-125                         584   \n",
              "3   462.168750  1.621572e-102                         476   \n",
              "6   315.828080   1.173962e-70                         341   \n",
              "2    25.688889   4.011299e-07                          40   \n",
              "7    23.705128   1.122829e-06                          61   \n",
              "4    16.531250   4.785484e-05                          28   \n",
              "\n",
              "   c_best_wrong_other_correct  significant_at_0.05  \n",
              "0                           7                 True  \n",
              "1                           7                 True  \n",
              "5                           9                 True  \n",
              "8                           6                 True  \n",
              "3                           4                 True  \n",
              "6                           8                 True  \n",
              "2                           5                 True  \n",
              "7                          17                 True  \n",
              "4                           4                 True  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_model_name = summary_df[\"model_name\"].iloc[0]\n",
        "y_true = all_results[best_model_name][\"y_true\"]\n",
        "y_best = all_results[best_model_name][\"y_pred\"]\n",
        "\n",
        "rows = []\n",
        "for name, res in all_results.items():\n",
        "    if name == best_model_name:\n",
        "        continue\n",
        "    y_pred = res[\"y_pred\"]\n",
        "    chi2_stat, p_value, b, c = mcnemars_test(y_true, y_best, y_pred)\n",
        "    rows.append({\n",
        "        \"model_name\": name,\n",
        "        \"vs_best_model\": best_model_name,\n",
        "        \"chi2_stat\": chi2_stat,\n",
        "        \"p_value\": p_value,\n",
        "        \"b_best_correct_other_wrong\": b,\n",
        "        \"c_best_wrong_other_correct\": c,\n",
        "        \"significant_at_0.05\": (p_value < 0.05) if not np.isnan(p_value) else np.nan,\n",
        "    })\n",
        "\n",
        "if rows:\n",
        "    mcnemar_df = pd.DataFrame(rows).sort_values(\"p_value\")\n",
        "    print(\"📈 McNemar's test (difference vs best model):\")\n",
        "    display(mcnemar_df)\n",
        "else:\n",
        "    print(\"Only one model available — McNemar's test needs at least two.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Analysis for Circular: [1, 2, 4] -> ['Clockwise', 'Counter Clockwise', 'Eight'] ---\n",
            "Counts within group:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pred_label</th>\n",
              "      <th>Clockwise</th>\n",
              "      <th>Counter Clockwise</th>\n",
              "      <th>Eight</th>\n",
              "      <th>Move</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Clockwise</th>\n",
              "      <td>1231</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Counter Clockwise</th>\n",
              "      <td>0</td>\n",
              "      <td>1276</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eight</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1882</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "pred_label         Clockwise  Counter Clockwise  Eight  Move\n",
              "true_label                                                  \n",
              "Clockwise               1231                  0      2     1\n",
              "Counter Clockwise          0               1276      2     1\n",
              "Eight                      6                  0   1882     1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row-normalized confusion (fractions):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pred_label</th>\n",
              "      <th>Clockwise</th>\n",
              "      <th>Counter Clockwise</th>\n",
              "      <th>Eight</th>\n",
              "      <th>Move</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Clockwise</th>\n",
              "      <td>0.997569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.000810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Counter Clockwise</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.997654</td>\n",
              "      <td>0.001564</td>\n",
              "      <td>0.000782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eight</th>\n",
              "      <td>0.003176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.996294</td>\n",
              "      <td>0.000529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "pred_label         Clockwise  Counter Clockwise     Eight      Move\n",
              "true_label                                                         \n",
              "Clockwise           0.997569           0.000000  0.001621  0.000810\n",
              "Counter Clockwise   0.000000           0.997654  0.001564  0.000782\n",
              "Eight               0.003176           0.000000  0.996294  0.000529"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Analysis for Static_vs_Move: [0, 3] -> ['Stop', 'Move'] ---\n",
            "Counts within group:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pred_label</th>\n",
              "      <th>Clockwise</th>\n",
              "      <th>Eight</th>\n",
              "      <th>Move</th>\n",
              "      <th>Stop</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Move</th>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1265</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stop</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1479</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "pred_label  Clockwise  Eight  Move  Stop\n",
              "true_label                              \n",
              "Move                1     36  1265     0\n",
              "Stop                0      2     0  1479"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row-normalized confusion (fractions):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pred_label</th>\n",
              "      <th>Clockwise</th>\n",
              "      <th>Eight</th>\n",
              "      <th>Move</th>\n",
              "      <th>Stop</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Move</th>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.02765</td>\n",
              "      <td>0.971582</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stop</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.99865</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "pred_label  Clockwise    Eight      Move     Stop\n",
              "true_label                                       \n",
              "Move         0.000768  0.02765  0.971582  0.00000\n",
              "Stop         0.000000  0.00135  0.000000  0.99865"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if GESTURE_GROUPS:\n",
        "    for group_name, group_ids in GESTURE_GROUPS.items():\n",
        "        focus_labels = [label_map.get(i, str(i)) for i in group_ids]\n",
        "        print(f\"--- Analysis for {group_name}: {group_ids} -> {focus_labels} ---\")\n",
        "\n",
        "        res = all_results[summary_df[\"model_name\"].iloc[0]]\n",
        "        mask = np.isin(res[\"y_true\"], group_ids)\n",
        "        if mask.sum() == 0:\n",
        "            print(\"No samples from this group in the dataset.\")\n",
        "            continue\n",
        "\n",
        "        sim_df = pd.DataFrame({\n",
        "            \"true_id\": res[\"y_true\"][mask],\n",
        "            \"pred_id\": res[\"y_pred\"][mask],\n",
        "        })\n",
        "        sim_df[\"true_label\"] = sim_df[\"true_id\"].map(label_map)\n",
        "        sim_df[\"pred_label\"] = sim_df[\"pred_id\"].map(label_map)\n",
        "\n",
        "        pivot_counts = pd.crosstab(sim_df[\"true_label\"], sim_df[\"pred_label\"], dropna=False)\n",
        "        pivot_rates = pivot_counts.div(pivot_counts.sum(axis=1), axis=0).fillna(0.0)\n",
        "\n",
        "        print(\"Counts within group:\")\n",
        "        display(pivot_counts)\n",
        "\n",
        "        print(\"Row-normalized confusion (fractions):\")\n",
        "        display(pivot_rates)\n",
        "else:\n",
        "    print(\"No gesture groups configured.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness and efficiency trade-offs (higher efficiency_score favors compact, fast, robust models):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>macro_f1_with_noise</th>\n",
              "      <th>macro_f1_drop_with_noise</th>\n",
              "      <th>model_disk_mb</th>\n",
              "      <th>latency_ms_per_sample</th>\n",
              "      <th>throughput_per_mb</th>\n",
              "      <th>efficiency_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib</td>\n",
              "      <td>0.078233</td>\n",
              "      <td>0.076849</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.003266</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>6.482711e+08</td>\n",
              "      <td>4.981881e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib</td>\n",
              "      <td>0.077285</td>\n",
              "      <td>0.075160</td>\n",
              "      <td>0.002125</td>\n",
              "      <td>0.003156</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>2.826881e+08</td>\n",
              "      <td>2.124692e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tflite::point_history_classifier_mlp_20251121_015029_05.tflite</td>\n",
              "      <td>0.865130</td>\n",
              "      <td>0.835780</td>\n",
              "      <td>0.029351</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.011891</td>\n",
              "      <td>1.317770e+07</td>\n",
              "      <td>1.101365e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>joblib::point_history_classifier_xgboost_20251121_015824_09.joblib</td>\n",
              "      <td>0.989920</td>\n",
              "      <td>0.698629</td>\n",
              "      <td>0.291291</td>\n",
              "      <td>1.306925</td>\n",
              "      <td>0.002518</td>\n",
              "      <td>3.038679e+05</td>\n",
              "      <td>2.122911e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>keras::point_history_classifier_lstm_20251121_015250_06.keras</td>\n",
              "      <td>0.917978</td>\n",
              "      <td>0.801075</td>\n",
              "      <td>0.116903</td>\n",
              "      <td>0.050328</td>\n",
              "      <td>0.091215</td>\n",
              "      <td>2.178316e+05</td>\n",
              "      <td>1.744994e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib</td>\n",
              "      <td>0.988483</td>\n",
              "      <td>0.950460</td>\n",
              "      <td>0.038023</td>\n",
              "      <td>0.700834</td>\n",
              "      <td>0.009240</td>\n",
              "      <td>1.544286e+05</td>\n",
              "      <td>1.467782e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite</td>\n",
              "      <td>0.951543</td>\n",
              "      <td>0.893427</td>\n",
              "      <td>0.058116</td>\n",
              "      <td>0.108757</td>\n",
              "      <td>0.062281</td>\n",
              "      <td>1.476341e+05</td>\n",
              "      <td>1.319003e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite</td>\n",
              "      <td>0.986303</td>\n",
              "      <td>0.551718</td>\n",
              "      <td>0.434585</td>\n",
              "      <td>0.216469</td>\n",
              "      <td>0.073735</td>\n",
              "      <td>6.265165e+04</td>\n",
              "      <td>3.456602e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib</td>\n",
              "      <td>0.931733</td>\n",
              "      <td>0.930845</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.604142</td>\n",
              "      <td>0.159203</td>\n",
              "      <td>1.039704e+04</td>\n",
              "      <td>9.678036e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joblib::point_history_classifier_randomforest_20251121_014946_04.joblib</td>\n",
              "      <td>0.993138</td>\n",
              "      <td>0.829916</td>\n",
              "      <td>0.163222</td>\n",
              "      <td>17.628541</td>\n",
              "      <td>0.016540</td>\n",
              "      <td>3.429694e+03</td>\n",
              "      <td>2.846358e+03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                              model_name  \\\n",
              "8          joblib::point_history_classifier_logisticregression_20251121_014943_01.joblib   \n",
              "9                   joblib::point_history_classifier_linearsvc_20251121_014943_00.joblib   \n",
              "7                         tflite::point_history_classifier_mlp_20251121_015029_05.tflite   \n",
              "1                     joblib::point_history_classifier_xgboost_20251121_015824_09.joblib   \n",
              "6                          keras::point_history_classifier_lstm_20251121_015250_06.keras   \n",
              "2                  joblib::point_history_classifier_kneighbors_20251121_014943_02.joblib   \n",
              "4           tflite::point_history_classifier_graph_transformer_20251121_015515_07.tflite   \n",
              "3  tflite::point_history_classifier_transformer_self_attention_20251121_015810_08.tflite   \n",
              "5                     joblib::point_history_classifier_svc_rbf_20251121_014945_03.joblib   \n",
              "0                joblib::point_history_classifier_randomforest_20251121_014946_04.joblib   \n",
              "\n",
              "   macro_f1  macro_f1_with_noise  macro_f1_drop_with_noise  model_disk_mb  \\\n",
              "8  0.078233             0.076849                  0.001384       0.003266   \n",
              "9  0.077285             0.075160                  0.002125       0.003156   \n",
              "7  0.865130             0.835780                  0.029351       0.006382   \n",
              "1  0.989920             0.698629                  0.291291       1.306925   \n",
              "6  0.917978             0.801075                  0.116903       0.050328   \n",
              "2  0.988483             0.950460                  0.038023       0.700834   \n",
              "4  0.951543             0.893427                  0.058116       0.108757   \n",
              "3  0.986303             0.551718                  0.434585       0.216469   \n",
              "5  0.931733             0.930845                  0.000888       0.604142   \n",
              "0  0.993138             0.829916                  0.163222      17.628541   \n",
              "\n",
              "   latency_ms_per_sample  throughput_per_mb  efficiency_score  \n",
              "8               0.000472       6.482711e+08      4.981881e+07  \n",
              "9               0.001121       2.826881e+08      2.124692e+07  \n",
              "7               0.011891       1.317770e+07      1.101365e+07  \n",
              "1               0.002518       3.038679e+05      2.122911e+05  \n",
              "6               0.091215       2.178316e+05      1.744994e+05  \n",
              "2               0.009240       1.544286e+05      1.467782e+05  \n",
              "4               0.062281       1.476341e+05      1.319003e+05  \n",
              "3               0.073735       6.265165e+04      3.456602e+04  \n",
              "5               0.159203       1.039704e+04      9.678036e+03  \n",
              "0               0.016540       3.429694e+03      2.846358e+03  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "robustness_df = summary_df.copy()\n",
        "robustness_df[\"macro_f1_drop_with_noise\"] = robustness_df[\"macro_f1\"] - robustness_df[\"macro_f1_with_noise\"]\n",
        "robustness_df[\"throughput_per_mb\"] = robustness_df[\"samples_per_second\"] / robustness_df[\"model_disk_mb\"].replace({0: np.nan})\n",
        "robustness_df[\"efficiency_score\"] = robustness_df[\"macro_f1_with_noise\"] * robustness_df[\"throughput_per_mb\"]\n",
        "\n",
        "print(\"Robustness and efficiency trade-offs (higher efficiency_score favors compact, fast, robust models):\")\n",
        "display(robustness_df[[\n",
        "    \"model_name\",\n",
        "    \"macro_f1\",\n",
        "    \"macro_f1_with_noise\",\n",
        "    \"macro_f1_drop_with_noise\",\n",
        "    \"model_disk_mb\",\n",
        "    \"latency_ms_per_sample\",\n",
        "    \"throughput_per_mb\",\n",
        "    \"efficiency_score\",\n",
        "]].sort_values(by=[\"efficiency_score\", \"macro_f1_with_noise\"], ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "- Update `TRAINING_TIME_OVERRIDES` if you have ground-truth training durations (especially for the TFLite exports).\n",
        "- Adjust `GESTURE_GROUPS`, `NOISE_STD`, or `TOP_K` in the config cell to explore other scenarios.\n",
        "- Re-run the notebook after training a new model; any `.joblib` or `.tflite` files inside `models/point_history` will be picked up automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f295bb58",
      "metadata": {},
      "source": [
        "## 1. Big picture\n",
        "\n",
        "We essentially have **three groups of models**:\n",
        "\n",
        "1. **High-performance, high-quality classifiers**\n",
        "\n",
        "   * Random Forest, KNN, XGBoost, Transformer Self-Attention, Graph Transformer, SVC-RBF, LSTM\n",
        "   * Macro F1 (clean) mostly **0.92–0.99+**\n",
        "\n",
        "2. **“Good enough” but clearly weaker**\n",
        "\n",
        "   * Tiny TFLite MLP\n",
        "   * Macro F1 ≈ **0.86**\n",
        "\n",
        "3. **Very fast but basically unusable as classifiers**\n",
        "\n",
        "   * Logistic Regression, Linear SVC\n",
        "   * Macro F1 ≈ **0.07–0.08** (near random)\n",
        "\n",
        "All models are fast enough for real-time, so the real trade-offs are **accuracy + robustness vs model size vs efficiency metric**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Raw performance (clean data)\n",
        "\n",
        "Ordering by **macro_f1** on clean data:\n",
        "\n",
        "1. **Random Forest** – 0.9931\n",
        "2. **XGBoost** – 0.9899\n",
        "3. **KNN** – 0.9885\n",
        "4. **Transformer Self-Attention (TFLite)** – 0.9863\n",
        "5. **Graph Transformer (TFLite)** – 0.9515\n",
        "6. **SVC-RBF** – 0.9317\n",
        "7. **LSTM (Keras)** – 0.9180\n",
        "8. **MLP (TFLite)** – 0.8651\n",
        "   9–10. **Logistic Regression / Linear SVC** – 0.07–0.08\n",
        "\n",
        "**Takeaways:**\n",
        "\n",
        "* The task is **very learnable**: several models are effectively “near perfect” on clean test data.\n",
        "* Classical models (Random Forest, KNN, XGBoost, SVC-RBF) still sit at the top in terms of pure discrimination.\n",
        "* The two TFLite transformer models land just below the very top, but they’re **much smaller** and deployment-friendly.\n",
        "\n",
        "If our benchmark is purely “how close to 1.0 can I get?”, Random Forest wins by a hair, followed closely by XGBoost and KNN.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Robustness to noise\n",
        "\n",
        "Our noise metrics (`macro_f1_with_noise` and `macro_f1_drop_with_noise`) are critical, because real landmark streams are messy.\n",
        "\n",
        "### Most robust (tiny F1 drop)\n",
        "\n",
        "* **SVC-RBF**\n",
        "\n",
        "  * 0.9317 → 0.9308 (drop ≈ 0.0009)\n",
        "* **Logistic Regression**\n",
        "\n",
        "  * 0.0782 → 0.0768 (drop ≈ 0.0014 — but already bad)\n",
        "* **KNN**\n",
        "\n",
        "  * 0.9885 → 0.9505 (drop ≈ 0.0380; absolute performance still very high)\n",
        "* **MLP (TFLite)**\n",
        "\n",
        "  * 0.8651 → 0.8358 (drop ≈ 0.0294)\n",
        "* **Graph Transformer (TFLite)**\n",
        "\n",
        "  * 0.9515 → 0.8934 (drop ≈ 0.0581)\n",
        "\n",
        "### Moderately affected\n",
        "\n",
        "* **LSTM**\n",
        "\n",
        "  * 0.9180 → 0.8011 (drop ≈ 0.1169)\n",
        "* **Random Forest**\n",
        "\n",
        "  * 0.9931 → 0.8299 (drop ≈ 0.1632)\n",
        "\n",
        "### Strongly affected\n",
        "\n",
        "* **XGBoost**\n",
        "\n",
        "  * 0.9899 → 0.6986 (drop ≈ 0.2913)\n",
        "* **Transformer Self-Attention**\n",
        "\n",
        "  * 0.9863 → 0.5517 (drop ≈ 0.4346)\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* **KNN, SVC-RBF, Graph Transformer** are our best combination of “high baseline + decent robustness.”\n",
        "\n",
        "  * KNN remains **~0.95 macro F1 under noise**, which is excellent.\n",
        "  * SVC-RBF barely changes at all.\n",
        "  * Graph Transformer sacrifices some F1 but stays solidly high.\n",
        "* **XGBoost and Transformer SA are fragile**: fantastic on clean data, but they **crash hard** when we perturb the inputs. That’s a red flag if we expect jitter or tracking noise from MediaPipe in the wild.\n",
        "* Random Forest starts as the absolute best model but loses a lot of quality under noise — still usable, but less impressive.\n",
        "\n",
        "If we care about “will it still work when the camera is a bit shaky?”, KNN and SVC-RBF are standouts on the classical side, and Graph Transformer is the most attractive of the TFLite models.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Size, latency, and “efficiency” metric\n",
        "\n",
        "### Model size (disk MB)\n",
        "\n",
        "* **Tiny (≤ 0.01 MB)**\n",
        "\n",
        "  * Logistic Regression, Linear SVC (~0.0032 MB)\n",
        "  * TFLite MLP (~0.0064 MB)\n",
        "* **Small (0.05–0.25 MB)**\n",
        "\n",
        "  * LSTM (0.0503)\n",
        "  * Graph Transformer (0.1088)\n",
        "  * Transformer SA (0.2165)\n",
        "* **Medium (0.6–1.3 MB)**\n",
        "\n",
        "  * SVC-RBF (0.6041)\n",
        "  * KNN (0.7008)\n",
        "  * XGBoost (1.3069)\n",
        "* **Huge**\n",
        "\n",
        "  * Random Forest (17.63 MB)\n",
        "\n",
        "### Latency per sample\n",
        "\n",
        "All are **sub-millisecond**, so latency is not a bottleneck:\n",
        "\n",
        "* Fastest: Logistic Regression, Linear SVC (microseconds).\n",
        "* Fast and accurate: XGBoost (0.0025 ms), KNN (0.0092 ms), MLP (0.0119 ms), Random Forest (0.0165 ms).\n",
        "* Slowest: SVC-RBF (0.1592 ms), but that’s still ~6.3k samples/s — far above real-time needs.\n",
        "\n",
        "### Efficiency metric\n",
        "\n",
        "Ranked by our `efficiency_score`:\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Linear SVC\n",
        "3. TFLite MLP\n",
        "4. XGBoost\n",
        "5. LSTM\n",
        "6. KNN\n",
        "7. Graph Transformer\n",
        "8. Transformer SA\n",
        "9. SVC-RBF\n",
        "10. Random Forest\n",
        "\n",
        "This metric clearly **favors tiny models with massive throughput_per_mb**, even if their accuracy is terrible. That’s why LR/Linear SVC end up at the top.\n",
        "\n",
        "**Takeaway:**\n",
        "\n",
        "* Our efficiency metric is useful to spot “compute/storage cheap” models, but **we must filter by a minimum acceptable macro F1 first**, otherwise LR/LinearSVC look artificially good.\n",
        "* Among *viable* models (say macro F1 ≥ 0.9), **XGBoost, LSTM, KNN, Graph Transformer** offer the best trade-off of speed/size vs quality, according to that score.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Model-by-model quick notes\n",
        "\n",
        "* **Random Forest (joblib)**\n",
        "\n",
        "  * Best macro F1 on clean data.\n",
        "  * Very large (17.6 MB) and moderately robust.\n",
        "  * Great if we don’t care about size and want a simple, classical baseline for server-side.\n",
        "\n",
        "* **KNN (joblib)**\n",
        "\n",
        "  * Almost as good as Random Forest on clean data.\n",
        "  * Very robust to noise and still relatively compact (0.7 MB).\n",
        "  * Good choice for high-accuracy desktop/server deployments; less ideal if memory is extremely tight.\n",
        "\n",
        "* **XGBoost (joblib)**\n",
        "\n",
        "  * High accuracy, smallish model, extremely fast.\n",
        "  * **Big robustness issue**: F1 collapses to ~0.70 with noise.\n",
        "  * Great for clean or pre-filtered data; more risky in raw real-time pipelines.\n",
        "\n",
        "* **Transformer Self-Attention (TFLite)**\n",
        "\n",
        "  * Excellent clean F1 but **very sensitive to noise** (drops to ~0.55).\n",
        "  * Moderately small (~0.22 MB).\n",
        "  * Might need better regularization/data augmentation or architectural tweaks.\n",
        "\n",
        "* **Graph Transformer (TFLite)**\n",
        "\n",
        "  * Strong overall F1, decent robustness, tiny (0.11 MB).\n",
        "  * A very attractive **edge-deployment model**: high quality + tiny footprint.\n",
        "\n",
        "* **SVC-RBF (joblib)**\n",
        "\n",
        "  * Good F1 and almost no degradation under noise.\n",
        "  * Slowest of the bunch but still far beyond real-time needs.\n",
        "  * Solid robust classical option.\n",
        "\n",
        "* **LSTM (Keras)**\n",
        "\n",
        "  * Reasonable F1 and good efficiency score.\n",
        "  * Moderate drop under noise.\n",
        "  * Makes sense if we want a “sequence-aware” neural model in Keras, but it doesn’t clearly beat the Graph Transformer in this comparison.\n",
        "\n",
        "* **TFLite MLP**\n",
        "\n",
        "  * Small, fast, decent F1 but below the main contenders.\n",
        "  * Might be okay for super-constrained devices, but we can generally do better (Graph Transformer) without much size penalty.\n",
        "\n",
        "* **Logistic Regression / Linear SVC**\n",
        "\n",
        "  * Essentially underfitting badly; performance near random.\n",
        "  * Only interesting as a sanity-check baseline or to validate our efficiency metric.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Recommendations\n",
        "\n",
        "If we combine accuracy, robustness, size, and simplicity:\n",
        "\n",
        "* **For edge / mobile (on-device, tiny storage, TFLite)**\n",
        "\n",
        "  * **Primary pick:** **Graph Transformer (TFLite)**\n",
        "\n",
        "    * Good macro F1 (~0.95), reasonably robust to noise, ~0.11 MB.\n",
        "  * **Secondary:** TFLite MLP if we need an even simpler architecture, accepting lower F1.\n",
        "\n",
        "* **For robust high accuracy on “real” noisy data (server/desktop)**\n",
        "\n",
        "  * **KNN** or **SVC-RBF**\n",
        "\n",
        "    * KNN: high accuracy + robust + moderate size.\n",
        "    * SVC-RBF: slightly lower F1 but exceptionally stable under noise.\n",
        "\n",
        "* **For clean or well-denoised inputs where noise isn’t a concern**\n",
        "\n",
        "  * **Random Forest** or **XGBoost**\n",
        "\n",
        "    * Random Forest gives peak F1.\n",
        "    * XGBoost gives excellent speed, smaller size, and good efficiency score, but we accept fragility to noise.\n",
        "\n",
        "* **Not recommended for production**\n",
        "\n",
        "  * Logistic Regression and Linear SVC (very poor F1).\n",
        "  * Transformer SA in its current form if we expect noisy landmarks (large performance collapse).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Overall conclusion\n",
        "\n",
        "* The gesture classification task using MediaPipe finger trajectories is **solved very well** by multiple models; the choice is no longer about “can we classify?” but “which trade-off do we like best?”\n",
        "* **Classical models still dominate raw accuracy**, especially Random Forest, KNN, and XGBoost.\n",
        "* **Graph Transformer (TFLite)** stands out as the **best overall deployment candidate**: small, fast, robust enough, and with high macro F1.\n",
        "* **Our efficiency metric needs to be used alongside a quality threshold**—otherwise, tiny but useless models bubble to the top.\n",
        "* Going forward, if we want a single “default” model that works well almost everywhere, I’d lean toward:\n",
        "\n",
        "  * **Graph Transformer (TFLite)** for on-device use, and\n",
        "  * **KNN or SVC-RBF** for server/desktop pipelines where memory is acceptable and robustness matters.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
